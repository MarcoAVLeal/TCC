---
output:
  pdf_document: 
    number_sections: yes
    latex_engine: xelatex
fontsize: 12pt 
documentclass: article
classoption: a4paper
lang: pt-br
mainfont: Times New Roman
sansfont: Times New Roman
monofont: Times New Roman
indent: true
nocite: '@*'
header-includes:
  - \usepackage{geometry}
  - \geometry{headheight=30pt,left=3cm,bottom=2cm,top=3cm,right=2cm}
  - \usepackage{graphicx}
  - \usepackage[utf8]{inputenc}
  - \usepackage[onehalfspacing]{setspace}
  - \usepackage{fancyhdr}
  - \usepackage{ stmaryrd }
  - \usepackage{eqnarray,amsmath}
  - \usepackage{float}
  - \usepackage{fontspec}
---

\pagestyle{fancy}
\fancyhead[ROE]{\leftmark}
\fancyhead[LO]{}
\fancyfoot[C]{}
\fancyfoot[E]{}
\fancyfoot[RO]{\thepage}
\setlength{\parindent}{1.25cm} 
<!--\setlength{\parskip}{1.0em} -->

 
\setmainfont{Times New Roman}

\newcommand{\regterm}{\beta_0+{\beta_1}x+\varepsilon}
\newcommand{\regest}{\beta_0+{\beta_1}x+\varepsilon}
\newcommand{\regesti}{\beta_0+{\beta_1}x_i+\varepsilon_i}
\newcommand{\regesp}{\beta_0+{\beta_1}x}
\newcommand{\regespi}{\beta_0+{\beta_1}x_i}
\newcommand{\regefiti}{\hat{\beta_0}+\hat{{\beta_1}x_i}}
\newcommand{\bz}{\beta_0}
\newcommand{\bzh}{\hat{\beta_0}}
\newcommand{\bum}{\beta_1}
\newcommand{\bumh}{\hat{\beta_1}}
\newcommand{\somat}{\sum_{i=1}^{n}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\bh}{\hat{\beta}}

\begin{titlepage} 
\begin{center}
{\normalsize \bf UNIVERSIDADE ESTADUAL DE MARINGÁ         \\
        CENTRO DE CIÊNCIAS EXATAS                    \\
        CURSO DE ESTATÍSTICA                          \\

      

 }
 
\end{center} 

\vfill
\begin{center}
{\bf \Large AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA PREDIÇÃO EM MODELOS ADITIVOS }\\[1cm]
\end{center}

\vfill

\begin{center}
{\bf  \Large Marco Aurelio Valles Leal    }\\[1cm]
\end{center}

 

\vfill
 
   \hspace{.45\textwidth}
   \begin{minipage}{.5\textwidth}
     
   \end{minipage}


\vfill
\begin{center}
{\normalsize	 Maringá \\ 2022 }\\[0.2cm]
\end{center}
\end{titlepage}




\begin{titlepage} 

\begin{center}
{\bf  \Large MARCO AURELIO VALLES LEAL    }\\[1cm]
\end{center}

\vfill

\begin{center}
{\bf \Large AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA PREDIÇÃO EM MODELOS ADITIVOS}\\[1cm]
\end{center}


 

\vfill
 
   \hspace{.35\textwidth}
   \begin{minipage}{.5\textwidth}
    
       Trabalho de conclusão de curso apresentado como requisito parcial para a obtenção do título de bacharel em Estatística pela Universidade Estadual de Maringá. \\


\begin{tabular}{ll}
Orientador:   &    Profº Drº George Lucas Moraes Pezzot    \\
Coorientador: &    Profº Drº Willian  Luís de Oliveira  
\end{tabular}




     
   \end{minipage}


\vfill
\begin{center}
{\normalsize	 Maringá \\ 2022 }\\[0.2cm]
\end{center}
\end{titlepage}



\begin{titlepage} 


\begin{flushleft}
{ AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA PREDIÇÃO EM MODELOS ADITIVOS}\\[1cm]
\end{flushleft}


\vfill


\begin{flushleft}
{MARCO AURELIO VALLES LEAL    }\\[1cm]
\end{flushleft}

 

\vfill
 
   \hspace{.45\textwidth}
   \begin{minipage}{.5\textwidth}
    
       Trabalho de conclusão de curso apresentado como requisito parcial para a obtenção do título de bacharel em Estatística pela Universidade Estadual de Maringá.

     
   \end{minipage} \\[1cm]

\vfill

\begin{flushleft}
Aprovado em: \_\_\_\_\_\_\_\_/\_\_\_\_\_\_\_\_/\_\_\_\_\_\_\_\_. 
\end{flushleft}

\vfill

\begin{center}
\textbf {BANCA EXAMINADORA}\\[1cm] 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{ Orientador }\\
 Profº Drº George Lucas Moraes Pezzot \\
Universidade Estadual de Maringá
\end{center}

\vfill

\begin{center}
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{Membro da banca} \\
Nome do professor membro da banca \\
Instituição do professor membro da banca
\end{center}

\vfill

\begin{center}
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{Membro da banca} \\
Nome do professor membro da banca \\
Instituição do professor membro da banca
\end{center}


\end{titlepage}


\begin{titlepage}

\begin{center}
\bf RESUMO
\end{center}

\noindent É comum, nas mais diversas áreas, investigar e modelar a relação entre variáveis. O modelo mais simples é denominado modelo de regressão linear simples e assume que a média da variável resposta é modelada como uma função linear das variáveis explicativas, supondo erros aleatórios com média zero, variância constante e não correlacionados. Entretanto, nem sempre a relação existente é perfeitamente linear. Neste contexto, é possível flexibilizar o modelo de regressão linear modelando a dependência da variável resposta com cada uma das variáveis explicativas em um contexto não paramétrico. Esta nova classe de modelos é dita modelos aditivos e mantêm a característica dos modelos de regressão lineares de serem aditivos nos efeitos preditivos. Portanto, este projeto visa apresentar os modelos aditivos, além de técnicas de suavização utilizadas para ajustar modelos no contexto não paramétrico. Por fim, a metodologia é aplicada em dados artificiais (simulados) e em dados reais, dando enfoque à qualidade das predições.

\vspace{1.5cm}

\noindent \textbf{Palavras-chave} : Regressão. Modelo aditivo. Suavizadores.

\end{titlepage}


```{r setup, include=FALSE}
rm(list=ls())
source(file = "funcoes.R",encoding = "UTF-8")
library(tidyverse)
library(binsmooth)
library(knitr)
library(kableExtra)
library(additive.models)
library(rms)
library(locfit)
knitr::opts_chunk$set(echo = FALSE,warning= FALSE, message= FALSE,
                      out.width = "100%",fig.align = "center",size ="large",fig.height = 2.5)
```




\clearpage
\tableofcontents



\subsection{Aplicações}

Nesta seção 



\subsubsection{Aplicação 1}

 Para esta aplicação serão empregadas as técnicas de suavização em dados reais. Os dados foram retirados do site NIST Standard Reference Database 140 \footnote{https://www.itl.nist.gov/div898/strd/index.html} . É um estudo referente a expansão térmica de cobre. A variável resposta é o coeficiente de expansão térmica e a variável preditora é a temperatura em graus kelvin. Neste trabalho sera abordado um modelo com apenas uma covariável neste caso, sendo o modelo aditivo da seguinte forma

\begin{equation*}
 y = \alpha + f(X) + \epsilon,
\end{equation*}
onde os erros $\epsilon$ são independentes, com $E(\epsilon) = 0$ e $var(\epsilon) = \sigma^2$. A $f(X)$ é uma função univariada arbitrária, que será suavizada pelos métodos vistos até o momento.

```{r}
source(file = "funcoes.R",encoding = "UTF-8")
library(locfit)
library(splines)
library(segmented)
library(rms)
library(pacman)
library(igraph)
library(zoo)

dados <- read.table(textConnection(
  "
        .591E0         24.41E0  
       1.547E0         34.82E0  
       2.902E0         44.09E0  
       2.894E0         45.07E0  
       4.703E0         54.98E0  
       6.307E0         65.51E0  
       7.03E0          70.53E0  
       7.898E0         75.70E0  
       9.470E0         89.57E0  
       9.484E0         91.14E0  
      10.072E0         96.40E0  
      10.163E0         97.19E0  
      11.615E0        114.26E0  
      12.005E0        120.25E0  
      12.478E0        127.08E0  
      12.982E0        133.55E0  
      12.970E0        133.61E0  
      13.926E0        158.67E0  
      14.452E0        172.74E0  
      14.404E0        171.31E0  
      15.190E0        202.14E0  
      15.550E0        220.55E0  
      15.528E0        221.05E0  
      15.499E0        221.39E0  
      16.131E0        250.99E0  
      16.438E0        268.99E0  
      16.387E0        271.80E0  
      16.549E0        271.97E0  
      16.872E0        321.31E0  
      16.830E0        321.69E0  
      16.926E0        330.14E0  
      16.907E0        333.03E0  
      16.966E0        333.47E0  
      17.060E0        340.77E0  
      17.122E0        345.65E0  
      17.311E0        373.11E0  
      17.355E0        373.79E0  
      17.668E0        411.82E0  
      17.767E0        419.51E0  
      17.803E0        421.59E0  
      17.765E0        422.02E0  
      17.768E0        422.47E0  
      17.736E0        422.61E0  
      17.858E0        441.75E0  
      17.877E0        447.41E0  
      17.912E0        448.7E0   
      18.046E0        472.89E0  
      18.085E0        476.69E0  
      18.291E0        522.47E0  
      18.357E0        522.62E0  
      18.426E0        524.43E0  
      18.584E0        546.75E0  
      18.610E0        549.53E0  
      18.870E0        575.29E0  
      18.795E0        576.00E0  
      19.111E0        625.55E0  
        .367E0         20.15E0  
        .796E0         28.78E0  
       0.892E0         29.57E0  
       1.903E0         37.41E0  
       2.150E0         39.12E0  
       3.697E0         50.24E0  
       5.870E0         61.38E0  
       6.421E0         66.25E0  
       7.422E0         73.42E0  
       9.944E0         95.52E0  
      11.023E0        107.32E0  
      11.87E0         122.04E0  
      12.786E0        134.03E0  
      14.067E0        163.19E0  
      13.974E0        163.48E0  
      14.462E0        175.70E0  
      14.464E0        179.86E0  
      15.381E0        211.27E0  
      15.483E0        217.78E0  
      15.59E0         219.14E0  
      16.075E0        262.52E0  
      16.347E0        268.01E0  
      16.181E0        268.62E0  
      16.915E0        336.25E0  
      17.003E0        337.23E0  
      16.978E0        339.33E0  
      17.756E0        427.38E0  
      17.808E0        428.58E0  
      17.868E0        432.68E0  
      18.481E0        528.99E0  
      18.486E0        531.08E0  
      19.090E0        628.34E0  
      16.062E0        253.24E0  
      16.337E0        273.13E0  
      16.345E0        273.66E0  
      16.388E0        282.10E0  
      17.159E0        346.62E0  
      17.116E0        347.19E0  
      17.164E0        348.78E0  
      17.123E0        351.18E0  
      17.979E0        450.10E0  
      17.974E0        450.35E0  
      18.007E0        451.92E0  
      17.993E0        455.56E0  
      18.523E0        552.22E0  
      18.669E0        553.56E0  
      18.617E0        555.74E0  
      19.371E0        652.59E0  
      19.330E0        656.20E0  
       0.080E0         14.13E0  
       0.248E0         20.41E0  
       1.089E0         31.30E0  
       1.418E0         33.84E0  
       2.278E0         39.70E0  
       3.624E0         48.83E0  
       4.574E0         54.50E0  
       5.556E0         60.41E0  
       7.267E0         72.77E0  
       7.695E0         75.25E0  
       9.136E0         86.84E0  
       9.959E0         94.88E0  
       9.957E0         96.40E0  
      11.600E0        117.37E0  
      13.138E0        139.08E0  
      13.564E0        147.73E0  
      13.871E0        158.63E0  
      13.994E0        161.84E0  
      14.947E0        192.11E0  
      15.473E0        206.76E0  
      15.379E0        209.07E0  
      15.455E0        213.32E0  
      15.908E0        226.44E0  
      16.114E0        237.12E0  
      17.071E0        330.90E0  
      17.135E0        358.72E0  
      17.282E0        370.77E0  
      17.368E0        372.72E0  
      17.483E0        396.24E0  
      17.764E0        416.59E0  
      18.185E0        484.02E0  
      18.271E0        495.47E0  
      18.236E0        514.78E0  
      18.237E0        515.65E0  
      18.523E0        519.47E0  
      18.627E0        544.47E0  
      18.665E0        560.11E0  
      19.086E0        620.77E0  
       0.214E0         18.97E0  
       0.943E0         28.93E0  
       1.429E0         33.91E0  
       2.241E0         40.03E0  
       2.951E0         44.66E0  
       3.782E0         49.87E0  
       4.757E0         55.16E0  
       5.602E0         60.90E0  
       7.169E0         72.08E0  
       8.920E0         85.15E0  
      10.055E0         97.06E0  
      12.035E0        119.63E0  
      12.861E0        133.27E0  
      13.436E0        143.84E0  
      14.167E0        161.91E0  
      14.755E0        180.67E0  
      15.168E0        198.44E0  
      15.651E0        226.86E0  
      15.746E0        229.65E0  
      16.216E0        258.27E0  
      16.445E0        273.77E0  
      16.965E0        339.15E0  
      17.121E0        350.13E0  
      17.206E0        362.75E0  
      17.250E0        371.03E0  
      17.339E0        393.32E0  
      17.793E0        448.53E0  
      18.123E0        473.78E0  
      18.49E0         511.12E0  
      18.566E0        524.70E0  
      18.645E0        548.75E0  
      18.706E0        551.64E0  
      18.924E0        574.02E0  
      19.1E0          623.86E0  
       0.375E0         21.46E0  
       0.471E0         24.33E0  
       1.504E0         33.43E0  
       2.204E0         39.22E0  
       2.813E0         44.18E0  
       4.765E0         55.02E0  
       9.835E0         94.33E0  
      10.040E0         96.44E0  
      11.946E0        118.82E0  
      12.596E0        128.48E0  
      13.303E0        141.94E0  
      13.922E0        156.92E0  
      14.440E0        171.65E0  
      14.951E0        190.00E0  
      15.627E0        223.26E0  
      15.639E0        223.88E0  
      15.814E0        231.50E0  
      16.315E0        265.05E0  
      16.334E0        269.44E0  
      16.430E0        271.78E0  
      16.423E0        273.46E0  
      17.024E0        334.61E0  
      17.009E0        339.79E0  
      17.165E0        349.52E0  
      17.134E0        358.18E0  
      17.349E0        377.98E0  
      17.576E0        394.77E0  
      17.848E0        429.66E0  
      18.090E0        468.22E0  
      18.276E0        487.27E0  
      18.404E0        519.54E0  
      18.519E0        523.03E0  
      19.133E0        612.99E0  
      19.074E0        638.59E0  
      19.239E0        641.36E0  
      19.280E0        622.05E0  
      19.101E0        631.50E0  
      19.398E0        663.97E0  
      19.252E0        646.9E0   
      19.89E0         748.29E0  
      20.007E0        749.21E0  
      19.929E0        750.14E0  
      19.268E0        647.04E0  
      19.324E0        646.89E0  
      20.049E0        746.9E0   
      20.107E0        748.43E0  
      20.062E0        747.35E0  
      20.065E0        749.27E0  
      19.286E0        647.61E0  
      19.972E0        747.78E0  
      20.088E0        750.51E0  
      20.743E0        851.37E0  
      20.83E0         845.97E0  
      20.935E0        847.54E0  
      21.035E0        849.93E0  
      20.93E0         851.61E0  
      21.074E0        849.75E0  
      21.085E0        850.98E0  
      20.935E0        848.23E0
  
  
  
  
  "
  
  
  
))

x1            <- dados$V2
y             <- dados$V1
dados         <- data.frame(x = x1,  y = y)
x = dados$x
y = dados$y
```




```{r,fig.cap="\\label{fig:dispersao_aplicacao1}Gráfico de dispersão referente ao estudo de espansão térmica do cobre versus a temperatura em graus Kelvin.", fig.height=2.25}

#plot.curves(x = x1,y = y)
plot.curves(x = dados$x,y = dados$y,labelx = "Temperatura (ºKelvin)",labely = "Coeficiente de Exp. Térmica",title = NULL)
#plot.curves(x = x3,y = y)
```



```{r,echo=FALSE}

### Loess LOOCV
df        <- cbind(x= dados$x,y = dados$y) %>% as.data.frame
tr= 1:nrow(df)
dftr= df[tr,]
number_of_bins   = seq(0.1,0.29,0.01)
number_of_bins_sp= 1:length(number_of_bins) 
cv.errors.loess  = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins) )
cv.errors.kernel = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins) )
cv.errors.sp1    = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins_sp) )
cv.errors.sp3    = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins_sp) ) 
cv.errors.poly   = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins_sp) )

for( i in 1:length(number_of_bins)){ # for each number of knots to test
  
  for( j in tr ){ # for each fold
    
    
    fit.loess              <- loess(y ~ x,degree=1, span = number_of_bins[i], data=df[tr!=j,])
    loess.pred             = predict( fit.loess, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.loess[j,i]   = mean( ( df$y[tr==j] - loess.pred )^2,na.rm = TRUE ) 
    
    fit.kernel             <- locfit(y ~ x,deg=1, alpha = number_of_bins[i],kern="gauss", data=df[tr!=j,])
    kernel.pred            <- predict( fit.kernel, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.kernel[j,i]  <- mean( ( df$y[tr==j] - kernel.pred )^2,na.rm = TRUE ) 
    
    
    p              <-  seq(1,(i),1)/(i+1)
    knots          <-  quantile(df$x[tr!=j]  , p = p)
    fit.spg1       <-  ols(as.formula(glue::glue("y ~ lsp(x, knots)")), data=df[tr!=j,])
    spg1.pred    <- predict( fit.spg1, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.sp1[j,i] <- mean( ( df$y[tr==j] - spg1.pred )^2,na.rm = TRUE )
    
    #p              <-  seq(1,i-1,1)/i
    knots          <-  quantile(df$x[tr!=j]  , p = p)
    fit.spg3       <- lm(y ~ bs(x, knots = knots), data=df[tr!=j,] )
    spg3.pred      <- predict( fit.spg3, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.sp3[j,i] <- mean( ( df$y[tr==j] - spg3.pred )^2,na.rm = TRUE )
    
    
    fit.poly <- lm(y ~ poly(x,i))
    poly.pred      <- predict( fit.poly, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.poly[j,i] <- mean( ( df$y[tr==j] - poly.pred )^2,na.rm = TRUE )
    
  }}

cv.errors.mean.sp1   = apply(cv.errors.sp1,2,mean,na.rm = TRUE)
min.cv.index.sp1     = which.min( cv.errors.mean.sp1 )
cv.min.sp1           = cv.errors.mean.sp1[min.cv.index.sp1]
par.sp1              = number_of_bins_sp[min.cv.index.sp1]  

cv.errors.mean.sp3   = apply(cv.errors.sp3,2,mean,na.rm = TRUE)
min.cv.index.sp3     = which.min( cv.errors.mean.sp3 )
cv.min.sp3           = cv.errors.mean.sp3[min.cv.index.sp3]
par.sp3              = number_of_bins_sp[min.cv.index.sp3]


cv.errors.mean.loess   = apply(cv.errors.loess,2,mean,na.rm = TRUE)
min.cv.index.loess     = which.min( cv.errors.mean.loess )
cv.min.loess           = cv.errors.mean.loess[min.cv.index.loess]
par.loess              = number_of_bins[min.cv.index.loess]
### Kernel (Nadaraya-Watson) LOOCV

cv.errors.mean.kernel  = apply(cv.errors.kernel,2,mean,na.rm = TRUE)
min.cv.index.kernel    = which.min( cv.errors.mean.kernel )
cv.min.kernel          = cv.errors.mean.kernel[min.cv.index.kernel]
par.kernel              = number_of_bins[min.cv.index.kernel]

cv.errors.mean.poly  = apply(cv.errors.poly,2,mean,na.rm = TRUE)
min.cv.index.poly    = which.min( cv.errors.mean.poly )
cv.min.poly          = cv.errors.mean.poly[min.cv.index.poly]
par.poly              = number_of_bins_sp[min.cv.index.poly]

mt <- c()
mt <- rbind(mt,c(cv.min.kernel,cv.min.loess,cv.min.sp1,cv.min.sp3,cv.min.poly,par.loess,par.kernel,par.sp1,par.sp3,par.poly))





```




```{r,echo=FALSE}



df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins, y = cv.errors.mean.kernel,EQM = cv.errors.mean.kernel)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p4.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = par.kernel,color ="red")+
  annotate("text",x = number_of_bins[min.cv.index.kernel]+0.05,y = max(cv.errors.mean.kernel)*0.9,label=paste0("Span:",number_of_bins[min.cv.index.kernel],"\nEQM:",round(cv.min.kernel,4))) +
  axis.theme()

par4 = number_of_bins[min.cv.index.kernel]

```


```{r,echo=FALSE}

### Kernel (Nadaraya-Watson) LOOCV
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins, y = cv.errors.mean.loess,EQM = cv.errors.mean.loess)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p5.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = par.loess,color ="red")+
  annotate("text",x = number_of_bins[min.cv.index.loess]+0.05,y = max(cv.errors.mean.loess)*0.9,label=paste0("Span:",number_of_bins[min.cv.index.loess],"\nEQM:",round(cv.min.loess,4))) +
  axis.theme()

par5 = number_of_bins[min.cv.index.loess]
```


```{r,echo=FALSE}
df <- data.frame(x = dados$x, y = dados$y)
df1 <- data.frame(x = number_of_bins_sp, y = cv.errors.mean.sp1,EQM = cv.errors.mean.sp1)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p6.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = number_of_bins_sp[min.cv.index.sp1],color ="red")+
  annotate("text",x = number_of_bins_sp[min.cv.index.sp1]-5,y = max(cv.errors.mean.sp1)*0.8,label=paste0("Nº Nós:",par.sp1,"\nEQM:",round(cv.min.sp1,4))) +
  axis.theme()

par6 = number_of_bins[min.cv.index.sp1]
```

```{r,echo=FALSE}
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins_sp, y = cv.errors.mean.sp3,EQM = cv.errors.mean.sp3)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p7.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = number_of_bins_sp[min.cv.index.sp3],color ="red")+
  annotate("text",x = number_of_bins_sp[min.cv.index.sp3]-5,y = max(cv.errors.mean.sp3)*0.8,label=paste0("Nº Nós:",par.sp3,"\nEQM:",round(cv.min.sp3,4))) +
  axis.theme()

par7 = number_of_bins[min.cv.index.sp3]

```


```{r,echo=FALSE}
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins_sp[1:19], y = cv.errors.mean.poly[1:19],EQM = cv.errors.mean.poly[1:19])


colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p8.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = number_of_bins_sp[min.cv.index.poly],color ="red")+
  annotate("text",x = number_of_bins_sp[min.cv.index.poly]-5,y = max(df1$y)*0.8,label=paste0("Nº Nós:",par.poly,"\nEQM:",round(cv.min.poly,4))) +
  axis.theme()

par8 = number_of_bins[min.cv.index.poly]

```


```{r,echo=FALSE,include=TRUE,fig.cap="\\label{fig:smoothers_cv_aplicacao_1}Comparação entre diferentes ajustes para valores de parâmetros distintos, considerando os suavizadores (A) Kernel, (B) Loess, (C) Splines de Regressão Grau 1 e (D) Splines de Regressão Grau 3.", fig.height=4.0}

partial_plots <- cowplot::plot_grid(p4.cv,p5.cv,p6.cv,p7.cv,p8.cv,ncol=2,nrow =3,labels=LETTERS[1:5])
partial_plots

```

Na Tabela \ref{tab:tab_eqm_aplicao1} é apresentado a os EQM's proveniente do processo de *leave one out cross-validation* para os suavizadores *kernel*, *loess*, *splines* de regressão consdierando grau 1 e grau 3.

```{r,echo=FALSE,warning=FALSE}

library(Metrics)
df_metrics <- data.frame(Smoother = c("Kernel", "Loess","Splines Grau 1","Splines Grau 3","Poly"),
                         EQM      =  c(
                           cv.min.kernel,
                           cv.min.loess,
                           cv.min.sp1,
                           cv.min.sp3,
                           cv.min.poly))


kable_data(data = df_metrics,cap = "\\label{tab:tab_eqm_aplicao1}Erro Quadrático Médio para os suavizadores Loess, Kernel e Spline Cúbico",foot = NULL)

```




```{r,  echo=F}

fit4 <- loess(y ~ x, degree=1, span = par.loess, data=dados)



# kernel - span = 6
fit5 <-  locfit(y ~ x,deg=1, alpha = par.kernel,kern="gauss", data=dados)


#Spline grau 1


k = par.sp1

p         <-  seq(1,k-1,1)/(k)
knots     <-  quantile(dados$x  , p = p)
fit6       <-  ols(as.formula(glue::glue("y ~ lsp(x, knots)")), data = dados)

# spline cubico
require(splines)

k = par.sp3 
p         <-  seq(1,k-1,1)/(k)
knots     <-  quantile(dados$x  , p = p)
fit7 <- lm(y ~ bs(x, knots = knots),data = dados )

k = par.poly 

fit8 <- lm(y ~ poly(x,k))


spans = c(par4,par5,par6,par7)
df = cbind(x= dados$x, y = dados$y, Loess = fit4$fitted,Kernel = fitted.values(fit5),`Spline Grau 1`= predict(fit6),`Spline Cubico`= fit7$fitted,`Poly`= fit8$fitted)
df1 = df %>% as.data.frame
colnames(df) = c("x", "y",
                 paste0("A4\n", "Loess|Span: ", par.loess),
                 paste0("A5\n", "Kernel|Width: ", par.kernel),
                 paste0("A6\n", "Spline Grau 1|Nº Nós: ", par.sp1),
                 paste0("A7\n", "Spline Grau 3|Nº Nós: ", par.sp3),
                 paste0("A8\n", "Poly|Nº Nós: ", par.poly))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )

```


Na Figura \ref{fig:smoothers_fit_aplicacao_1_bestloocv} é apresentado os ajustes para cada técnica considerando os melhores parâmetros obtidos por meio da validação cruzada.

```{r echo=F,fig.height=2.75, fig.cap="\\label{fig:smoothers_fit_aplicacao_1_bestloocv}Comparação dos ajustes entre os métodos"}
plot.mult.curves(df = dados,df_fit = df,title = NULL,labelx = "X",labely = "Y",line.s = 1.05,alpha.o = .99)
```


\clearpage

\subsection{Aplicação 2}

site : https://www.itl.nist.gov/div898/strd/lls/data/Filip.shtml


site : https://www.itl.nist.gov/div898/strd/lls/data/LINKS/DATA/Filip.dat


NIST/ITL StRD
Dataset Name:  Filip (Filip.dat)

File Format:   ASCII
               Certified Values  (lines 31 to 55)
               Data              (lines 61 to 142)

Procedure:     Linear Least Squares Regression

Reference:     Filippelli, A., NIST.

Data:          1 Response Variable (y)
               1 Predictor Variable (x)
               82 Observations
               Higher Level of Difficulty
               Observed Data

Model:         Polynomial Class
               11 Parameters (B0,B1,...,B10)

               y = B0 + B1*x + B2*(x**2) + ... + B9*(x**9) + B10*(x**10) + e



```{r}
source(file = "funcoes.R",encoding = "UTF-8")
library(locfit)
library(splines)
library(segmented)
library(rms)
library(pacman)
library(igraph)
library(zoo)

dados <- read.table(textConnection(
  "
0.8116   -6.860120914
            0.9072   -4.324130045
            0.9052   -4.358625055
            0.9039   -4.358426747
            0.8053   -6.955852379
            0.8377   -6.661145254
            0.8667   -6.355462942
            0.8809   -6.118102026
            0.7975   -7.115148017
            0.8162   -6.815308569
            0.8515   -6.519993057
            0.8766   -6.204119983
            0.8885   -5.853871964
            0.8859   -6.109523091
            0.8959   -5.79832982
            0.8913   -5.482672118
            0.8959   -5.171791386
            0.8971   -4.851705903
            0.9021   -4.517126416
            0.909    -4.143573228
            0.9139   -3.709075441
            0.9199   -3.499489089
            0.8692   -6.300769497
            0.8872   -5.953504836
            0.89     -5.642065153
            0.891    -5.031376979
            0.8977   -4.680685696
            0.9035   -4.329846955
            0.9078   -3.928486195
            0.7675   -8.56735134
            0.7705   -8.363211311
            0.7713   -8.107682739
            0.7736   -7.823908741
            0.7775   -7.522878745
            0.7841   -7.218819279
            0.7971   -6.920818754
            0.8329   -6.628932138
            0.8641   -6.323946875
            0.8804   -5.991399828
            0.7668   -8.781464495
            0.7633   -8.663140179
            0.7678   -8.473531488
            0.7697   -8.247337057
            0.77     -7.971428747
            0.7749   -7.676129393
            0.7796   -7.352812702
            0.7897   -7.072065318
            0.8131   -6.774174009
            0.8498   -6.478861916
            0.8741   -6.159517513
            0.8061   -6.835647144
            0.846    -6.53165267
            0.8751   -6.224098421
            0.8856   -5.910094889
            0.8919   -5.598599459
            0.8934   -5.290645224
            0.894    -4.974284616
            0.8957   -4.64454848
            0.9047   -4.290560426
            0.9129   -3.885055584
            0.9209   -3.408378962
            0.9219   -3.13200249
            0.7739   -8.726767166
            0.7681   -8.66695597
            0.7665   -8.511026475
            0.7703   -8.165388579
            0.7702   -7.886056648
            0.7761   -7.588043762
            0.7809   -7.283412422
            0.7961   -6.995678626
            0.8253   -6.691862621
            0.8602   -6.392544977
            0.8809   -6.067374056
            0.8301   -6.684029655
            0.8664   -6.378719832
            0.8834   -6.065855188
            0.8898   -5.752272167
            0.8964   -5.132414673
            0.8963   -4.811352704
            0.9074   -4.098269308
            0.9119   -3.66174277
            0.9228   -3.2644011
"
  
  
  
))

set.seed(20)
q <- seq(from=0, to=20, by=0.1)
y <- 500 + 0.4 * (q-10)^3
noise <- rnorm(length(q), mean=10, sd=80)
noisy.y <- y + noise
x1            <- q
y             <- noisy.y
dados         <- data.frame(x = x1,  y = y)
x = dados$x
y = dados$y
```




```{r,fig.cap="\\label{fig:dispersao_aplicacao2}Gráfico de dispersão dos dados gerados para o estudo de simulação", fig.height=2.25}

plot.curves(x = x1,y = y)
#plot.curves(x = dados$x,y = dados$y)
#plot.curves(x = x3,y = y)
```


```{r,echo=FALSE}

### Loess LOOCV
df        <- cbind(x= dados$x,y = dados$y) %>% as.data.frame
tr= 1:nrow(df)
dftr= df[tr,]
number_of_bins   = seq(0.1,0.29,0.01)
number_of_bins_sp= 1:length(number_of_bins) 
cv.errors.loess  = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins) )
cv.errors.kernel = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins) )
cv.errors.sp1    = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins_sp) )
cv.errors.sp3    = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins_sp) ) 
cv.errors.poly   = matrix( NA,nrow =  nrow(df), ncol = length(number_of_bins_sp) )

for( i in 1:length(number_of_bins)){ # for each number of knots to test
  
  for( j in tr ){ # for each fold
    
    
    fit.loess              <- loess(y ~ x,degree=1, span = number_of_bins[i], data=df[tr!=j,])
    loess.pred             = predict( fit.loess, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.loess[j,i]   = mean( ( df$y[tr==j] - loess.pred )^2,na.rm = TRUE ) 
    
    fit.kernel             <- locfit(y ~ x,deg=1, alpha = number_of_bins[i],kern="gauss", data=df[tr!=j,])
    kernel.pred            <- predict( fit.kernel, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.kernel[j,i]  <- mean( ( df$y[tr==j] - kernel.pred )^2,na.rm = TRUE ) 
    
    
    p              <-  seq(1,(i),1)/(i+1)
    knots          <-  quantile(df$x[tr!=j]  , p = p)
    fit.spg1       <-  ols(as.formula(glue::glue("y ~ lsp(x, knots)")), data=df[tr!=j,])
    spg1.pred    <- predict( fit.spg1, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.sp1[j,i] <- mean( ( df$y[tr==j] - spg1.pred )^2,na.rm = TRUE )
    
    #p              <-  seq(1,i-1,1)/i
    knots          <-  quantile(df$x[tr!=j]  , p = p)
    fit.spg3       <- lm(y ~ bs(x, knots = knots), data=df[tr!=j,] )
    spg3.pred      <- predict( fit.spg3, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.sp3[j,i] <- mean( ( df$y[tr==j] - spg3.pred )^2,na.rm = TRUE )
    
    
    fit.poly <- lm(y ~ poly(x,i))
    poly.pred      <- predict( fit.poly, newdata=data.frame(x = df$x[tr==j]) )
    cv.errors.poly[j,i] <- mean( ( df$y[tr==j] - poly.pred )^2,na.rm = TRUE )
    
  }}

cv.errors.mean.sp1   = apply(cv.errors.sp1,2,mean,na.rm = TRUE)
min.cv.index.sp1     = which.min( cv.errors.mean.sp1 )
cv.min.sp1           = cv.errors.mean.sp1[min.cv.index.sp1]
par.sp1              = number_of_bins_sp[min.cv.index.sp1]  

cv.errors.mean.sp3   = apply(cv.errors.sp3,2,mean,na.rm = TRUE)
min.cv.index.sp3     = which.min( cv.errors.mean.sp3 )
cv.min.sp3           = cv.errors.mean.sp3[min.cv.index.sp3]
par.sp3              = number_of_bins_sp[min.cv.index.sp3]


cv.errors.mean.loess   = apply(cv.errors.loess,2,mean,na.rm = TRUE)
min.cv.index.loess     = which.min( cv.errors.mean.loess )
cv.min.loess           = cv.errors.mean.loess[min.cv.index.loess]
par.loess              = number_of_bins[min.cv.index.loess]
### Kernel (Nadaraya-Watson) LOOCV

cv.errors.mean.kernel  = apply(cv.errors.kernel,2,mean,na.rm = TRUE)
min.cv.index.kernel    = which.min( cv.errors.mean.kernel )
cv.min.kernel          = cv.errors.mean.kernel[min.cv.index.kernel]
par.kernel              = number_of_bins[min.cv.index.kernel]

cv.errors.mean.poly  = apply(cv.errors.poly,2,mean,na.rm = TRUE)
min.cv.index.poly    = which.min( cv.errors.mean.poly )
cv.min.poly          = cv.errors.mean.poly[min.cv.index.poly]
par.poly              = number_of_bins_sp[min.cv.index.poly]

mt <- c()
mt <- rbind(mt,c(cv.min.kernel,cv.min.loess,cv.min.sp1,cv.min.sp3,cv.min.poly,par.loess,par.kernel,par.sp1,par.sp3,par.poly))





```




```{r,echo=FALSE}



df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins, y = cv.errors.mean.kernel,EQM = cv.errors.mean.kernel)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p4.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = par.kernel,color ="red")+
  annotate("text",x = number_of_bins[min.cv.index.kernel]+0.05,y = max(cv.errors.mean.kernel)*0.9,label=paste0("Span:",number_of_bins[min.cv.index.kernel],"\nEQM:",round(cv.min.kernel,4))) +
  axis.theme()

par4 = number_of_bins[min.cv.index.kernel]

```


```{r,echo=FALSE}

### Kernel (Nadaraya-Watson) LOOCV
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins, y = cv.errors.mean.loess,EQM = cv.errors.mean.loess)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p5.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = par.loess,color ="red")+
  annotate("text",x = number_of_bins[min.cv.index.loess]+0.05,y = max(cv.errors.mean.loess)*0.9,label=paste0("Span:",number_of_bins[min.cv.index.loess],"\nEQM:",round(cv.min.loess,4))) +
  axis.theme()

par5 = number_of_bins[min.cv.index.loess]
```


```{r,echo=FALSE}
df <- data.frame(x = dados$x, y = dados$y)
df1 <- data.frame(x = number_of_bins_sp, y = cv.errors.mean.sp1,EQM = cv.errors.mean.sp1)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p6.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = number_of_bins_sp[min.cv.index.sp1],color ="red")+
  annotate("text",x = number_of_bins_sp[min.cv.index.sp1]-5,y = max(cv.errors.mean.sp1)*0.8,label=paste0("Nº Nós:",par.sp1,"\nEQM:",round(cv.min.sp1,4))) +
  axis.theme()

par6 = number_of_bins[min.cv.index.sp1]
```

```{r,echo=FALSE}
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins_sp, y = cv.errors.mean.sp3,EQM = cv.errors.mean.sp3)



colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p7.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = number_of_bins_sp[min.cv.index.sp3],color ="red")+
  annotate("text",x = number_of_bins_sp[min.cv.index.sp3]-5,y = max(cv.errors.mean.sp3)*0.8,label=paste0("Nº Nós:",par.sp3,"\nEQM:",round(cv.min.sp3,4))) +
  axis.theme()

par7 = number_of_bins[min.cv.index.sp3]

```


```{r,echo=FALSE}
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins_sp[1:19], y = cv.errors.mean.poly[1:19],EQM = cv.errors.mean.poly[1:19])


colnames(df1) <- c("x","y",
                   paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p8.cv <-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM") +
  geom_vline(xintercept = number_of_bins_sp[min.cv.index.poly],color ="red")+
  annotate("text",x = number_of_bins_sp[min.cv.index.poly]-5,y = max(df1$y)*0.8,label=paste0("Nº Nós:",par.poly,"\nEQM:",round(cv.min.poly,4))) +
  axis.theme()

par8 = number_of_bins[min.cv.index.poly]

```


```{r,echo=FALSE,include=TRUE,fig.cap="\\label{fig:smoothers_cv_aplicacao_1}Comparação entre diferentes ajustes para valores de parâmetros distintos, considerando os suavizadores (A) Kernel, (B) Loess, (C) Splines de Regressão Grau 1 e (D) Splines de Regressão Grau 3.", fig.height=4.0}

partial_plots <- cowplot::plot_grid(p4.cv,p5.cv,p6.cv,p7.cv,p8.cv,ncol=2,nrow =3,labels=LETTERS[1:5])
partial_plots

```

Na Tabela \ref{tab:tab_eqm_aplicao1} é apresentado a os EQM's proveniente do processo de *leave one out cross-validation* para os suavizadores *kernel*, *loess*, *splines* de regressão consdierando grau 1 e grau 3.

```{r,echo=FALSE,warning=FALSE}

library(Metrics)
df_metrics <- data.frame(Smoother = c("Kernel", "Loess","Splines Grau 1","Splines Grau 3","Poly"),
                         EQM      =  c(
                           cv.min.kernel,
                           cv.min.loess,
                           cv.min.sp1,
                           cv.min.sp3,
                           cv.min.poly))


kable_data(data = df_metrics,cap = "\\label{tab:tab_eqm_aplicao1}Erro Quadrático Médio para os suavizadores Loess, Kernel e Spline Cúbico",foot = NULL)

```




```{r,  echo=F}

fit4 <- loess(y ~ x, degree=1, span = par.loess, data=dados)



# kernel - span = 6
fit5 <-  locfit(y ~ x,deg=1, alpha = par.kernel,kern="gauss", data=dados)


#Spline grau 1


k = par.sp1

p         <-  seq(1,k,1)/(k+1)
knots     <-  quantile(dados$x  , p = p)
fit6       <-  ols(as.formula(glue::glue("y ~ lsp(x, knots)")), data = dados)

# spline cubico
require(splines)

k = par.sp3 
p         <-  seq(1,k,1)/(k+1)
knots     <-  quantile(dados$x  , p = p)
fit7 <- lm(y ~ bs(x, knots = knots),data = dados )

k = 3

fit8 <- lm(y ~ poly(x,k))


spans = c(par4,par5,par6,par7)
df = cbind(x= dados$x, y = dados$y, Loess = fit4$fitted,Kernel = fitted.values(fit5),`Spline Grau 1`= predict(fit6),`Spline Cubico`= fit7$fitted,`Poly`= fit8$fitted)
df1 = df %>% as.data.frame
colnames(df) = c("x", "y",
                 paste0("A4\n", "Loess|Span: ", par.loess),
                 paste0("A5\n", "Kernel|Width: ", par.kernel),
                 paste0("A6\n", "Spline Grau 1|Nº Nós: ", par.sp1),
                 paste0("A7\n", "Spline Grau 3|Nº Nós: ", par.sp3),
                 paste0("A8\n", "Poly|Nº Nós: ", par.poly))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )

```


Na Figura \ref{fig:smoothers_fit_aplicacao_1_bestloocv} é apresentado os ajustes para cada técnica considerando os melhores parâmetros obtidos por meio da validação cruzada.

```{r echo=F,fig.height=2.75, fig.cap="\\label{fig:smoothers_fit_aplicacao_1_bestloocv}Comparação dos ajustes entre os métodos"}
plot.mult.curves(df = dados,df_fit = df,title = NULL,labelx = "X",labely = "Y",line.s = 1.05,alpha.o = .99)
```


\clearpage

\section{Referências}




\noindent BUJA, A., HASTIE, T. & TIBSHIRANI, R. (1989). **Linear smoothers and additive models**. The Annals of Statistics, 17, 453-510.

\vspace{0.25cm}
\noindent CLEVELAND, W. S. (1979). **Robust locally weighted regression and smoothing scatterplots**. Journal of the American Statistical Association, 74,
829-836. 

\vspace{0.25cm}
\noindent DELICADO, P., 2008 **Curso de Modelos no Paramétricos** p. 200.

\vspace{0.25cm}
\noindent EUBANK, R. L(1999)  **Nonparametric Regression and Spline Smoothing**. Marcel Dekker, 2o edição. Citado na pág. 1, 2, 29 

\vspace{0.25cm}
\noindent FAHRMEIR, L. & TUTZ, G. (2001) **Multivariate Statistical Modelling Based on Generalized Linear Models**. Springer, 2o edição. Citado na pág. 15

\vspace{0.25cm}
\noindent GREEN, P. J.  & YANDELL, B. S. (1985) **Semi-parametric generalized linear models**. Lecture Notes in Statistics, 32:4455. Citado na pág. 15

\vspace{0.25cm}
\noindent GREEN P. J. & SILVERMAN B. W. (1994). **Nonparametric regression and generalized linear models: a roughness penalty approach**. Chapman & Hall, London.

\vspace{0.25cm}
\noindent HASTIE, T. J. & TIBSHIRANI, R. J. (1990). **Generalized additive models**, volume 43. Chapman and Hall, Ltd., London. ISBN 0-412-34390-8.

\vspace{0.25cm}
\noindent MONTGOMERY, D. C. & PECK, E. A. & VINING, G. G. **Introduction to Linear Regression Analysis**. 5th Edition. John Wiley & Sons, 2012. 

\vspace{0.25cm}
\noindent IZBICK, r & SANTOS, T. M. **Aprendizado de máquina: uma abordagem estatística**. ISBN 978-65-00-02410-4.

\vspace{0.25cm}
\noindent TEAM, R. CORE. R: **A language and environment for statistical computing**. (2013). 











