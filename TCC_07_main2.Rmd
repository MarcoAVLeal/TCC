---
output:
  pdf_document: 
    number_sections: yes
    latex_engine: xelatex
fontsize: 12pt 
documentclass: article
classoption: a4paper
lang: pt-br
mainfont: Times New Roman
sansfont: Times New Roman
monofont: Times New Roman
indent: true
nocite: '@*'
header-includes:
  - \usepackage{geometry}
  - \geometry{headheight=30pt,left=3cm,bottom=2cm,top=3cm,right=2cm}
  - \usepackage{graphicx}
  - \usepackage[utf8]{inputenc}
  - \usepackage[onehalfspacing]{setspace}
  - \usepackage{fancyhdr}
  - \usepackage{ stmaryrd }
  - \usepackage{eqnarray,amsmath}
  - \usepackage{float}
  - \usepackage{fontspec}
  - \usepackage{adjustbox,mdframed}
---

\pagestyle{fancy}
\fancyhead[ROE]{\leftmark}
\fancyhead[LO]{}
\fancyfoot[C]{}
\fancyfoot[E]{}
\fancyfoot[RO]{\thepage}
\setlength{\parindent}{1.25cm} 
<!--\setlength{\parskip}{1.0em} -->

 
\setmainfont{Times New Roman}

\newcommand{\regterm}{\beta_0+{\beta_1}x+\varepsilon}
\newcommand{\regest}{\beta_0+{\beta_1}x+\varepsilon}
\newcommand{\regesti}{\beta_0+{\beta_1}x_i+\varepsilon_i}
\newcommand{\regesp}{\beta_0+{\beta_1}x}
\newcommand{\regespi}{\beta_0+{\beta_1}x_i}
\newcommand{\regefiti}{\hat{\beta_0}+\hat{{\beta_1}x_i}}
\newcommand{\bz}{\beta_0}
\newcommand{\bzh}{\hat{\beta_0}}
\newcommand{\bum}{\beta_1}
\newcommand{\bumh}{\hat{\beta_1}}
\newcommand{\somat}{\sum_{i=1}^{n}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\bh}{\hat{\beta}}


\newtheorem{definition}{Procedimento}

\newenvironment{defbox}{\begin{mdframed}[linecolor=gray!25,roundcorner=12pt,backgroundcolor=gray!10,linewidth=1pt,leftmargin=0cm,rightmargin=0cm,topline=true,bottomline=true,skipabove=12pt]
\begin{definition}
}
{
\end{definition}
\end{mdframed}
}




\begin{titlepage} 
\begin{center}
{\normalsize \bf UNIVERSIDADE ESTADUAL DE MARINGÁ         \\
        CENTRO DE CIÊNCIAS EXATAS                    \\
        CURSO DE ESTATÍSTICA                          \\

      

 }
 
\end{center} 

\vfill
\begin{center}
{\bf \Large AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA PREDIÇÃO EM MODELOS ADITIVOS }\\[1cm]
\end{center}

\vfill

\begin{center}
{\bf  \Large Marco Aurelio Valles Leal    }\\[1cm]
\end{center}

 

\vfill
 
   \hspace{.45\textwidth}
   \begin{minipage}{.5\textwidth}
     
   \end{minipage}


\vfill
\begin{center}
{\normalsize	 Maringá \\ 2022 }\\[0.2cm]
\end{center}
\end{titlepage}




\begin{titlepage} 

\begin{center}
{\bf  \Large MARCO AURELIO VALLES LEAL    }\\[1cm]
\end{center}

\vfill

\begin{center}
{\bf \Large AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA PREDIÇÃO EM MODELOS ADITIVOS}\\[1cm]
\end{center}


 

\vfill
 
   \hspace{.35\textwidth}
   \begin{minipage}{.5\textwidth}
    
       Trabalho de conclusão de curso apresentado como requisito parcial para a obtenção do título de bacharel em Estatística pela Universidade Estadual de Maringá. \\


\begin{tabular}{ll}
Orientador:   &    Profº Drº George Lucas Moraes Pezzot    \\
Coorientador: &    Profº Drº Willian  Luís de Oliveira  
\end{tabular}




     
   \end{minipage}


\vfill
\begin{center}
{\normalsize	 Maringá \\ 2022 }\\[0.2cm]
\end{center}
\end{titlepage}



\begin{titlepage} 


\begin{flushleft}
{ AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA PREDIÇÃO EM MODELOS ADITIVOS}\\[1cm]
\end{flushleft}


\vfill


\begin{flushleft}
{MARCO AURELIO VALLES LEAL    }\\[1cm]
\end{flushleft}

 

\vfill
 
   \hspace{.45\textwidth}
   \begin{minipage}{.5\textwidth}
    
       Trabalho de conclusão de curso apresentado como requisito parcial para a obtenção do título de bacharel em Estatística pela Universidade Estadual de Maringá.

     
   \end{minipage} \\[1cm]

\vfill

\begin{flushleft}
Aprovado em: \_\_\_\_\_\_\_\_/\_\_\_\_\_\_\_\_/\_\_\_\_\_\_\_\_. 
\end{flushleft}

\vfill

\begin{center}
\textbf {BANCA EXAMINADORA}\\[1cm] 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{ Orientador }\\
 Profº Drº George Lucas Moraes Pezzot \\
Universidade Estadual de Maringá
\end{center}

\vfill

\begin{center}
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{Membro da banca} \\
Nome do professor membro da banca \\
Instituição do professor membro da banca
\end{center}

\vfill

\begin{center}
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{Membro da banca} \\
Nome do professor membro da banca \\
Instituição do professor membro da banca
\end{center}


\end{titlepage}


\begin{titlepage}

\begin{center}

\bf RESUMO
\end{center}

\noindent É comum, nas mais diversas áreas, investigar e modelar a relação entre variáveis. O modelo mais simples é denominado modelo de regressão linear simples e assume que a média da variável resposta é modelada como uma função linear das variáveis explicativas, supondo erros aleatórios com média zero, variância constante e não correlacionados. Entretanto, nem sempre a relação existente é perfeitamente linear. Neste contexto, é possível flexibilizar o modelo de regressão linear modelando a dependência da variável resposta com cada uma das variáveis explicativas em um contexto não paramétrico. Esta nova classe de modelos é dita modelos aditivos e mantêm a característica dos modelos de regressão lineares de serem aditivos nos efeitos preditivos. Portanto, este projeto visa apresentar os modelos aditivos, além de técnicas de suavização utilizadas para ajustar modelos no contexto não paramétrico. Por fim, a metodologia é aplicada em dados artificiais (simulados) e em dados reais, dando enfoque à qualidade das predições.

\vspace{1.5cm}

\noindent \textbf{Palavras-chave} : Regressão. Modelo aditivo. Suavizadores.

\end{titlepage}


```{r setup, include=FALSE}
rm(list=ls())

knitr::opts_chunk$set(echo = FALSE,warning= FALSE, message= FALSE,
                      out.width = "100%",fig.align = "center",size ="large",fig.height = 2.5)
```




\clearpage
\tableofcontents

\clearpage
\section{Introdução}


\hspace{1.25cm} A análise de regressão é uma técnica amplamente utilizada na estatística que visa explorar e modelar a relação entre variáveis (MONTGOMERY ET AL.,2012). Resumidamente, a análise de regressão tem como objetivo  descrever uma relação entre uma variável de interesse, chamada de variável resposta ou dependente ($y$) e um conjunto de $p$ variáveis preditoras ou independentes ($x=(x_1,x_2,\ldots,x_p$)), chamadas comumente de covariáveis. Em particular, o modelo  de regressão linear múltipla é aplicado nas mais diversas áreas do conhecimento do conhecimento como, por exemplo, na engenharia, ciências físicas e químicas, economia, ciência biológicas. Tal modelo parte do pressuposto que a variável resposta   está relacionada com as covariáveis  pela seguinte relação linear:
\begin{equation*}
y = \bz + \sum_{j=1}^p\beta_j x_j +\varepsilon
\end{equation*}
onde os parâmetros $\beta_0, \beta_1,\ldots,\beta_p$ são os coeficientes de regressão e o termo $\varepsilon$ é um erro aleatório que deve satisfazer os pressupostos de ter média zero, variância $\sigma^2$ constante e  serem não correlacionados.  O parâmetro $\beta_j$  representa a mudança esperada na resposta $y$ a cada mudança unitária  de $x_j$ quando quando todas as outras variáveis regressoras são constantes e, por isso,  são frequentemente chamados de coeficientes parciais de regressão.  Em geral, os coeficientes de regressão são parâmetros desconhecidos e podem ser estimados utilizando, dentre outras técnicas, o método do mínimos quadrados, por exemplo.

Os modelos de regressão linear múltipla são vistos como modelos empíricos ou funções aproximadas no sentido que a verdadeira função que descreve o relacionamento entre $y$ e $x_1, x_2, \ldots, x_p$ é desconhecida, mas em certos intervalos das variáveis regressoras, o modelo de regressão linear é uma aproximação adequada para a verdadeira função desconhecida. 
Porém, em muitos casos a relação existente entre a variável resposta e cada uma das covariáveis pode não ser linear. Um solução seria acrescentar uma transformação nas variáveis regressoras adotando, por exemplo, o método de transformação de Box-Cox. Contudo, determinar uma transformação que represente a correta relação existente nem sempre é uma tarefa fácil. Outra possibilidade é flexibilizar o modelo de regressão linear, modelando a dependência da variável resposta com cada uma das variáveis explicativas em um contexto não paramétrico (EUBANK, 1999). Esta nova classe de modelos é dita modelos aditivos (HASTIE E TIBSHIRANI,1990). 

Nos modelos aditivos, considera-se que cada uma das covariáveis está relacionada com a variável resposta $y$ através de uma função univariada desconhecida (função suave) não especificada de uma forma paramétrica, ou seja, o componente sistemático é formado por uma soma de funções suaves não especificadas das covariáveis, além de um termo aleatório. Esta nova classe de modelos é dita modelos aditivos e mantêm a característica dos modelos de regressão lineares de serem aditivos nos efeitos preditivos. Os modelos aditivos são um caso particular de uma classe mais geral denominada modelos aditivos generalizados (HASTIE & TIBSHIRANI, 1990), definido por,
$$y = \alpha + \sum_{j=1}^{p} f_j (x_j) + \varepsilon,$$
onde os erros $\varepsilon$ são independentes com média zero e variância constante $\sigma^2$. Cada $f_j(x_j)$ é uma função univariada arbitrária. Observe que um modelo de regressão linear múltiplo é obtido adotando $f_j(x_j) = \beta_jx_j$ na equação acima.

Os modelos aditivos mantêm muitas das boas propriedades dos modelos lineares. Por exemplo, uma das vantagens de modelos lineares é sua simplicidade na interpretação: caso o interesse seja saber como a previsão muda conforme mudanças em $x_j$,  é necessário saber apenas o valor de $\beta_j$. Observe que a função de resposta parcial $f_j$ desempenha esse mesmo papel em um modelo aditivo, mas não  precisa ter necessariamente um comportamento linear. De certa forma, os modelos aditivos podem ser vistos como uma flexibilização do modelo de regressão linear.

As funções $f_j$ devem ser estimadas por meio de suavizadores, que estimam uma tendênica menos variável e descreve sua dependência em relação à variável resposta. Algumas das técnicas mais conhecidas  para obter as estimativas suavizadas são: Bin smoother (REFERENCIA), running mean (REFERENCIA), running line (REFERENCIA), Loess ou Lowess (REFERENCIA), suavizador de Kernels  (REFERENCIA) e  Splines (REFERENCIA). De fato, a "suavização" de cada técnica supracitada depende da escolha (estimativa) do seu respectivo  "parâmetro de suavização".  Em algumas técnicas, por exemplo, dependendo do valor do parâmetro de suavização, pode-se ter uma reta estimada ou até uma interpolação dos dados. Em termos gerais, pode-se dizer que a variância e o viés na estimação/predição do modelo suavizado depende do parâmetro suavizador. Este
problema é, de certa forma, análogo à questão de quantas variáveis preditoras colocar em uma equação de regressão.
Logo, torna-se de suma importância, além do estudo comparativo das técnicas de suavização  no  ajuste em modelos aditivos no contexto não paramétrico, também a estimação do parâmetro suavizador de cada técnica. Adicionalmente, o estudo do desempenho na predição dessas técnicas também é pertinente partindo da hipótese de estudo que nem sempre o melhor ajuste resulta em melhor poder preditivo.
Dito isto, descrevemos a seguir os objetivos de estudo deste trabalho nesta direção.


\subsection{Objetivo Geral}

\hspace{1.25cm} O objetivo deste trabalho é estudar algumas das principais técnicas de estimação do modelo aditivo no contexto não paramétrico. Em particular, serão abordados os suavizadores *Kernel*, *Loess* e *Splines* de regressão considerando apenas uma covariável, e  seus desempenhos  serão comparados com foco principal na predição.

\subsection{Objetivos Específicos}

* Apresentar algumas das principais  técnicas de suavização da literatura para estimar funções não paramétricas presentes nos modelos aditivos, identificando suas principais características;

* Introduzir uma métrica para estimar o parâmetro de suavização de cada técnica;

* Introduzir uma métrica para estimar avaliar  a qualidade de estimação e predição; 

* Realizar um estudo de simulação para comparar a qualidade do ajuste e predição dos modelos em alguns cenários, considerando os modelos aditivos;

* Aplicar a metodologia estudada a um conjunto de dados reais, comparando modelos e técnicas de estimação e predição.

\section{Referencial Teórico}



\hspace{1.25cm} Existem distintas abordagens obter estimativas das funções em métodos de regressão não-paramétricos. Estas estimativas dependem dos próprios dados e de suas observações vizinhas em torno de um dado ponto. Um dos primeiros e mais utilizados métodos de regressão não-paramétrica foi apresentado por Nadaraya-Watson (1964), denominados estimadores tipo núcleo (Kernel), os quais foram aperfeiçoados com os métodos de regressão polinomial local, conhecidos como loess (CLEVELAND, 1979).
Em um modelo com apenas uma covariável, a estimação de $f(x)$ consiste em  ajustes locais, realizando vários ajustes paramétricos por meio de regressão polinomial com pesos (Lowess), considerando os dados mais próximos do ponto onde deve ser feita a estimação da função (DELICADO, 2008). Ainda deve-se escolher de forma apropriada os parâmetros da largura da banda (parâmetro de suavização) e os graus de ajuste polinomiais para obter o melhor ajuste da regressão.

Além destes, tem-se o métodos splines (vide, por exemplo, Reinsch 1967 e Eubank 1999) corresponde em encontrar um estimador para $f(x)$ que minimiza a soma de quadrados dos resíduos, por exemplo (GREEN E SILVERMAN, 1994). Por exemplo, tem-se os splines cúbicos, que  são generalizações de polinômios cúbicos adotados na regressão paramétrica, sendo amplamentes utilizados na literatura por apresentar em geral um bom ajuste.  As técnicas Kernel, Lowess e Splines de regressão (linear e cúbico) são discutidas em mais detalhes na seção a seguir.








 

\hspace{1.25cm}


\section{Metodologia}

\hspace{1.25cm} Neste trabalho, consideraremos o estudo da relação entre uma variável resposta ($y$) e uma variável explicativa ($x$) a partir de um modelo aditivo (HASTIE & TIBSHIRANI, 1990) da seguinte forma
$$y = \alpha +  f (x) + \varepsilon,$$
onde os erros $\varepsilon$ são independentes com média zero e variância constante $\sigma^2$ e  $f(x)$ é uma função univariada arbitrária.

\subsection{Suavizadores e Técnicas de suavização}

\hspace{1.25cm} A função $f(x)$ do componente sistemático pode ser estimada através de um suavizador (\textit{smoother}).
Um suavizador pode ser definido como uma ferramenta para resumo da tendência das medidas $y$ como função de uma (ou mais medidas) $x$. É importante destacar que as estimativas das tendências terão menos variabilidade que as variáveis respostas observadas, o que explica o nome de suavizador para a técnica aplicada (HASTIE & TIBSHIRANI, 1990). Chamamos a estimativa produzida por um suavizador  de “\textit{smooth}”. No caso de uma variável preditora é chamado de suavizador em diagrama de dispersão.

Os suavizadores possuem dois usos principais, sendo o primeiro uso a descrição. Um suavizador em diagrama de dispersão pode ser usado para melhorar a aparência visual do gráfico  de $x$ *versus* $y$ para nos ajudar a encontrar uma tendência nos dados. O segundo uso é de estimar a dependência da esperança de $y$. 

O suavizador mais simples é o caso dos preditores categóricos, como sexo (masculino, feminino), por exemplo. Para suavizar $y$ podemos simplesmente realizar a médias dos valores de $y$ para cada categoria. Este processo captura a tendência de $y$ em $x$. Pode não parecer que simplesmente realizar as médias seja um processo de suavização, mas este conceito é a base para a configuração mais geral, já que a maioria dos suavizadores tenta "imitar" a média da categoria através da média local, ou seja, realizar a média dos valores de $y$ tendo os valores preditores próximos dos valores alvo. Esta média é feita nas vizinhanças em torno do valor alvo.
Nesse caso, tem-se duas decisões a serem tomadas: 


* O quão grande a vizinhança deve ser;

* Como realizar a média dos valores da resposta $y$ em cada vizinhança.


 O tamanho da vizinhança a ser tomada é normalmente expressa em forma de um parâmetro (parâmetro suavizador). Intuitivamente grandes vizinhanças irão produzir estimativas com variância pequena mas potencialmente com um grande viés e inversamente quando adotado vizinhanças pequenas. Portanto, temos uma troca fundamental entre variância e viés estipulada pelo parâmetro suavizador.  

A questão de como realizar a média em uma vizinhança é a questão de qual tipo de suavizador utilizar, pois os suavizadores diferem principalmente pelo jeito de realizar as médias. Algumas das técnicas mais conhecidas para obter as estimativas suavizadas são: Bin smoother (REFERENCIA), running mean (REFERENCIA), running line (REFERENCIA), Loess ou Lowess
(REFERENCIA), suavizador de Kernels (REFERENCIA) e Splines (REFERENCIA), e são discutidas a seguir.

O suavizador bin, também conhecido como regressograma, imita um suavizador categórico, particionando os valores preditores em regiões disjuntas e então realizando a média da resposta em cada região. A estimativa final não tem uma forma bem suavizada, pois é possível ver um salto em cada ponto de corte.

A média móvel (\textit{running mean}) é outra técnica que leva em conta o cálculo da média. É muito comum utilizar uma vizinhança/região de $(2k + 1)$ observações, $k$ para a esquerda e $k$ para a direita de cada observação, onde o valor de $k$ tem um comportamento de troca entre suavidade e qualidade do ajuste.

Um problema comum encontrado na média móvel é o viés. Uma saída é usar pesos para dar mais importância às vizinhanças mais próximas. Uma solução ainda melhor é utilizar a técnica de linha móvel (\textit{running line}), na qual novamente são definidas as vizinhanças para cada ponto, tipicamente os $k$ pontos mais próximos de cada lado. Nesse caso é mais interessante considerar a proporção de pontos em cada vizinhança, ou seja, $w = \dfrac{(2k+1)}{n}$, denominado \textit{span}. Então ajusta-se uma linha de regressão aos pontos de cada região, que é usada para encontrar o valor predito suavizado para o ponto de interesse.

\subsubsection{Loess}

\hspace{1.25cm} Também chamado de \textit{Lowess}, essa técnica pode ser vista como uma linha móvel com pesos locais (\textit{locally weighted running line}). Um suavizador desse tipo, seja denominado $s(x_0)$, usando $k$ vizinhos mais próximos pode ser computada por meio dos seguintes passos:

\begin{defbox}

\begin{itemize}
\item[]
    \item Os $k$ vizinhos próximos de $x_0$ são identificados e denotados por $N_{x_0}$;
    \item É computada a distância do vizinho-próximo mais distante de $x_0$:
    \begin{equation*}
        \Delta(x_0) = max_{x\in N_{x_0}} |x_0 - x|
    \end{equation*}
    \item Os pesos $w_i$ são designados para cada ponto em $N_{x_0}$, usando a função de peso tri-cúbica:
    \begin{equation*}
        w_i = W \Bigg( \dfrac{|x_0 - x_i|}{\Delta (x_0)}\Bigg)
    \end{equation*}
    onde
    \begin{equation*}
        W(u) = 
        \begin{cases}
        (1-u^3)^3, \quad 0 \leq u \leq 1 \\
        0 , \qquad \qquad \mbox{caso contrário}
        \end{cases}
    \end{equation*}
    \item $s(x_0)$ é o valor ajustado no ponto $x_0$ do ajuste de mínimos quadrados ponderados de $y$ para $x$ contidos em $N_{x_0}$ usando os pesos computados anteriormente. (não entendi esta parte, se der tempo, tenta explicar melhor).
    
\end{itemize}

\end{defbox}



As hipóteses em relação ao modelo \textit{Loess} são menos restritivas se comparadas às do modelo de regressão linear, já que assume-se que ao redor de cada ponto $x_0$ o modelo deve ser aproximadamente uma função (linear?) local.

Destaca-se que nessa técnica deve-se ter atenção à escolha do valor do \textit{span}. Um valor muito pequeno faz com que a curva seja muito irregular e tenha variância alta. Por outro lado, um valor muito grande fará com que a curva seja sobre-suavizada, podendo não se ajustar bem aos dados e resultando em perda de informações e viés alto. Nos passos mostrados anteriormente o valor do \textit{span} foi escolhido através do método de vizinhos mais próximos.


\subsubsection{Kernels}

\hspace{1.25cm} Um suavizador kernel usa pesos que decrescem suavemente enquanto se distancia do ponto de interesse $x_0$. Vários métodos podem ser chamados de suavizadores kernel através dessa definição. Porém, na prática, o suavizador kernel representa a sequência de pesos descrevendo a forma da função peso através de uma função densidade com um parâmetro de escala que ajusta o tamanho e a forma dos pesos perto de $x_0$.
Um suavizador Kernel pode ser definido da forma

\begin{equation*}
    \hat{y}_i = \dfrac{\sum_{j=1}^{n} y_i K \Big( \dfrac{x_i - x_j}{b} \Big)  }{\sum_{j=1}^{n} K \Big( \dfrac{x_i - x_j}{b} \Big)}
\end{equation*}
onde $b$ é o tamanho da vizinhança  (parâmetro -- de escala -- suavizador), e $K$ uma função kernel, ou seja, uma função densidade. Existem diferentes escolhas para $K$, geralmente usa-se a densidade de uma Normal, tendo-se assim um kernel Gaussiano.


\subsubsection{Splines de regressão}

\hspace{1.25cm} Um \textit{Spline} pode ser visto como uma função definida por um polinômio por partes. Pontos distintos são escolhidos no intervalo das observações (nós) e um polinômio é definido para cada intervalo, dessa forma é possível modelar com polinômios mais simples as curvas mais complexas. Os \textit{splines} dependem principalmente do grau do polinômio e do número e localização dos nós.
Essa técnica é interessante pois tem uma maior flexibilidade para o ajuste dos modelos em comparação com o modelo de regressão polinomial ou linear e, após a determinação da localização e quantidade de nós, o modelo é de fácil ajuste. Além disso, o \textit{spline} permite modelar um comportamento atípico dos dados, o que não seria possível com apenas uma função.


Existem várias diferentes configurações para um \textit{spline}, mas uma escolha popular é o \textit{spline} cúbico, contínuo  e contendo primeira e segunda derivadas contínuas nos nós. As \textit{splines} cúbicas são as de menor ordem nas quais a descontinuidade nos nós são suficientemente suaves para não serem vistas a olho nu, então a não ser que seja necessário mais derivadas suavizadas, existe pouca justificativa para utilizar \textit{splines} de maior ordem.

Para qualquer grupo de nós, o \textit{spline} de regressão é ajustado a partir de mínimos quadrados em um grupo apropriado de vetores base. Esses vetores são as funções base representando a família do pedaço do polinômio cúbico, com valor dado a partir dos valores observados de $x$.

Uma variação do \textit{spline} cúbico é o \textit{spline} cúbico natural, que contêm a restrição adicional de que a função é linear além dos nós dos limites. Para impor essa condição, é necessário que, nas regiões dos limites: $f''' = f'' = 0$, o que reduz a dimensão do espaço de $K + 4$ para $K$, se há $K$ nós. Então com $K$ nós no interior e dois nos limites, a dimensão do espaço do ajuste é de $K + 2$. 

Quando trabalha-se com \textit{splines}, existe uma dificuldade em escolher a localização e quantidade ideal dos nós, sendo mais importante o número de nós do que sua localização. Salienta-se que incluir mais nós que o necessário pode resultar em uma piora do ajuste do modelo. Existem algumas maneiras para fazer essas escolhas, como por exemplo colocar os nós nos quantis das variável preditora (três nós interiores nos três quartis).

Outro problema é a escolha de funções base para representar o \textit{spline} para dados nós. Suponha que os nós interiores são denotados por $\xi_1 < ... < \xi_k$ e os nós dos limites são $\xi_0$ e $\xi_{k+1}$. Uma escolha simples de funções base para um \textit{spline} cúbico é conhecida como base de séries de potência truncada, que deriva de:

\begin{equation*}
    s(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \sum_{j=1}^{k} \theta_j (x - \xi_j)^3_+
\end{equation*}
onde $(x-\xi_j)_+ = \max(0,x-\xi_j)$. A função $s$ tem as propriedades necessárias: é um polinômio cúbico em qualquer subintervalo $[\xi_j , \xi_{j+1})$, possui duas derivadas contínuas e possui uma terceira derivada.



Observa-se que \textit{splines} de regressão podem ser atrativos devido à sua facilidade computacional, quando os nós são dados. Porém a dificuldade em escolher o número e localização dos nós pode ser uma grande desvantagem da técnica. 



\subsection{Seleção dos parâmetros de suavização}

\hspace{1.25cm} Em regressão, a fim de elaborar boas funções de predição, cria-se um critério para mensurar o desempenho que determinada função predição $g:  \mathbb{R}^d \Rightarrow  \mathbb{R}$, valendo-se, por exemplo, do do risco quadrático (IZBICKI E SANTOS, 2020).

$$R_{pred}(g) = E \left [  (Y - g(X))^2\right].$$
Constata-se que $(X,Y)$ é uma observação nova não utilizada ao se estimar $g$. Sendo assim, melhor será a função de predição $g$, quanto menor for o risco.  Outras funções de perda pode ser empregadas, porém, a função $L(g(X);Y) = (Y - g(X))^2$ (denominada função de perda quadrática), será utilizada neste trabalho.

Para se medir a performance de um estimador baseando-se em seu risco quadrático, criar uma boa função de predição  equivale a encontrar um bom estimador para a função de regressão, sendo que a melhor função de prediçao para $y$ é a função de regressão.
Normalmente, é habitual ajustar distintos modelos para a função de regressão e encontrar qual deles apresenta um melhor poder preditivo, ou seja, aquele que possui o menor risco. Um modelo pode interpolar os dados e, mesmo assim, possuir um baixo poder preditivo (IZBICKI E SANTOS, 2020).

O método de seleção de modelos pretende selecionar uma boa função $g$. Nesse sentido, usa-se o critério do risco quadrático para averiguar a qualidade da função. Assim, escolhe-se uma função $g$ em uma classe de candidatos $G$ que tenha um bom poder preditivo (baixo risco quadrático). Dessa maneira, visa-se evitar modelos que tenham sub ou super-ajuste.

O risco observado, conhecido como erro quadrático médio em relação aos dados, e determinado por,

$$EQM(g) = \frac{1}{n}E \sum^n_{i = 1}\left [  (Y_i - g(X_i))^2\right],$$
onde $g$ é escolhida a fim de minimizar o EQM acima, sendo $\hat{Y}_i = g(X_i)$ o valor predito de $Y_i$ por $g$. Note que a predição é feita para cada observação, após o ajuste do modelo utilizando todos os dados disponíveis. Contudo, este estimador, se usado para realizar a seleção de modelos poder levar um super-ajuste (ajuste perfeito aos dados). 


Usualmente é comum dividir os dados em dois conjuntos, um de treinamento e validação. Utiliza-se os dados de treinamento para estimar a regressão e  avalia-se o erro quadrático médio por meio do conjunto de validação. Este procedimento de divisão é chamado de *data splitting* (REFERENCIA)

Algumas variações podem ser realizadas como o processo *k-fold cross validation* (REFERENCIA) que consiste em dividir a base dados em K conjuntos, no qual o modelo será treinado com K-1 conjuntos restantes onde o conjunto que ficou de fora na primeira vez será empregado como conjunto de teste e o algoritmo faz o rodízio entre os K conjuntos
até que todos os dados sejam vistos como dados de treino e validação. Alternativamente, pode utilizar o *leave-one-out cross validation* (LOOCV) (REFERENCIA), no qual o modelo é ajustado utilizando todas as observações com exceção da i-ésima delas, sendo um caso particular da técnica anterior *k-fold* no caso de $K=n-1$. 

O processo para seleção do melhor parâmetro de suavização, será aquele que prover o menor erro quadrático médio e pode ser obtido por meio do procedimento abaixo: 



\begin{defbox}

\begin{enumerate}
\item[]
\item Supondo  o parâmetro suavizador, denotado por $p$ (tamanho do span ou número de nós), para cada valor possível de seu domínio faça:


\begin{enumerate}

\item Supondo um conjunto de dados de tamanho $n$, para cada observação presente no conjunto de dados faça:

\begin{enumerate}

\item  Divida o conjunto de dados em dados de treino e teste. Considere para os dados de treino todas as observações, exceto i-ésima delas, consequentemente ter-se-a apenas uma observação compondo os dados de teste.

\item  Construa (ajuste) o modelo utilizando apenas os dados de treino.

\item  Utilize o modelo para predizer o valor da resposta ($\hat{y}_i = g(x_i)$)  considerando a observação que compoe os dados de teste e calcule a distância $(y_i - \hat{y}_i)^2$.



\end{enumerate}

\item  Repita o processo (a) $n$ vezes até que todas as observações sejam "vistas" como dados de teste. Ao final, um vetor de tamanho $n$ das diferenças quadráticas. O erro quadrático médio, para o respectivo parâmetro suavizador, será a média: 
$$EQM(p) = \dfrac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2.$$

\end{enumerate}


\item Repita a etapa (1), para todo o domínio do parâmetro suavizador. O melhor parâmetro suavizador $p$ será aquele que gerou o menor EQM, dentre todos os cadidatos possíveis em seu dominio.




\end{enumerate}
\end{defbox}





\subsection{Seleção das técnicas de suavização}

\hspace{1.25cm} Após selecionado o melhor parâmetro de suavização, ter-se-á um parâmetro considerado ótimo, que fornecera o melhor ajuste para um determinado suavizador. Em seguida, consideraremos duas métricas para selecionar a melhor técnica de suavização.
\begin{enumerate}
\item[(i)] $EQM_c$: Erro quadrático médio "completo": Após a escolha do parâmetro de suaviazação e considerando todos as observações do conjuntos dados, ajusta-se o modelo. Para cada observação, considere a predição $\hat{y}_i = g(x_i)$, para todo $i=1,2,\ldots,n$. Em seguida, calcula-se o EQM:

$$EQM_c = \dfrac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2.$$

Destacar-se-á   a melhor técnica de suavização aquela obter o menor $EQM_{c}$ dentre as técnicas comparadas.  

\item[(ii)] $EQM_{loocv}$: Erro quadrático médio "LOOCV": Essa métrica será exatamente o EQM escolhido no **Procedimento 2** durante a escolha do parâmetro suavizador.  Novamente,  a melhor técnica de suavização será aquela que obter o menor $EQM_{loocv}$ dentre as técnicas comparadas.  
\end{enumerate}

De fato,  o menor  $EQM_{c}$ apresentará a técnica com melhor ajuste aos dados e o menor  $EQM_{loocv}$ escolherá aquela de maior poder preditivo.


\clearpage

\section{Resultados e Discussão}

\subsection{Estudo de simulação}

\hspace{1.25cm} Nesta seção, serão utilizadas simulações de dados para gerar situações nas quais possam ser aplicadas as técnicas estudadas, analisando, assim, suas respectivas performances. Para os resultados obtidos, quatro técnicas de suavização serão empregadas, sendo elas: o suavizador de *kernel*, *Loess*, *splines* de regressão de grau 1 e grau 3. Realizar-se-ão ajustes para o  primeiro cenário, considerando distintos parâmetros de suavização para avaliar, visualmente, os comportamentos das curvas em diagramas de dispersão. Em seguida, adotando o método de *data splitting*, *leave one out cross-validation*, encontrar-se-á um parâmetro de suavização que forneça a ocorrência do menor erro quadrático médio possível, comparando, desta forma, os métodos de suavização.

Posteriormente, este procedimento será repetido para cada cenário em mil amostras, contabilizando a quantidade de vezes em que cada técnica apresenta o menor erro quadrático médio. Por exemplo, para o primeiro cenário, será gerado mil amostras aleatórias de tamanho $n$. Para cada amostra será empregado o procedimento acima, salvando seus repectivos erros quadráticos médio. Ao final da simulação, será contabilizado se a ocorrência do erro quadrático médio em cada técnica foi mínima para, por fim, comparar estes resultados e verificar qual técnica obtém o melhor resultado em uma simulação de mil amostras. Vale ressaltar que serão empregados dois comportamentos, uma proveniente de uma função senoidal e outra de uma função Gamma: Cenário 1 e Cenário 2. Ainda, serão gerados nove sub-cenários, valendo-se da combinação de três tamanhos amostrais (150, 250 e 350), em três valores de desvio padrão distintos.


\subsubsection{Cenário 1}


\hspace{1.25cm} Para este cenário, será considerado $X$ uma sequência de 0 a 50 e $Y$, definido pela função
$$ y = 10 + 5sen\pi \dfrac{x}{24} + \varepsilon,$$
  onde $\varepsilon$ é um termo aleatório, normalmente, distribuído com média zero e variância constante. Os tamanhos amostrais utilizados serão iguais a $150,250$ e $350$ e valores de desvio padrão $0.5,1$ e $2$. Na Figura \ref{fig:sim_cenario1}, tem-se o comportamento  dos dados para cada desvio padrão, considerando 350 observações com a curva : $10 + 5sen\pi \dfrac{x}{24}$.