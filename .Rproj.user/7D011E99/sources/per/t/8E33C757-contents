---
output:
  pdf_document:
    toc: false
    toc_depth: 3
    keep_tex: no
    number_sections: true
fontsize: 12pt 
documentclass: article
classoption: a4paper
lang: pt-br
mainfont: Times New Roman
sansfont: Times New Roman
monofont: Times New Roman
indent: true
nocite: '@*'
header-includes:
  - \usepackage{geometry}
  - \geometry{headheight=30pt,left=3cm,bottom=2cm,top=3cm,right=2cm}
  - \usepackage{graphicx}
  - \usepackage[utf8]{inputenc}
  - \usepackage[onehalfspacing]{setspace}
  - \usepackage{fancyhdr}
  - \usepackage{ stmaryrd }
  - \usepackage{eqnarray,amsmath}
  - \usepackage{float}
---

\pagestyle{fancy}
\fancyhead[ROE]{\leftmark}
\fancyhead[LO]{}
\fancyfoot[C]{}
\fancyfoot[E]{}
\fancyfoot[RO]{\thepage}
\setlength{\parindent}{1.25cm} 
<!--\setlength{\parskip}{1.0em} -->


\newcommand{\regterm}{\beta_0+{\beta_1}x+\varepsilon}
\newcommand{\regest}{\beta_0+{\beta_1}x+\varepsilon}
\newcommand{\regesti}{\beta_0+{\beta_1}x_i+\varepsilon_i}
\newcommand{\regesp}{\beta_0+{\beta_1}x}
\newcommand{\regespi}{\beta_0+{\beta_1}x_i}
\newcommand{\regefiti}{\hat{\beta_0}+\hat{{\beta_1}x_i}}
\newcommand{\bz}{\beta_0}
\newcommand{\bzh}{\hat{\beta_0}}
\newcommand{\bum}{\beta_1}
\newcommand{\bumh}{\hat{\beta_1}}
\newcommand{\somat}{\sum_{i=1}^{n}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\bh}{\hat{\beta}}

\begin{titlepage} 
\begin{center}
{\normalsize \bf UNIVERSIDADE ESTADUAL DE MARINGÁ         \\
        CENTRO DE CIÊNCIAS EXATAS                    \\
        CURSO DE ESTATÍSTICA                          \\

      

 }
 
\end{center} 

\vfill
\begin{center}
{\bf \Large AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA ESTIMAÇÃO DE MODELOS ADITIVOS }\\[1cm]
\end{center}

\vfill

\begin{center}
{\bf  \Large Marco Aurelio Valles Leal    }\\[1cm]
\end{center}

 

\vfill
 
   \hspace{.45\textwidth}
   \begin{minipage}{.5\textwidth}
     
   \end{minipage}


\vfill
\begin{center}
{\normalsize	 Maringá \\ 2021 }\\[0.2cm]
\end{center}
\end{titlepage}




\begin{titlepage} 

\begin{center}
{\bf  \Large MARCO AURELIO VALLES LEAL    }\\[1cm]
\end{center}

\vfill

\begin{center}
{\bf \Large AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA ESTIMAÇÃO DE MODELOS ADITIVOS}\\[1cm]
\end{center}


 

\vfill
 
   \hspace{.35\textwidth}
   \begin{minipage}{.5\textwidth}
    
       Trabalho de conclusão de curso apresentado como requisito parcial para a obtenção do título de bacharel em Estatística pela Universidade Estadual de Maringá. \\


\begin{tabular}{ll}
Orientador:   &    Profº Drº George Lucas Moraes Pezzot    \\
Coorientador: &    Profº Drº Willian  Luís de Oliveira  
\end{tabular}




     
   \end{minipage}


\vfill
\begin{center}
{\normalsize	 Maringá \\ 2022 }\\[0.2cm]
\end{center}
\end{titlepage}



\begin{titlepage} 


\begin{flushleft}
{ AVALIAÇÃO DE MÉTODOS NÃO PARAMÉTRICOS PARA ESTIMAÇÃO DE MODELOS ADITIVOS}\\[1cm]
\end{flushleft}


\vfill


\begin{flushleft}
{MARCO AURELIO VALLES LEAL    }\\[1cm]
\end{flushleft}

 

\vfill
 
   \hspace{.45\textwidth}
   \begin{minipage}{.5\textwidth}
    
       Trabalho de conclusão de curso apresentado como requisito parcial para a obtenção do título de bacharel em Estatística pela Universidade Estadual de Maringá.

     
   \end{minipage} \\[1cm]

\vfill

\begin{flushleft}
Aprovado em: \_\_\_\_\_\_\_\_/\_\_\_\_\_\_\_\_/\_\_\_\_\_\_\_\_. 
\end{flushleft}

\vfill

\begin{center}
\textbf {BANCA EXAMINADORA}\\[1cm] 
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{ Orientador }\\
 Profº Drº George Lucas Moraes Pezzot \\
Universidade Estadual de Maringá
\end{center}

\vfill

\begin{center}
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{Membro da banca} \\
Nome do professor membro da banca \\
Intituição do professor membro da banca
\end{center}

\vfill

\begin{center}
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\textbf{Membro da banca} \\
Nome do professor membro da banca \\
Intituição do professor membro da banca
\end{center}


\end{titlepage}


\begin{titlepage}

\begin{center}
\bf RESUMO
\end{center}

É comum, nas mais diversas áreas, investigar e modelar a relação entre variáveis. O modelo mais simples é denominado modelo de regressão linear simples e assume que a média da variável resposta é modelada como uma função linear das variáveis explicativas, supondo erros aleatórios com média zero, variância constante e não correlacionados. Entretanto, nem sempre a relação existente é perfeitamente linear. Neste contexto, é possível flexibilizar o modelo de regressão linear modelando a dependência da variável resposta com cada uma das variáveis explicativas em um contexto não paramétrico. Esta nova classe de modelos é dita modelos aditivos e mantêm a característica dos modelos de regressão lineares de serem aditivos nos efeitos preditivos. Portanto, este projeto visa apresentar os modelos aditivos, além de técnicas de suavização utilizadas para ajustar modelos no contexto não paramétrico. Por fim, a metodologia é aplicada em dados artificiais (simulados) e em dados reais, dando enfoque à qualidade das predições.

Palavras-chave : regressão, modelo aditivo, suavizadores  

\end{titlepage}


```{r setup, include=FALSE}
rm(list=ls())
source(file = "funcoes.R",encoding = "UTF-8")
library(tidyverse)
library(binsmooth)
library(knitr)
library(kableExtra)
library(additive.models)

knitr::opts_chunk$set(echo = FALSE,warning= FALSE, message= FALSE,
                      out.width = "100%",fig.align = "center",size ="large",fig.height = 2.5)
```




\clearpage
\tableofcontents

\clearpage
\section{Introdução}

\hspace{1.25cm} Análise de regressão é uma técnica estatística para investigar e modelar a relação entre variáveis, sendo essa uma técnica amplamente utilizada na estatística. Usualmente, é de interesse apenas uma variável, chamada de variável resposta ou dependente, e desejamos estudar como esta variável depende de um conjunto de variáveis observáveis, chamadas de variáveis explicativas ou independentes. Nesse contexto, os modelos de regressão linear simples e múltipla podem ser utilizados.

Nota-se, porém, que em muitos casos a relação existente entre a variável resposta (média) e cada uma das variáveis explicativas não é perfeitamente linear e determinar uma função que estima a relação correta existente nem sempre é fácil. Uma alternativa é flexibilizar o modelo de regressão linear, modelando a dependência da variável resposta com cada uma das variáveis explicativas em um contexto não paramétrico. Esta nova classe de modelos é dita modelos aditivos e mantêm a característica dos modelos de regressão lineares de serem aditivos nos efeitos preditivos.

As funções suaves do componente sistemático do modelo podem ser estimadas através de um suavizador (\textit{smoother}). Entretanto, algumas técnicas de suavização podem não ser viáveis em alguns problemas. 

Portanto, o objetivo do presente trabalho é apresentar os modelos aditivos e estudar as principais técnicas de suavização existentes utilizadas para ajustar modelos no contexto não paramétrico, em particular os modelos aditivos, apresentando suas principais características e aplicações. Além disso, pretende-se mostrar o ganho na predição, ao se utilizar modelos mais flexiveis, em determinadas situações. O \textit{software} estatístico R (\textit{R Team Core}) será utilizado para a realização de todas as análises do estudo.  


\subsection{Objetivo Geral}

\hspace{1.25cm}O objetivo deste projeto é apresentar os modelos aditivos, uma generalização dos modelos de regressão linear, descrevendo suas principais características e estudar algumas técnicas de estimação do modelo, no contexto não paramétrico.

\subsection{Objetivos Específicos}

* Introduzir os modelos de regressão linear e apresentar os principais métodos de estimação dos parâmetros do modelo assim como as técnicas de diagnóstico e qualidade do ajuste;

* Apresentar técnicas de suavização utilizadas para estimar funções não paramétricas presentes nos modelos aditivos, identificando suas principais características;

* Introduzir os modelos aditivos, especificando sua principais características;

* Apresentar métricas para verificar a qualidade de predição;

* Realizar um estudo de simulação para verificar a qualidade do ajuste dos modelos em alguns cenários, considerando os modelos aditivos;

* Aplicar a metodologia estudada a um conjunto de dados reais, comparando modelos e técnicas de estimação.

\section{Referencial Teórico}




\section{Metodologia}


\subsection{Regressão Linear}

\hspace{1.25cm} Análise de regressão é uma técnica estatística para investigar e modelar a relação entre variáveis. Aplicações de regressão são numerosas e ocorrem em quase todos os campos, incluindo engenharia, as ciências físicas e químicas, economia, ciência biológicas, etc. A regressão tem como objetivo  descrever uma relação entre uma variável de interesse, chamada de variável resposta ou dependente (Y) e um conjunto de variáveis preditoras ou independentes (X), as co-variáveis.

Um modelo de regressão pode ser usado para predição, onde se é esperado que grande parte da variação de Y seja explicado pelas variáveis X, dessa forma obtem-se valores de Y correspondentes a valores de X que não estavam entre os dados. Além disso, através do modelo é possível estimar parâmetros e fazer inferências sobre os mesmos, como testes de hipóteses e intervalos de confiança.


\hspace{1.25cm} O modelo de regressão linear simples, constitui uma tentativa de estabelecer uma equação matemática linear que descreve o relacionamento entre duas variáveis, X (preditora) e Y (resposta). O modelo de regressão linear populacional é definido por:

\begin{equation}
   Y = \regest 
\end{equation}

 O intercepto $\beta_0$ e a inclinação da reta $\beta_1$ são constantes desconhecidas e $\varepsilon$ é um erro aleatório. Pressupõe-se que os erros têm média zero, variância $\sigma^2$ desconhecida e  são não correlacionados, assim, as respostas também não têm relação. Os erros possuem distribuição normal: $\varepsilon_i \sim N(0,\sigma^2)$. A média da distribuição é dada por uma função linear de x:
\begin{equation}
    E(Y/x) = \regesp
\end{equation}

Trata-se a variável regressora x controlada pelos dados e mensurada com um erro, enquanto  a variável resposta Y é aleatória, ou seja, há uma distribuição de probabilidade para y a cada possível valor de x.

Os parâmetros $\bz$ e $\bum$ também são chamados de coeficientes da regressão, tendo uma interpretação simples e muitas vezes útil. A inclinação  $\bum$ é a alteração média da distribuição de Y produzida por uma mudança unitária  da variável X, ou seja, o quanto varia a média de Y para o aumento de uma unidade de X.

Se os dados de X incluem $x=0$, então o intercepto $\bz$ é a média da distribuição da resposta Y quando $x=0$. Porém, se a observação no zero não estiver incluída, $\bz$ não tem interpretação prática e  é chamado de intercepto ou coeficiente linear, pois é o ponto onde a reta regressora corta o eixo y.

 


\subsubsection{Estimação dos parâmetros pelo Métodos dos Mínimos Quadrados}

\hspace{1.25cm} Os parâmetros $\bz$ e $\bum$ são desconhecidos e devem ser estimados usando dados de uma amostra. Suponha que tem-se $n$ pares de dados, $(x_1, y_1), ..., (x_n, y_n)$. Esses dados podem ter sido resultado de um experimento controlado feito especificamente para coletá-los, de um estudo observacional, etc. Uma maneira de estimar esses parâmetros, é utilizando o Método dos Mínimos Quadrados, onde não é necessário conhecer a distribuição dos erros. Esse método tem como objetivo encontrar os valores de $\bz$ e $\bum$ que minimizam a soma dos quadrados dos erros ou desvios do modelo. A equação de regressão linear amostral é definida como:

 \begin{equation}
  y_i=\regesti   \text{, para i=1,2...n}
 \end{equation}
 
 A partir disso tem-se:
$$
\varepsilon_i = y_i - \bz - \bum x_i \Rightarrow
\somat \varepsilon_i ^2 = \somat{(y_i - \bz - \bum x_i)^2} \Rightarrow
S(\bz,\bum)=\somat(y_i-\bz-\bum x_i)^2
$$
Denomina-se os estimadores de mínimos quadrados de $\bz$ e $\bum$, $\bzh$ e $\bumh$, respectivamente. Para obtê-los deve-se encontrar os valores para $\bzh$ e $\bumh$ que minimizam a equação acima, ou seja, deve-se derivar a equação, igualar a zero, isolar os parâmetros e, se a segunda derivada em relação a cada parâmetro for positiva, os valores encontrados são os que minimizam a equação. Sendo assim é fácil verificar que:
 

\begin{minipage}{.3\textwidth}
\begin{flushleft}
\begin{eqnarray}
\bzh & =   &\bar{ y} - \bumh \bar{x}  \nonumber
\end{eqnarray}
\end{flushleft}
\end{minipage}
\hspace{1.5cm}
\begin{minipage}{0.45\textwidth}
\begin{flushleft}
\begin{eqnarray}
\bumh& = &\dfrac{\somat(x_i - \bar{x})(y_i - \bar{y})}{\somat(x_i - \bar{x})^2}  \nonumber
\end{eqnarray}
\end{flushleft}
\end{minipage}

Note que a soma dos quadrados dos desvios da média de x denotamos por
\begin{center}
$S_{xx} = {\somat(x_i - \bar{x})^2}$ 
\end{center}
E a soma dos produtos cruzados dos desvios de x e y  
\begin{center}
$S_{xy} = \somat(x_i - \bar{x})(y_i - \bar{y})$
\end{center}
Dessa forma, é conveniente escrever que

\begin{eqnarray}
\bumh = \dfrac{S_{xy}}{S{xx}} \nonumber
\end{eqnarray}

Então, $\bzh$ e $\bumh$ são os estimadores de mínimos quadrados do intercepto e da inclinação respectivamente. Um resultado importante a respeito da qualidade dos estimadores de mínimos quadrados é o \textbf{Teorema Gauss-Markov}, dizendo que para um modelo de regressão com as suposições $E(\varepsilon) =0 , Var(\varepsilon) = \sigma^2$ e erros independentes, esses estimadores são não viesados e têm variância mínima entre todos os estimadores não viesados que são combinações lineares dos $y_i$, com $Var(\bumh) = \dfrac{\sigma^2}{S_{xx}}$ e $Var(\bzh) = \sigma^2 \Bigg( \dfrac{1}{n} + \dfrac{\bar{x}^2}{S_{xx}} \Bigg)$.

E portanto podemos definir, o modelo de regressão linear simples estimado como sendo
\begin{equation}
    \hat{y} = \bzh + \bumh x
\end{equation}

A diferença entre o valor observado $y_i$ e o correspondente valor estimado é o resíduo, que é importante para verificar a adequação do modelo. Matematicamente, o i-ésimo resíduo é
$$
e_i = y_i - \hat{y_i} = y_i -  (\bzh + \bumh x), \quad i=1,...,n \nonumber
$$


 \subsubsection{Estimação de Máxima Verossimilhança}
  
\hspace{1.25cm} É possível mostrar que os estimadores de máxima verossimilhança para os parâmetros coincidem com os estimadores de mínimos quadrados quando os erros são independentes e normalmente distribuídos.

O modelo é $y_i = \bz +  \bum x_i + \varepsilon$ e $\varepsilon \sim N(0, \sigma^2)$, em que $\varepsilon_i$  é independente de $\varepsilon_j$ para $i \neq j$. E a densidade dos erros é dada por:
            \begin{equation*}
                f(\varepsilon_i) = \dfrac{1}{\sqrt{2\pi}\sigma} exp \left( -\dfrac{1}{2\sigma^2} \varepsilon_i ^2 \right)
            \end{equation*}

Tem-se que a função de verossimilhança é
            \begin{equation*}
                L(\bz,\bum,\sigma^2|\varepsilon_1,..., \varepsilon_n) = \prod_{i = 1}^{n} f(\varepsilon_i) = \dfrac{1}{(2\pi)^{n/2}\sigma^n}exp \left( -\dfrac{1}{2\sigma^2} \sum_{i}^n\varepsilon_i^2 \right)
            \end{equation*}
            
 Como $\varepsilon_i = y_i - \bz - \bum x_1$, tem-se 
        \begin{equation*}
        L(\bz,\bum,\sigma^2|\varepsilon_1,..., \varepsilon_n) = \dfrac{1}{(2\pi)^{n/2}\sigma^n}exp \left( -\dfrac{1}{2\sigma^2} \sum_{i=1}^n(y_i - \bz - \bum x_i)^2 \right)
    \end{equation*}
    
A função log-verossimilhança pode ser vista como
        \begin{equation*}
            lnL(y, X, \beta, \sigma^2) = -\dfrac{n}{2}ln(2\pi) - nln(\sigma) - \dfrac{1}{2\sigma^2}\sum_{i=1}^n(y_i - \bz - \bum x_i)^2
        \end{equation*}
        

 Para um valor fixo de $\sigma$,a função acima é maximizada quando o termo $\sum_{i=1}^n(y_i - \bz - \bum x_i)^2$ é minimizado, ou seja, os estimadores para o $\bz$ e $\bum$ são iguais aos estimadores de mínimos quadrados. Esses estimadores são não viesados, têm variância mínima comparado com todos os outros estimadores não viesados, são consistentes e são um conjunto de estatísticas suficientes.



\subsubsection{Estimação de $\sigma^2$}

\hspace{1.25cm} Além de estimar $\bz$ e $\bum$, um estimador de $\sigma^2$ é necessário para testar hipóteses e construir intervalos pertinentes ao modelo de regressão. O ideal seria que o estimador não dependesse da adequação do modelo estimado. Porém isso só é possível quando existem várias observações de y para pelo menos um valor de x, ou quando uma informação a respeito de $\sigma^2$ está disponível.Quando essa abordagem não pode ser usada, o estimador de $\sigma^2$ é obtido através da soma do quadrado dos erros (ou resíduos).

\begin{eqnarray*}
SQE = \somat e_i^2 = \somat (y_i - \hat{y_i})^2& =& \somat (y_i - (\bzh + \bumh x_i))^2  \\ 
= \somat y_i^2 - n\bar{y}^2 - \bumh S_{xy} &= &\somat (y_i - \bar{y})^2 - \bumh S_{xy} = S{yy} - \bumh S_{xy} 
\end{eqnarray*}

SQE é um estimador viesado para $\sigma^2$, pois $E(SQE) = \sigma^2 (n-2)$. Então, um estimador não viesado para $\sigma^2$ é dado pelo quadrado médio dos erros
\begin{center}
    $QME = \hat{\sigma}^2 = \dfrac{SQE}{n-2}$
\end{center}
A raiz quadrada de $\hat{\sigma}^2$ é chamada de erro padrão da regressão, tendo as mesmas unidades da variável resposta Y. Devido à $\hat{\sigma}^2$ depender da soma do quadrado dos erros, qualquer violação das suposições dos erros do modelo ou qualquer especificação errada da forma do modelo pode tornar $\hat{\sigma}^2$ inutilizável como estimador de $\sigma^2$.

O estimador de máxima verossimilhança de $\sigma^2$ é dado por,

\begin{equation*}
\Tilde{\sigma}^2 = \dfrac{\sum_{i=1}^n(y_i - \bz - \bum x_i)^2 }{n} 
\end{equation*} 

Note que apesar de ser um estimador viesado, conforme n se torna grande o suficiente, este se torna assintóticamente não viesado.

\subsubsection{Teste de Hipóteses para $\bz$ e $\bum$}


\hspace{1.25cm} Seja, $\varepsilon_i \sim N^{iid}(0,\sigma^2)$, logo temos que  $Y_i \sim N(\regespi,\sigma^2)$. Queremos testar a hipótese de $\bum$ ser igual a uma constante qualquer, que chamaremos de $\beta_{10}$, já que por meio desse teste temos um indicativo de associação linear entre as variáveis Y e X. Então, definimos as hipóteses $H_0 : \bum =\beta_{10}  \times H_1 : \bum \neq \beta_{10}$.
Temos que $\bumh \sim N\left(\bum,\frac{\sigma^2}{ \sum( x_i- \bar{x}^2)} \right)$. Assim, 

$$Z_0 = \frac{\bumh-\beta_{10}}{\sqrt{\sigma^2/S_{xx}}} \sim N(0,1)$$

Como normalmente $\sigma^2$ é desconhecido, não podemos usar $Z_0$, como sabemos que QME é um estimador não viciado para $\sigma^2$, conseguimos definir a seguinte a estatística:
$$t_0 = \frac{\bumh-\beta_{10}}{\sqrt{QME/\sum{(x_i -\bar{x})^2}}} \sim t_{n-2}$$

Logo calculamos a estatística $t_0$ e a comparamos com ponto $t_{\alpha,n-2}$ da distribuição T de Student, ou seja, a regra de decisão pode ser definida e rejeitamos a hipótese nula quando $|t_0| > t_{\alpha/2,n-2}$.

Quando estamos interessados em testar o parâmetro $\bz$, usamos o mesmo processo, definimos as hipóteses a respeito de $\bz$ e a estatística do teste e, por fim, comparamos se esta estatística pertence a uma região rejeição ou de não rejeição para hipótese nula $H_0$.

Quando estamos interessados em avaliar se  existe uma boa "correlação" entre a resposta e a variável explicativa, o teste de significância da regressão é um caso específico, onde definimos a hipótese nula testando $\bum$ igual a zero, ou seja:  
$$
\left\{ \begin{array}{l}
H_0 : \bum = 0  \\
H_1 : \bum \neq 0  
\end{array}\right.
$$

Logo se não rejeitamos a hipótese nula, $H_0: \bum =0$, implica dizer que não há relação linear entre x e y. Uma outra interpretação seria dizer que, o valor de x pode ser suficientemente pequeno e sendo assim, afeta muito pouco a variância de y, e que o melhor estimador de $y$ para qualquer $x$ seria, $\hat{y}=\bar{y}$. Contudo se rejeitarmos a hipótese nula, podemos dizer que x e um valor suficientemente grande e que exerce influência significativa sob a variabilidade em y. Logo mais, a estatística do teste, quando $\beta_{10} = 0$, é definida :

$$t_0 = \frac{\bumh}{\sqrt{QME/\sum{(x_i -\bar{x})^2}}} \sim t_{n-2}$$

E rejeitamos a hipótese nula quando $|t_0| > t_{\alpha/2,n-2}$.

\subsubsection{Análise de Variância}

\hspace{1.25cm}  A análise de variância é baseada no particionamento da variância total da variável resposta y, ou seja, a variação total é decomposta na soma de duas fontes de variação distintas, uma fonte de variação proveniente das observações em relação ao valores ajustados ($Y_i-\hat{Y_i}$), e a outra dos valores ajustados em relação à média ($\hat{Y_i}-\bar{Y_i}$). Portanto a tabela da análise de variância é derivada a partir deste particionamento assim como a estatística F utilizada no teste.
 \begin{eqnarray}
 SQT &=& \sum(\hat{y_i}-\bar{y})^2+\sum(y_i-\hat{y_i})^2\\
 SQT &=& SQR+SQE
 \end{eqnarray}
Na equação (6),  a soma de quadrado total (SQT) mensura a variabilidade total da variável dependente, a soma de quadrado da regressão (SQR) mensura a variabilidade da variável dependente explicada pelo modelo e a soma de quadrados dos erros (SQE) mensura a variabilidade residual, não explicada pelo modelo. Esta equação é chamada de identidade fundamental da análise de variância para o modelo de regressão. Ainda podemos definir a soma de quadrados da regressão como $SQR =\bumh S_{xy}$, onde  $S_{xy}=\sum y_i(x_i - \bar{x})$.
Ainda os quadrados médios podem ser definidas provenientes da divisão da soma de quadrados pelos seus respectivos graus de liberdade. Portanto temos que os quadrados médios da regressão (QMR) e os quadrados médios dos erros (QME) podem ser definidos como:

 \begin{itemize}
     \item $QMR=\dfrac{SQR}{1}=SQR=\sum_{i=1}^n(\widehat{Y}_i-\bar{Y})^2$, é o quadrado médio da regressão.
     
     \item $QME=\dfrac{SQE}{n-2}=\dfrac{\displaystyle\sum\limits_{i=1}^n(Y_i-\widehat{Y}_i)^2}{n-2}$, é quadrado médio dos erros (Resíduos).

 \end{itemize}
 

 E por fim, para definirmos a estatística do teste precisamos de alguns resultados sabemos que $\frac{(n-2)SQE}{\sigma^2} \sim \chi^2_{n-2}$ e  $\frac{SQR}{\sigma^2} \sim \chi^2_{1}$. Ainda para que estatística definida a seguir seja válida, assumimos que a soma de quadrados dos erros (SQE) e a soma de quadrados da regressão SQR são independentes. Portanto sabemos que, o resultado da divisão de duas qui-quadrados nos fornece uma distribuição F, ou seja :
  $$F_0=\frac{\frac{SQR}{\sigma^2}}{\frac{(n-2)SQE}{\sigma^2}}= \frac{QMR}{QME}\sim F_{1,n-2}$$
  
   Ainda podemos verificar que $E(SQE)=\sigma^2$ e  $E(SQR)=\sigma^2+\beta^2_1S_{xx}$. Sob $H_0$ verdadeira, se o valor observado de $F_0$ é significantemente grande então provavelmente a inclinação $\beta1 \neq 0$, logo teremos indícios para rejeitar a hipótese nula. Logo rejeitamos a hipótese nula com um nível de significância $\alpha$ quando $F_0>F_{1-\alpha,1,n-2}$ , ou seja, quando o valor calculado da estatística $F_0$ for maior do que o quantil 1-$\alpha$ da distribuição F com 1 grau de liberdade para o numerador e n-2 graus de liberdade para o denominador. A seguir a tabela 1, nos mostra um resumo do procedimento para análise de variância:
  
  \begin{table}[hbt!]
      \centering
      \caption{Análise de Variância para o teste de significância da regressão linear simples}
      \resizebox{\textwidth}{!}{%
      \begin{tabular}{lllll}
      \hline
           Fonte de Variação & Soma dos Quadrados & Graus de Liberdade & Média dos Quadrados & $F_0$  \\
           \hline
           Regressão         & $SQR=\bumh S_{xy}$     & 1                  &  QMR                & $\frac{QMR}{QME}$ \\
           Resíduos          & $SQE=SST-\bumh S_{xy}$ & n-2 & QME          &   \\
           Total             & SST                    & n-1 &              &  \\
         \hline
      \end{tabular}
      }
      \label{tab:my_label}
  \end{table}


\subsubsection{Regressão Linear Múltipla}

Como pode ser visto anteriormente, o modelo de regressão linear simples, com uma variável explicativa, aplica-se a várias situações. Entretanto, diversos problemas envolvem dois ou mais regressores influenciando o comportamento da variável resposta (dependente) $y$.

Suponha que o rendimento, em libras, de conversão em um processo químico dependa da temperatura e da concentração do catalisador. Um modelo de regressão múltipla que talvez descreva esse relacionamento é dado por
$$
y = \bz + \bum x_1 + \beta_2 x_2 + \varepsilon,
$$
onde $y$ é o rendimento, $x_1$ a temperatura e $x_2$ a concentração do catalisador. Esse é um modelo de regressão linear múltipla com duas variáveis regressoras. O termo linear é usado porque a equação é uma função linear dos parâmetros desconhecidos $\bum , \bz$ e $\beta_2$. Outro ponto a ser destacado é que esse modelo forma um plano no espaço tridimensional de $y, x_1$ e $x_2$. O parâmetro $\bz$ é o intercepto do plano; se os dados incluem $x_1 = x_2 = 0$, então $\bz$ é a média de $y$ quando $x_1 = x_2 = 0$. Caso contrário, $\bz$ não tem interpretação prática. Já $\bum$ indica a mudança na resposta média $y$ a cada mudança unitária de $x_1$ quando $x_2$ é constante. O parâmetro $\beta_2$ indica a mudança na resposta média a cada unidade de mudança em $x_2$, quando $x_1$ é constante. Em geral, a resposta $y$ pode estar relacionada a $k$ regressores ou variáveis preditoras. O modelo

\begin{equation*}
y = \bz + \bum x_1 + \beta_2 x_2 +...+ \beta_k x_k + \varepsilon
\end{equation*}
é chamado de modelo de regressão linear múltipla com $k$ regressores. Os parâmetros $\beta_j, j = 0,1,...,k$ são os coeficientes de regressão. Esse modelo descreve um hiperplano no espaço k-dimensional das variáveis regressoras $x_j$. O parâmetro $\beta_j$  representa a mudança esperada na resposta $y$ a cada mudança unitária  de $x_j$ quando quando todas as outras variáveis regressoras $x_i$, com $(i \neq j)$, são constantes. Por isso, os parâmetros $\beta_j$ são frequentemente chamados de coeficientes parciais de regressão.

Os modelos de regressão linear múltipla são frequentemente usados como modelos empíricos ou funções aproximadas. Isso é, a verdadeira função que descreve o relacionamento entre $y$ e $x_1, x_2, ..., x_k$ é desconhecida, mas em certos intervalos das variáveis regressoras, o modelo de regressão linear é uma aproximação adequada para a verdadeira função desconhecida. 

Modelos que tem uma estrutura mais complexa ainda podem ser analisados a partir de técnicas de regressão linear múltipla. Como exemplo, consideremos o modelo polinomial cúbico:

\begin{equation*}
y = \bz + \bum x_1 + \beta_2 x^2 + \beta_3 x^3 + \varepsilon
\end{equation*}

Se considerarmos $x_1 = x, x_2 = x^2$ e $x_3 = x^3$ temos

\begin{equation*}
y = \bz + \bum x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon
\end{equation*}

que é um modelo de regressão linear múltipla com 3 variáveis regressoras.

Isso também é válido para modelos que incluem efeitos de interação:

\begin{equation*}
y = \bz + \bum x_1 + \beta_2 x_2 + \beta_{12} x_1x_2 + \varepsilon
\end{equation*}

Considere $x_3 = x_1x_2$ e $\beta_3 = \beta_{12}$, temos

$$
y = \bz + \bum x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon
$$

que é um modelo de regressão linear.

Pode ser que o gráfico do modelo (superfície gerada) não seja linear. Mas, em geral, qualquer modelo de regressão linear que é linear nos parâmetros ($\beta$'s) é um modelo de regressão linear, independente da superfície gerada. Vale ressaltar que os modelos de regressão linear múltipla são frequentemente usados como modelos empíricos ou funções aproximadas. 


\subsubsection{Adequação do modelo - enfoque inferencial}

\hspace{1.25cm}  As suposições mais importantes que foram vistas neste estudo sobre análise de regressão foram:
  
  \begin{itemize}
      \item A relação entre a resposta $y$ e os regressores é linear;
      \item A média do termo do erro $\epsilon$ é zero;
      \item O termo do erro $\epsilon$ tem variâcia constante $\sigma^2$;
      \item Os erros não são correlacionados;
      \item Os erros são normalmente distribuídos.
  \end{itemize}
  
  Juntas, as duas última suposições implicam que os erros são variáveis aleatórias independentes. A última suposição é requisito para testar hipóteses e construir intervalos de confiança.No enfoque inferencial, deve-se sempre considerar essas suposições como duvidosas e fazer análises para examinar a adequação do modelo.  Violações graves das suposições podem gerar um modelo instável, ou seja, uma amostra diferente pode levar a um modelo completamente diferente com conclusões opostas.Neste momento serão apresentados métodos para diagnosticar as violações das suposições básicas da regressão. Esses métodos são baseados principalmente no estudo dos resíduos do modelo.
Os resíduos foram definidos anteriormente como, $e_i = y_i - \hat{y}_i,  \qquad i = 1,2,...,n$ onde $y_i$ é a observação e $\hat{y}_i$ é o valor estimado correspondente. É uma medida de variabilidade na variável resposta não explicada pelo modelo de regressão. Também é conveniente pensar nos resíduos como os valores observados ou realizados dos erros do modelo. Assim, quaisquer desvios das suposições sobre os erros devem aparecer nos resíduos. 

A plotagem dos resíduos é uma ferramenta muito eficaz para investigar até que ponto o modelo de regressão se encaixa nos dados e verificar as suposições para o modelo de regressaõ. Às vezes, é útil trabalhar com resíduos dimensionados, que ajudam a encontrar observações que são outliers ou valores extremos, isto é, observações que estão separadas de alguma forma do resto dos dados.



\subsection{Modelo Aditivo}


\hspace{1.25cm} Uma das mais populares e úteis ferramentas em análise de dados é o modelo de regressão linear. Se a dependência de de Y em X é linear ou quase linear, então o modelo de regressão linear é útil. Caso esta dependência seja de longe linear, não iremos querer resumi-la em uma linha reta. Poderíamos adicionar um termo quadrático, mas geralmente é dificultoso encontrar a forma mais apropriada. Nesse contexto, tem-se os modelos aditivos, que podem ser vistos como uma flexibilização do modelo de regressão linear, considerando a dependência da variável resposta com cada uma das variáveis explicativas em um contexto não paramétrico. 

Para isto, considera-se que cada uma das variáveis explicativas está relacionada à média da variável resposta Y através de uma função univariada desconhecida (função suave) não especificada de uma forma paramétrica, ou seja, o componente sistemático é formado por uma soma de funções suaves não especificadas das variáveis explicativas. Esta nova classe de modelos é dita modelos aditivos e mantêm a característica dos modelos de regressão lineares de serem aditivos nos efeitos preditivos. Os modelos aditivos são um caso particular de uma classe mais geral denominada modelos aditivos generalizados (HASTIE & TIBSHIRANI, 1990). Um modelo aditivo é definido por

\begin{equation*}
 y = \alpha + \sum_{j=1}^{p} f_j (X_j) + \epsilon,
\end{equation*}
onde os erros $\epsilon$ são independentes, com $E(\epsilon) = 0$ e $var(\epsilon) = \sigma^2$. As $f_js$ são funções univariadas arbitrárias, uma para cada preditor. Essas funções do componente sistemático podem ser estimadas através de um suavizador (\textit{smoother}), uma ferramenta que representa a tendência da variável resposta como função das covariáveis disponíveis. No caso em que apenas uma covariável está disponível para predizer a variável resposta, um suavizador do gráfico de dispersão é frequentemente utilizado. 


Os modelos aditivos mantêm muitas das boas propriedades dos modelos lineares, porém são mais flexíveis. Uma das vantagens de modelos lineares é sua simplicidade na interpretação: caso o interesse seja em saber como a previsão muda conforme mudanças em $x_j$, só é necessário saber o valor de $\beta_j$. A função de resposta parcial $f_j$ desempenha esse mesmo papel em um modelo aditivo.

\subsection{Suavizadores}

\hspace{1.25cm} Um suavizador (\textit{smoother}) pode ser definido como uma ferramenta para resumo da tendência das medidas Y como função de uma ou mais medidas X. É importante destacar que as estimativas das tendências terão menos variabilidade que as variáveis respostas observadas, o que explica o nome de suavizador para a técnica aplicada (HASTIE & TIBSHIRANI, 1990). Não assume forma rígida entre a dependência de Y e de suas medidas preditoras $(X_1, X_2, ..., X_p)$. Chamamos a estimativa produzida por um suavizador (\textit{smoother}) de “\textit{smooth}”. O caso de uma variável preditora é chamado de suavizador em diagrama de dispersão.

Os suavizadores possuem dois usos principais, sendo o primeiro uso a descrição. Um gráfico de dispersão suavizador pode ser usado para melhorar a aparência visual de um gráfico de dispersão de Y vs X, para nos ajudar a encontrar uma tendência no gráfico. O segundo uso é de estimar a dependência da esperança de Y com o seus preditores e nos servem como blocos de construção para os modelos aditivos.

O suavizador mais simples é o caso dos preditores categóricos, como sexo (masculino, feminino). Para suavizar Y podemos simplesmente realizar a médias dos valores de Y para cada categoria. Este processo captura a tendência de Y em X. Pode não parecer que simplesmente realizar as médias seja um processo se suavização, mas este conceito é a base para a configuração mais geral, já que a maioria dos suavizadores tenta imitar a média da categoria através da média local, ou seja, realizar a média dos valores de Y tendo os valores preditores próximos dos valores alvo. Esta média é feita nas vizinhanças em torno do valor alvo.

Nesse caso, tem-se duas decisões a serem tomadas:

• Como realizar a média dos valores da reposta em cada vizinhança;

• O quão grande esta vizinhança deve ser.

A questão de como realizar a média em uma vizinhança é a questão de qual tipo de suavizador utilizar, pois os suavizadores diferem principalmente pelo jeito de realizar as médias. O tamanho da vizinhança a ser tomada é normalmente expressa em forma de um parâmetro. Intuitivamente grandes vizinhanças irão produzir estimativas com variância pequena mas potencialmente com um grande viés e inversamente quando adotado vizinhanças pequenas. Portanto temos uma troca fundamental entre variância e viés estipulada pelo parâmetro suavizador. Este problema é análogo à questão de quantas variáveis preditoras colocar em uma equação de regressão.

\subsubsection{Técnicas de suavização}

\hspace{1.25cm} Entre as principais técnicas de suavização estão a regressão paramétrica, vista anteriormente e que consiste em uma linha de regressão estimada por mínimos quadrados. Essa abordagem pode ou não ser apropriada para dado conjunto de dados.

O suavizador bin, também conhecido como regressograma, imita um suavizador categórico, particionando os valores preditores em regiões disjuntas e então realizando a média da resposta em cada região. A estimativa final não tem uma forma bem suavizada, pois é possível ver um salto em cada ponto de corte.

A média móvel (\textit{running mean}) é outra técnica que leva em conta o cálculo da média. É muito comum utilizar uma vizinhança/região de $(2k + 1)$ observações, $k$ para a esquerda e $k$ para a direita de cada observação, onde o valor de $k$ tem um comportamento de troca entre suavidade e qualidade do ajuste.

Um problema comum encontrado na média móvel é o viés. Uma saída é usar pesos para dar mais importância às vizinhanças mais próximas. Uma solução ainda melhor é utilizar a técnica de linha móvel (\textit{running line}), na qual novamente são definidas as vizinhanças para cada ponto, tipicamente os $k$ pontos mais próximos de cada lado. Nesse caso é mais interessante considerar a proporção de pontos em cada vizinhança, ou seja, $w = \dfrac{(2k+1)}{n}$, denominado \textit{span}. Então ajusta-se uma linha de regressão aos pontos de cada região, que é usada para encontrar o valor predito suavizado para o ponto de interesse.

\subsubsection{Loess}

\hspace{1.25cm} Também chamado de \textit{Lowess}, essa técnica pode ser vista como uma linha móvel com pesos locais (\textit{locally weighted running line}). Um suavizador desse tipo, seja denominado $s(x_0)$, usando $k$ vizinhos mais próximos pode ser computada por meio dos seguintes passos:

\begin{itemize}
    \item Os $k$ vizinhos próximos de $x_0$ são identificados e denotados por $N(X_0)$;
    \item É computada a distância do vizinho-próximo mais distante de $x_0$:
    
    \begin{equation*}
        \Delta(x_0) = max_{N_{x_0}} |X_0 - x_i|
    \end{equation*}
    
    \item Pesos $w_i$ são designados para cada ponto $(N_{x_0}$, usando a função de peso tri-cúbica:
    
    \begin{equation*}
        W \Bigg( \dfrac{|X_0 - x_i|}{\Delta (x_0)}\Bigg)
    \end{equation*}
    
    Onde
    
    \begin{equation*}
        W(u) = 
        \begin{cases}
        (1-u^3)^3, \quad 0 \leq u \leq 1 \\
        0 , \qquad \qquad \mbox{caso contrário}
        \end{cases}
    \end{equation*}
    
    \item $s(x_0)$ é o valor ajustado no ponto $x_0$ do ajuste de mínimos quadrados ponderados de $y$ para $x$ contidos em $N(X_0)$ usando os pesos computados anteriormente.
    
\end{itemize}

As hipóteses em relação ao modelo \textit{Loess} são menos restritivas se comparadas às do modelo de regressão linear, já que assume-se que ao redor de cada ponto $x_0$ o modelo deve ser aproximadamente uma função local.

Destaca-se que nessa técnica deve-se ter atenção à escolha do valor do \textit{span}. Um valor muito pequeno faz com que a curva seja muito irregular e tenha variância alta. Por outro lado, um valor muito grande fará com que a curva seja sobre-suavizada, podendo não se ajustar bem aos dados e resultando em perda de informações e viés alto. Nos passos mostrados anteriormente o valor do \textit{span} foi escolhido através do método de vizinhos mais próximos.


\subsubsection{Kernels}

\hspace{1.25cm} Um suavizador kernel usa pesos que decrescem suavemente enquanto se distancia do ponto de interesse $x_0$. Vários métodos podem ser chamados de suavizadores kernel através dessa definição. Porém na prática, o suavizador kernel representa a sequência de pesos descrevendo a forma da função peso através de uma função densidade com um parâmetro de escala que ajusta o tamanho e a forma dos pesos perto de $x_0$.
Um suavizador Kernel pode ser definido da forma

\begin{equation*}
    \hat{y}_i = \dfrac{\sum_{j=1}^{n} y_i K \Big( \dfrac{x_i - x_j}{b} \Big)  }{\sum_{j=1}^{n} K \Big( \dfrac{x_i - x_j}{b} \Big)}
\end{equation*}

Onde $b$ é o tamanho da vizinhança  (parâmetro de escala), e $K$ uma função kernel, ou seja, uma função densidade. Existem diferentes escolhas para $K$, geralmente usa-se a densidade de uma Normal, tendo-se assim um kernel Gaussiano.


\subsubsection{Splines}

\hspace{1.25cm} Um \textit{Spline} pode ser visto como uma função definida por um polinômio por partes. Pontos distintos são escolhidos no intervalo das observações (nós) e um polinômio é definido para cada intervalo, dessa forma é possível modelar com polinômios mais simples as curvas mais complexas. Os \textit{splines} dependem principalmente do grau do polinômio e do número e localização dos nós.

Essa técnica é interessante pois tem uma maior flexibilidade para o ajuste dos modelos em comparação com o modelo de regressão polinomial ou linear e, após a determinação da localização e quantidade de nós, o modelo é de fácil ajuste. Além disso, o \textit{spline} permite modelar um comportamento atípico dos dados, o que não seria possível com apenas uma função.

\subsubsection{Splines de regressão}

\hspace{1.25cm} Existem várias diferentes configurações para um \textit{spline}, mas uma escolha popular é o \textit{spline} cúbico, contínuo  e contendo primeira e segunda derivadas contínuas nos nós. As \textit{splines} cúbicas são as de menor ordem nas quais a descontinuidade nos nós são suficientemente suaves para não serem vistas a olho nu, então a não ser que seja necessário mais derivadas suavizadas, existe pouca justificativa para utilizar \textit{splines} de maior ordem.

Para qualquer grupo de nós, o \textit{spline} de regressão é ajustado a partir de mínimos quadrados em um grupo apropriado de vetores base. Esses vetores são as funções base representando a família do pedaço do polinômio cúbico, com valor dado a partir dos valores observados de $X$.

Uma variação do \textit{spline} cúbico é o \textit{spline} cúbico natural, que contêm a restrição adicional de que a função é linear além dos nós dos limites. Para impor essa condição, é necessário que, nas regiões dos limites: $f''' = f'' = 0$, o que reduz a dimensão do espaço de $K + 4$ para $K$, se há $K$ nós. Então com $K$ nós no interior e dois nos limites, a dimensão do espaço do ajuste é de $K + 2$. 

Quando trabalha-se com \textit{splines}, existe uma dificuldade em escolher a localização e quantidade ideal dos nós, sendo mais importante o número de nós do que sua localização. Salienta-se que incluir mais nós que o necessário pode resultar em uma piora do ajuste do modelo. Existem algumas maneiras para fazer essas escolhas, como por exemplo colocar os nós nos quantis das variável preditora (três nós interiores nos três quartis).

Outro problema é a escolha de funções base para representar o \textit{spline} para dados nós. Suponha que os nós interiores são denotados por $\xi_1 < ... < \xi_k$ e os nós dos limites são $\xi_0$ e $\xi_{k+1}$. Uma escolha simples de funções base para um \textit{spline} cúbico é conhecida como base de séries de potência truncada, que deriva de:

\begin{equation*}
    s(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \sum_{j=1}^{K} \theta_j (x - \xi_j)^3_+
\end{equation*}

Onde $s$ tem as propriedades necessárias: é um polinômio cúbico em qualquer subintervalo $[\xi_j , \xi_{j+1})$, possui duas derivadas contínuas e possui uma terceira derivada.

A função $s$ pode ser escrita como uma combinação linear de $K + 4$ funções base $P_j (x): P_1 (x) = 1, P_2 (x) = x$ e assim por diante. Cada função deve satisfazer as três condições e ser linearmente independente para ser considerada uma base. Então fica claro que são necessários $(K + 4)$ parâmetros para representar um \textit{spline} cúbico.

As funções base B-\textit{spline} fornecem uma alternativa numericamente superior para a base de séries de potência truncada. A ideia principal é de que qualquer função base $B_j (x)$ é diferente de zero em um intervalo de no máximo cinco diferentes nós. Fica claro que as funções $B_j$ são \textit{splines} cúbicas, e são necessárias $K + 4$ delas para abranger o espaço.

A partir disso, observa-se que \textit{splines} de regressão podem ser atrativos devido à sua facilidade computacional, quando os nós são dados. Porém a dificuldade em escolher o número e localização dos nós pode ser uma grande desvantagem da técnica.

\subsubsection{Backfitting}

No contexto dos modelos aditivos, quando mais de uma covariável está disponível para predizer a resposta, frequentemente utiliza-se o algoritmo retroajuste (HASTIE & TIBSHIRANI, 1987; BUJA et. al., 1989; HASTIE & TIBSHIRANI, 1990), para estimar cada função suave em um cenário não paramétrico, além do intercepto. A ideia do geral do algoritmo pode ser dado pelos seguintes passos:

1. Adotam-se os valores iniciais $\beta_0^{(0)} = \sum_{i=1}^n$ e $s_1^{0}(.) = s_2^{0}(.) = ... = s_p^{0}(.) = 0$;
2. Aplica-se um ciclo retroajuste, ou seja, para cada $j_Y = 1,2,...,p$, as funções $s_{jY}(.)$ são atualizadas, suavizando $y - \beta_0 - \sum_{jj\neq jY}s_{jj}^{(0)} | (z_{jj})$ por meio de algum suavizador do gráfico de dispersão, o que resulta em novas funções suaves $s_1^{1}(.), s_2^{1}(.),...,s_p^{1}(.)$. Para acelerar a convergência, as funções suaves atualizadas podem ser utilizadas, por exemplo, $s_1^{1}(.)$ ao invés de $s_1^{0}(.)$ no cálculo de $s_2^{1}(.)$;
2. Repetem-se os passos 1 e 2 até que se obtenha a convergência.

Essa ideia é genérica, já que os detalhes diferem dependendo da técnica de suavização usada e do contexto no qual o algoritmo será utilizado.

\subsection{Seleção de Modelos - Enfoque de Predição}

**Inserir aqui a parte do livro do Rafael sobre seleção de modelos, Data splitting, Validação cruzada.. a partir da pagina 12 do livro do Rafael - Resumir o conteúdo adequando a notação que você usou acima**


\section{Resultados e Discussão}

\subsection{Estudo de simulação}

Neste momento será utilizada a simulação de dados, para gerar situações nas quais possam ser aplicadas algumas das técnicas estudadas, analisando assim suas respectivas performances. A partir disso, iremos avaliar sob estes dados, aspectos visuais dos ajustes de algumas das técnicas vistas até momento. Realizaremos comparações , adotando diferentes tamanhos de janelas (\textit{span}) para cada técnica e então para qual valor de \textit{span}, temos o melhor ajuste. Em um segundo momento, classificaremos para qual técnica obtemos o melhor ajuste. Para esta etapa iremos utilizar as técnicas: Suavizadores com Kernel, \textit{LOWESS} (Suavizador em diagrama de dispersão com pesos locais) e \textit{Splines} de Regressão. Por fim, compararemos as performances dos ajustes do modelo linear e modelo aditivo, em relação a qualidade da predição. Para esta finalidade, utilizaremos as métricas apresentadas na Seção 3.4.

  \subsubsection{Cenário 1}

```{r codigos,fig.height=2, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#library(tidyverse)
library(dslabs)
library(zoo)
library(igraph)
library(ggplot2)
library(dplyr)
library(tidyr)
library(wesanderson)
library(RColorBrewer)

############### PARA FAZER GRAFICOS ############

# textsize <- 10
# textsize2 <- 18
# mytheme <- theme(
#   axis.title = element_text(size = textsize),
#   axis.text = element_text(size = textsize),
#   legend.title = element_text(size = textsize),
#   legend.text = element_text(size = textsize))
# 
# axis_theme <- theme_bw()  +
#   theme(
#     axis.text.x = element_text(angle = 0,face = "bold",size = textsize),
#     axis.text.y = element_text(angle = 0,face = "bold",size = textsize),
#     legend.background = element_rect(fill = "transparent", colour = NA,size = 2),
#     panel.background = element_rect(fill = "transparent", colour = NA),
#     plot.background = element_rect(fill = "white", colour = NA),
#     axis.title.x = element_text(colour = "black",size = textsize,face = "bold"),
#     axis.title.y = element_text(colour = "black",size = textsize,face = "bold"),
#     legend.title = element_text(colour = "black",size = 10),
#     pos_leg = "right",
#     legend.text = element_text(colour = "black",size = 8,face = "bold"),
#     panel.grid = element_line(linetype="dashed"),
#     panel.grid.major = element_line(colour = "gray"),
#     title =element_text(size=textsize, face='bold',hjust = 0.5),
#     plot.title = element_text(hjust = 0.5),
#     axis.title = element_text(color="#000000", face="bold", size=textsize,lineheight = 2))


# plot.mult.curves <- function(df,labelx="Eixo X",labely="Eixo Y",
#                              title = "Gráfico de Dispersão"
# ){
#   
#   point.size  = 3
#   point.alpha = .2
#   point.color = "darkgrey"
#   line.size  = 0.8
#   line.alpha = 1
#   line.color = "red"
#   cores               <- c()
#   col.brew            <- brewer.pal(n = 9, name = "Set1")
#   colors              <- col.brew
#   
#   #display.brewer.pal(, "Set1")
#   plot <- ggplot(data = df,aes(x=x,y=y))+
#     geom_point(alpha=point.alpha,size=point.size, col = point.color)+
#     labs(x = labelx, 
#          y = labely,
#          color = "Legend")+
#     xlim(range(x)+c(-0,+0))+
#     ylim(range(y)+c(-0,+0))+
#     ggtitle(paste0(title))+
#     geom_line(aes(x=x,y = value,color=variable),size=line.size,alpha=line.alpha) +
#     scale_color_manual(values = colors) 
#   
#   plot
#   
# }
#################################

###### DADOS  ########

set.seed(102585)
normal = rnorm(200)
x = seq(0, 50, length.out = 200)
y = 10 + (5*sin(pi*x/24)) + normal

dados = data.frame(x,y)

## plotando a curva real
true = 10 + (5*sin(pi*x/24))

df = cbind(dados$x, dados$y, true)
colnames(df) = c("x", "y","Curva real")

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )

```

Foram geradas 200 observações, sendo $X$ uma sequência de 0 a 50 e $Y$ definido pela função
$$ y = 10 + 5sen\pi \dfrac{x}{24} + \epsilon $$
onde $\epsilon$ é um termo aleatório.
Para ter uma ideia da variabilidade dos dados, foi calculado o coeficiente de variação, obtendo-se que $CV_x = 0.5817$ e $CV_y = 0.3645$. Na Figura 1 temos o comportamento  dos dados juntamente com a curva real.



```{r curva_real, echo = FALSE,fig.cap="Gráfico de dispersão dos dados gerados e curva real"}
plot.mult.curves(df,title = NULL) 
```

\newpage

 **Bin Smoother**
```{r codigo bin, include=FALSE, echo = FALSE}
span1 = 0.5   # interpolou os pontos
fit1 <- with(dados, ksmooth(x, y, kernel = "box", bandwidth = span1))

span2 = 11  # ficou bom
fit2 <- with(dados, ksmooth(x, y, kernel = "box", bandwidth = span2))

span3 = 40  # tende a uma reta
fit3 <- with(dados, ksmooth(x, y, kernel = "box", bandwidth = span3))

spans = c(0.5, 11, 40)
df = cbind(dados$x, dados$y, fit1$y, fit2$y, fit3$y)
colnames(df) = c("x", "y",
                 paste("Ajuste1 - ", "tam:", spans[1]),
                 paste("Ajuste2 - ", "tam:", spans[2]),
                 paste("Ajuste3 - ", "tam:", spans[3]))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )
```

```{r bin, echo = FALSE,fig.cap="Alguns ajustes utilizando a técnica Bin Smoother"}
plot.mult.curves(df,title = NULL)
```

Para o bin, tem-se o tamanho da vizinhança, ou seja, quantos pontos são considerados para fazer o ajuste em cada região, sendo indicado no gráfico por $tam$. Na Figura 2, observa-se que com o tamanho da vizinhança menor (0.5) a técnica interpola os dados, já com o valor maior nota-se que o suavizador tende a uma reta. Com o valor 11 tem-se um equilíbrio da curva, que capta a tendência dos dados. Logo, dentre os valores observados, pode-se concluir que nesse cenário os valores próximos de 11 tendem a apresentar um resultado melhor.

 **Loess**
 
```{r codigo loess, include=FALSE, echo = FALSE}
span1 = 0.02
fit1 <- loess(y ~ x, degree=1, span = span1, data=dados)

span2 = 0.2
fit2 <- loess(y ~ x, degree=1, span = span2, data=dados)

span3 = 1
fit3 <- loess(y ~ x, degree=1, span = span3, data=dados)

spans = c(0.02, 0.2, 1)
df = cbind(dados$x, dados$y, fit1$fitted, fit2$fitted, fit3$fitted)
colnames(df) = c("x", "y",
                 paste("Ajuste1 - ", "Span:", spans[1]),
                 paste("Ajuste2 - ", "Span:", spans[2]),
                 paste("Ajuste3 - ", "Span:", spans[3]))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )
```

```{r loess, echo = FALSE,fig.cap="Alguns ajustes utilizando a ténica LOESS"}
plot.mult.curves(df,title = NULL)
```

Nesse caso tem-se o valor do \textit{span}, ou seja, a proporção de observações em cada vizinhança. Na Figura 3, com \textit{span} = 0.02 é possível observar que a técnica fica bastante irregular, enquanto que com \textit{span} = 0.2 a curva já fica mais suave e reflete bem o comportamento das observações. Nesse cenário, a técnica fica suave demais (tendendo a uma reta) quando o valor do \textit{span} é igual a 1. Dessa forma, nota-se que o resultado é mais satisfatório para valores de \textit{span} próximos de 0.2.

 **Kernel**

```{r codigo kernel, include=FALSE, echo=FALSE}
span1 = 0.4
fit1 <- with(dados, ksmooth(x, y, kernel = "normal", bandwidth = span1))

span2 = 6
fit2 <- with(dados, ksmooth(x, y, kernel = "normal", bandwidth = span2))

span3 = 45
fit3 <- with(dados, ksmooth(x, y, kernel = "normal", bandwidth = span3))

spans = c(0.4,6,45)
df = cbind(dados$x, dados$y, fit1$y, fit2$y, fit3$y)
colnames(df) = c("x", "y",
                 paste("Ajuste1 - ", "tam:", spans[1]),
                 paste("Ajuste2 - ", "tam:", spans[2]),
                 paste("Ajuste3 - ", "tam:", spans[3]))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )

```

```{r kernel, echo=F, fig.cap="Alguns ajustes utilizando os Suavizadores com Kernel"}
plot.mult.curves(df,title = NULL)
```
Nesse método tem-se a largura da região (\textit{bandwidth}), indicado no gráfico por $tam$. Observando a Figura 4, como nas demais comparações até o momento, a técnica Kernel se comporta de maneira semelhante em relação à dimensão da largura nesse cenário: o menor valor resulta em uma curva extremamente irregular, enquanto o maior valor faz a curva ficar muito suave, tendendo a uma reta. Então dentro dos valores observados, percebe-se que valores próximos de 6 podem ser uma escolha interessante, resultando em uma curva suave que acompanha os dados.

 **Splines de regressão**

```{r codigo splines, include=F, echo=F}
no <- function(x, no) ifelse(x < no, 0, x-no)  # funcao truncada, so pega os positivos

x15 = no(x,15)
x35 = no(x,35)

fit1 = lm(y ~ x + x15 + x35)

# cubica

require(splines)

nos = c(15,35)

fit <- lm(y ~ bs(x,knots = nos),data = dados )

df = cbind(dados$x, dados$y, fit1$fitted, fit$fitted)
colnames(df) = c("x", "y",
                 paste("Ajuste1 - ", "Spline grau 1"),
                 paste("Ajuste2 - ", "Spline cubico"))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )

```

```{r splines, echo=F,fig.cap="Alguns ajustes utilizando a técnica Spliness de Regressão"}
plot.mult.curves(df,title = NULL)
```

Através da Figura 5, nota-se que o \textit{spline} de regressão de grau um é extremamente desapropriado, pois a descontinuidade é muito grande. Já o \textit{spline} cúbico segue a tendência dos pontos de uma forma suave, não sendo possível notar qualquer descontinuidade da curva nos nós. Logo, nesse cenário, o \textit{spline} cúbico teve um resultado visualmente melhor.

Vale ressaltar que em todos os métodos vistos é possível notar a relação de troca entre a variância e o viés. Para valores menores de \textit{span} e largura/tamanho da vizinhança os pontos são interpolados (ou praticamente interpolados), o que resulta em uma curva com grande variância, porém viés pequeno, já que a curva está muito próxima de todas as observações. Por outro lado, quando esses valores aumentam, tem-se a situação inversa: a variância em relação à curva diminui, mas o viés aumenta.

 **Comparação entre os metódos**

```{r codigo metodos, include=F, echo=F}
# bin  -  span = 11
fit1 <- with(dados, ksmooth(x, y, kernel = "box", bandwidth = 11))

# loess -  span = 0.2
fit2 <- loess(y ~ x, degree=1, span = 0.2, data=dados)

# kernel - span = 6
fit3 <- with(dados, ksmooth(x, y, kernel = "normal", bandwidth = 6))

# spline cubico
require(splines)
fit4 <- lm(y ~ bs(x, knots = c(15,35)),data = dados )


spans = c(11, 0.2, 6)
df = cbind(x= dados$x, y = dados$y, Bin = fit1$y,Loess = fit2$fitted,Kernel = fit3$y,`Spline Cubico`= fit4$fitted)
df1 = df %>% as.data.frame
colnames(df) = c("x", "y",
                 paste("Ajuste1 - ", "Bin - tam = ", spans[1]),
                 paste("Ajuste2 - ", "Loess - Span = ", spans[2]),
                 paste("Ajuste3 - ", "Kernel - tam = ", spans[3]),
                 paste("Ajuste4 - ", "Spline cubico"))

df = as_tibble(df) %>%
  gather(key = "variable", value = "value", -x, -y )

```

```{r metodos, echo=F, fig.cap="Comparação dos ajustes entre os métodos"}
plot.mult.curves(df,title = NULL)
```

Pela figura 6, percebemos que o ajuste bin é extremamente irregular, sendo possível ver a descontinuidade da curva. A técnica kernel não tem um comportamento muito bom nas extremidades. É possivel notar que os ajustes das técnicas \textit{loess} e \textit{spline} cúbico ficaram muito próximas, se adequando muito bem aos dados, mas o \textit{spline} ficou melhor, pois está captando melhor o comportamento dos dados. Portanto, para os dados gerados e dentre os métodos vistos, o \textit{spline} cúbico mostrou um resultado mais satisfatório.

É importante notar que, para a escolha dos valores de \textit{span} ou tamanho/largura da vizinhança e do método mais adequado para esse cenário, a análise feita foi estritamente gráfica, ou seja, visual. Existem medidas numéricas adequadas para fazer essa análise.



```{r,echo=FALSE}

library(Metrics)
df_metrics <- data.frame(Smoother = c("Bin", "Loess","Kernel","Cubic Spline"),
                         EQM      =  c(rmse(actual = df1$y,df1$Bin),
                                       rmse(actual = df1$y,df1$Loess),
                                       rmse(actual = df1$y,df1$Kernel),
                                       rmse(actual = df1$y,df1$`Spline Cubico`)))


kable_data(data = df_metrics,cap = "Erro Quadrático Médio para os suavizadores Loess, Kernel e Spline Cúbico",foot = NULL)

```
  
\subsubsection{Avaliando melhores parâmetros para os suavizadores}

\subsubsection{Leave One Out Cross Validation}



```{r,fig.cap="Gráfico contendo valor de span ótimo para o ajuste Loess, considerando o LOOCV", fig.height=2}

nd <- length(dados$x)
xx <- dados$x
yy <- dados$y

ntrial <- 50
span1 <- seq(from = 0.1, by = 0.01, length = ntrial)

output.lo <- locv1(xx, yy, nd, span1, ntrial)
#cv <- output.lo
gcv <- output.lo

# plot(span1, gcv, type = "n", xlab = "span", ylab = "GCV")
# points(span1, gcv, pch = 3)
# lines(span1, gcv, lwd = 2)

eqm1 = gcv[which.min(gcv)]
gpcvmin <- seq(along = gcv)[gcv == min(gcv,na.rm = T)]
spangcv <- span1[gpcvmin]
gcvmin <- gcv[gpcvmin]
# points(spangcv, gcvmin, cex = 1, pch = 15)


df <- data.frame(x = span1, y = gcv)
 df1      <-  cbind(df,EQM = gcv)


colnames(df1) <- c("x","y",
                    paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p1 <- ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Span",y = "EQM")+
  geom_vline(xintercept = spangcv,color ="red")+
  annotate("text",x = spangcv+0.025,y = max(df1$y),label=spangcv,) +
  axis.theme()


```
  
```{r,fig.cap="Gráfico contendo valor para a LArgura da Janela ótimo para o ajuste Kernel", fig.height=2,echo=FALSE,include=TRUE}

fit.kernel <- cv_bws_npreg(x = dados$x,y = dados$y)
kbest <- fit.kernel$CV_MSEs %>% as.data.frame
bw <- rownames(kbest) %>% as.numeric
cverrors <- kbest[,1]
df <- data.frame(x = dados$x, y = dados$y)
df1 <- data.frame(x = bw, y = cverrors,EQM = cverrors)



colnames(df1) <- c("x","y",
                    paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p2 <- ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM")+
  geom_vline(xintercept = fit.kernel$best.bw,color ="red")+
  annotate("text",x = fit.kernel$best.bw+0.025,y = max(cverrors),label=fit.kernel$best.bw) +
  axis.theme()



```
  
```{r}

library(splines)
library(ISLR)
attach(Wage)
set.seed(103159)
df <- data.frame(x = dados$x,y = dados$y)

tr= sample(1:nrow(df), nrow(df)/2)
te= (-tr)
dftr= df[tr,]
# Tuning parameter= the bins, suggest some possible values
number_of_bins = seq(1,20)
k = nrow(dftr)
folds = sample( x = 1:k, size = nrow(dftr), replace=FALSE ) 
cv.errors = matrix( NA, k, length(number_of_bins) )

for( i in 1:length(number_of_bins)){ # for each number of knots to test
  
  for( j in 1:k ){ # for each fold
    cubicfit = glm( y ~ bs( x, df=i), data=dftr[folds!=j,] )
    cubicpred = predict( cubicfit, newdata=dftr[folds==j,] )
    cv.errors[j,i] = mean( ( dftr[folds==j,]$y - cubicpred )^2 ) }
  
}

cv.errors.mean = apply(cv.errors,2,mean)
cv.errors.stderr = apply(cv.errors,2,sd)/sqrt(k)

min.cv.index = which.min( cv.errors.mean )
one_se_up_value = ( cv.errors.mean+cv.errors.stderr )[min.cv.index] 

# Set up the x-y limits for plotting:
min_lim=min( one_se_up_value, cv.errors.mean, cv.errors.mean-cv.errors.stderr, cv.errors.mean+cv.errors.stderr ) * 0.9
max_lim=max( one_se_up_value, cv.errors.mean, cv.errors.mean-cv.errors.stderr, cv.errors.mean+cv.errors.stderr ) * 1.1

df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins, y = cv.errors.mean,EQM = cv.errors.mean)



colnames(df1) <- c("x","y",
                    paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p3<-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM")+
  geom_vline(xintercept = min.cv.index,color ="red")+
  annotate("text",x = min.cv.index+0.05,y = max(cv.errors.mean),label=min.cv.index) +
  axis.theme()




# plot( number_of_bins, cv.errors.mean, ylim=c(min_lim,max_lim), pch=19, type='b', xlab='number of cut bins', ylab='CV estimate of the prediction error' )
# lines( number_of_bins, cv.errors.mean-cv.errors.stderr, lty='dashed' )
# lines( number_of_bins, cv.errors.mean+cv.errors.stderr, lty='dashed' )
# abline( h=one_se_up_value, col='red' )




```
  
```{r,fig.cap="Comparação entre os ajustes considerando os parâmetros ótimos.", fig.height=2}





partial_plots <- cowplot::plot_grid(p1,p2,p3,ncol=3,labels=LETTERS[1:3]);partial_plots

```
```{r,fig.cap="Comparação entre os ajustes considerando os parâmetros ótimos.", fig.height=2}
library(car)

# nob = min.cv.index
# fit.best.spline = glm( y ~ bs( x, df=nob), data= df)

# dfte= df[te,]
# plot( dfte$x, dfte$y )
# aRng = range(dfte$x)
# 
# Xage= seq( from=aRng[1], to=aRng[2], length.out=100 )
# Ywage= predict( fit, newdata=list( x=Xage ) )
# lines( Xage, Ywage, col='red', lw=4 )

nob = min.cv.index
fit.best.loess              <-  loess(data = df,formula =  y ~x,span = spangcv)
fit.best.kernel <- ksmooth(x = x, y = y,kernel = "normal",bandwidth = fit.kernel$best.bw)
fit.best.spline = glm( y ~ bs( x, df=nob), data= df)



df1      <-  cbind( df,
                    "Loess" = fit.best.loess$fitted,
                    "Kernel Smoother" = fit.best.kernel$y,
                    "Spline Cúbico" = fit.best.spline$fitted.values) %>% as.data.frame


colnames(df1) <- c("x","y",
                    paste0("Ajuste1\nLoess\nSpan : ", spangcv, "\n"),
                    paste("Ajuste2\nKernel Smoother\nBandwidth :", fit.kernel$best.bw, "\n"),
                    paste("Ajuste3\nSplines de Regressão\nCubico\nNós :", min.cv.index, "\n")
                   )

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

plot.mult.curves(df = df1,title = NULL)


```
  

  \subsubsection{Cenário 2}

```{r , include=FALSE}
rm(list=ls())
source(file = "funcoes.R",encoding = "UTF-8")
library(tidyverse)
library(binsmooth)
library(knitr)
library(kableExtra)
library(additive.models)

knitr::opts_chunk$set(echo = FALSE,warning= FALSE, message= FALSE,
                      out.width = "100%",fig.align = "center",size ="large",fig.height = 3)

library(additive.models)

n <- 1e3
set.seed(103159)
n           <- 50
x           <- seq(0,2,0.01)
norms       <- rnorm(length(x),0,0.25)
dens_gamma  <- dgamma(x = x,shape = 6,rate = 10)
y           <- dens_gamma + norms

dados       <- data.frame(x=x,y=y,variable = "Gamma(6,10)",value = dens_gamma)




```


\hspace{1.25cm} Para este cenário, foram geradas 201 observações, sendo x uma sequência de 0 à 2 com intervalos de 0.01. Ainda, temos que $y = f(x) + e$, com $f(x) \sim Gamma(6,10)$ e $e ~ N(0,0.25)$. O gráfico de dispersão para estes dados pode ser verificado na Figura 7, onde podemos observar o seu comportamento.


```{r,fig.cap="Gráfico de dispersão dos dados gerados para o estudo de simulação", fig.height=2.5}

####### Aqui
#plot.curves(data = dados,x =  x, y = y,title = NULL)
df           <- as.tibble(dados) 
  #gather(key = "variable", value = "value",-x,-y)
  plot.mult.curves(df = df,title = NULL)
```

```{r,fig.cap="Suavizador bin para diferentes larguras de janela", fig.height=2.5}

  bin.smoother2  <- function(y,k){
    # y=dados$y
    # k=3
    library(reshape2)
    v   = split(y, ceiling(seq_along(y)/k))
    fit = sapply(v, function(item){rep(mean(item), length(item))})
    
    fit =  melt(data = fit,id.vars=1:k) %>% select(value)
    
  return(fit)
  
  }
  
  widths  <- c(5,15,45,90)
  df      <- cbind(dados$x,dados$y)
  for(size in widths){
  
  fit    <- bin.smoother2(y = dados$y,k = size)
  df     <- cbind(df,fit)
  }
  

  
  colnames(df) <- c("x","y",
                    paste("Ajuste1 - ", "Largura : ",widths[1]),
                    paste("Ajuste2 - ", "Largura : ",widths[2]),
                    paste("Ajuste3 - ", "Largura : ",widths[3]),
                    paste("Ajuste4 - ", "Largura : ",widths[4]))
  
  df           <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)
  plot.mult.curves(df = df,title = NULL)


```

O suavizador Bin (Figura 8) realiza um ajuste dividindo os dados em k conjuntos, e para cada conjunto disjunto realiza-se a média. Podemos observar que este suavizador tem uma forma altamente irregular, onde basicamente identificamos uma tendência de aumento ou decaimento brusco para as estimativas. Observamos ainda, quando adotamos valores pequenos para a largura de cada subconjunto, possuímos vários intervalos disjuntos de estimação, sendo assim várias estimativas são calculadas nos dando uma curva irregular no formato escada. Conforme aumentamos a largura de cada subconjunto consequentemente teremos menos intervalos de estimação, e quanto maior for este valor, as nossas estimativas tenderão a uma constante. A média móvel (Figura 9) é uma técnica de suavização onde fixado um ponto $x_0$ realiza-se a média para k ponto próximos a $x_0$, repetindo este processo até que todos os pontos possíveis em X tenha uma estimativa. Assim como o suavizador Bin, a média móvel, nos fornece um ajuste irregular.

```{r,fig.cap="Média Móvel para diferentes larguras de janela", fig.height=2.5}
library(igraph)
library(zoo)

widths    <- c(3,7)
df        <- cbind(dados$x,dados$y)
for(size in widths){
  
  fit     <-  rollmean(x = dados$y,k = size,align = "right",na.pad = T)
  df      <- cbind(df,fit)
}

colnames(df) <- c("x","y",paste("Ajuste1 - ", "Largura : ",widths[1]) ,
                    paste("Ajuste2 - ", "Largura : ",widths[2]))

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

plot.mult.curves(df = df,title = NULL)


```


A linha móvel (Figura 10) segue basicamente o  mesmo principio da média móvel, mas ao invés da média, ajusta-se um regressão linear com k vizinhos próximos a $x_0$ e obtem-se a estimativa para este ponto. Como podemos observar quando escolhemos uma proporção de pontos baixa (0.05,0.1), temos um ajuste mais suavizado quando comparado com as técnicas anteriores, porém visualizamos irregularidade em sua forma. Quando aumentamos a proporção de pontos para o ajuste, teremos uma forma altamente suavizada, e quando adotamos um número grande o suficiente de pontos próximos, o ajuste tende a ser uma reta.


```{r,fig.cap="Linha Móvel para diferentes intervalos", fig.height=2.5}
library(igraph)

library(zoo)


widths    <- c(0.05,0.1,0.25,.45)
df        <- cbind(dados$x,dados$y)
for(size in widths){
  
  fit     <-  running.line(x = dados$x,y = dados$y,f = size)$fitted.values 
  df      <-  cbind(df,fit)
}

colnames(df) <- c("x","y",
                    paste("Ajuste1 - ", "Largura : ",widths[1]),
                    paste("Ajuste2 - ", "Largura : ",widths[2]),
                    paste("Ajuste3 - ", "Largura : ",widths[3]),
                    paste("Ajuste4 - ", "Largura : ",widths[4]))

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

plot.mult.curves(df = df,title = NULL)


```


Para a técnica \textit{Lowess}, o parâmetro \textit{span} controla a proporção de pontos que influência a suavização para cada valor. Observamos então que para valores de \textit{span} pequenos, ou seja, para os ajustes que utilizam um proporção de pontos pequena (Conforme Figura 11, Ajuste1 - Span : 0.05), nos fornece formas mais irregulares. Logo, quando adotamos poucos pontos, o ajuste tende a interpolar uma quantidade maior de observações. A medida que aumentamos este valor percebemos que o ajuste fica mais suave. Porém quando este valor se torna grande,  o ajuste se torna demasiadamente suavizado, e tende a forma linear. Para elegermos o melhor ajuste iremos adotar o critério de troca entre variância e viés que consiga captar a maior quantidade de observações possíveis. Logo para  vizinhanças grandes por adotarmos valores altos para o \textit{span}, estas estimativas tendem a ter variância pequena, porém um viés alto. Ainda estes ajustes não captam de forma adequada a tendência produzida pelos dados simulados. Os ajustes das curvas 4 e 5 (linha roxa e laranja respectivamente, Figura 11), não nos fornece uma suavização que capta de forma adequada a tendência dos dados. Quando observamos atentamente a Figura 11, concluimos que ao adotarmos valores para o \textit{span} que estejam entre 0.15 e 0.25 teremos para estes dados um ajuste que capte a tendência dos dados levando em consideração um forma que tenha uma relação de troca entre viés e variância equilibrada. 


```{r,fig.cap="Ajustes Lowess para diferentes valores de span", fig.height=2.5}
library(igraph)
library(zoo)
widths    <- c(0.05,0.15,0.25,.65,.95)   ## Diferentes tamanha de span em um vetor
df        <- cbind(dados$x,dados$y)  # cria um data frame contendo uma coluna com os valores de x e outra para y
for(size in widths){        ## loop for, para cada span, faz um ajuste e concantena no dataframe atraves do cbind
  
  fit     <-  lowess(x = dados$x,y = dados$y,f = size)$y  ## valores ajustados para a técnica lowess, e paga somente os valores y
  df      <-  cbind(df,fit)  ## concatena por coluna dentro do data frame para cada ajuste com span diferentes
}


### Renomeias as colunas do dataframe df, as duas primeira sao os valores originais dos dados simulados. para cada span temos uma coluna diferente para valores ajustados no dataframe df, logo temos atribuimos um nome especifico para cada coluna.
colnames(df) <- c("x","y",
                    paste("\nAjuste1\n", "Span\n",widths[1]),
                    paste("\nAjuste2\n", "Span\n",widths[2]),
                    paste("\nAjuste3\n", "Span\n",widths[3]),
                    paste("\nAjuste4\n", "Span\n",widths[4]),
                    paste("\nAjuste5\n", "Span\n",widths[5]))

## procedimento para colocar os valores ajustados empilhados em um unica coluna e criasse uma coluna com os rotulos para os valores ajustados
df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)


## funcao do arquivo funcoes.R que na pasa Relatorio final
plot.mult.curves(df = df,title = NULL)




```

\newpage

A Figura 12 mostra alguns ajustes considerando os suavizadores com kernel (kernel gaussiano), considerando valores distintos para a largura de banda (\textit{bandwidth}). De forma análoga a técnica \textit{Lowess}, quando obtemos os ajustes para valores pequenos do parâmetro largura da banda (\textit{bandwidth}), temos uma maior irregularidade em sua forma (figura 14, Ajuste1). A medida que valores maiores para a largura de banda são adotados a curva se torna altamente suavizada, de modo que o ajuste não capta a tendência dos dados (Vide figura 14). Portanto ao avaliarmos podemos dizer o melhor ajuste  se encontra para valores da largura de banda de 0.05 à 0.35. 

```{r,fig.cap="Ajustes com Suavizadores Kernel para diferentes valores de largura de banda", fig.height=2.5}
library(igraph)

library(zoo)


widths    <- c(0.05,0.15,0.25,.65,.95)
df        <- cbind(dados$x,dados$y)
for(size in widths){
  
  fit     <-  ksmooth(x = dados$x,y = dados$y,kernel ="normal", bandwidth = size)$y
  df      <-  cbind(df,fit)
}

colnames(df) <- c("x","y",
                    paste("\nAjuste1\n", "Bandwidth\n",widths[1]),
                    paste("\nAjuste2\n", "Bandwidth\n",widths[2]),
                    paste("\nAjuste3\n", "Bandwidth\n",widths[3]),
                    paste("\nAjuste4\n", "Bandwidth\n",widths[4]),
                    paste("\nAjuste5\n", "Bandwidth\n",widths[5]))

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

p1 <- plot.mult.curves(df = df,title = NULL) + axis.theme(pos_leg = "right")
p1

```




Para análise do \textit{Spline} de Regressão iremos comparar os ajustes quando adotamos um polinômio de grau 1 e logo após utilizaremos polinômios cúbicos. Poderemos ainda verificar as consequências ao se adotar diferentes quantidades de nós para os dois casos. Utilizando o polinômio de grau 1 (ajuste linear), temos que para cada nó escolhido, este tipo de \textit{spline}, ajusta uma reta. Como podemos observar na Figura 13 (gráfico A), ao escolhermos valores extremos para esta técnica, quando adotamos uma quantidade pequena de nós, a tendência dos dados não é muito bem explicada por poucas retas, e em contrapartida, quando observamos para muitos nós, temos uma forma irregular. 

\newpage
```{r,fig.cap="Splines de Regressão com polinômio de grau 1, para diferentes quantidades de nós", fig.height=2.5}
library(segmented)
#p_load(kirkegaard, rms)
library(rms)
library(pacman)
library(igraph)
library(zoo)
library(splines)


df        <- as.data.frame(cbind(dados$x,dados$y))
nos       = c(2,7,8,9,50)
colnames(df)<-c("x","y")
for(k in nos){
  
  p         <-  seq(1,k-1,1)/k
  knots     <-  quantile(df$x  , p = p)
  

  fit       <-  ols(as.formula(glue::glue("y ~ lsp(x, knots)")), data = df)
  df        <-  cbind(df,predict(fit))
}

colnames(df) <- c("x","y",
                    paste("Ajuste1 - ", "Nós : ",nos[1]-1),
                    paste("Ajuste2 - ", "Nós : ",nos[2]-1),
                    paste("Ajuste3 - ", "Nós : ",nos[3]-1),
                    paste("Ajuste4 - ", "Nós : ",nos[4]-1),
                    paste("Ajuste5 - ", "Nós : ",nos[5]-1)
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

plot1 <- plot.mult.curves(df = df,title = NULL)



```
Como podemos ver os ajuste mais adequados para representar a tendência destes dados  estaria próximos de 7 e 8 nós. Os ajustes anteriores está considerando uma distribuição equidistante para diferentes valores de quantis para os dados simulados. Porém para estas técnicas além da quantidade nós, a localização de cada nó, pode nos levar a uma representação diferente e mais adequada para os dados. A Figura 15 (gráfico B), nos mostra um ajuste realizado escolhendo a posição utilizando dois nós.

 
```{r,fig.cap="Splines de Regressão com polinômio de grau 1 para a escolha de nós distintos", fig.height=5}
library(segmented)
#p_load(kirkegaard, rms)
library(rms)
library(pacman)
library(igraph)
library(zoo)
library(splines)


df        <- as.data.frame(cbind(dados$x,dados$y))

colnames(df)<-c("x","y")
k=4
  knots     <-  c(0.5,1.0)
  fit       <-  ols(as.formula(glue::glue("y ~ lsp(x, knots)")), data = df)
  df        <-  cbind(df,predict(fit))

  colnames(df) <- c("x","y",
                    paste("\nAjuste1\n", "Nós\nc(0.5,1.0)")
                    )
  
  df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

plot2 <- plot.mult.curves(df = df,title = NULL) +axis.theme(pos_leg = "right")

cowplot::plot_grid(plot1,plot2,ncol=1,nrow=2,labels=LETTERS[1:2])

```

O ajuste selecionando apenas dois nós nos quantís 0.5 e 1.0 (gráfico B) nos fornece um ajuste relativamente mais adequado para representar os dados. Conseguimos melhorar o ajuste dos dados utilizando um polinômio de grau maior. 


\newpage
Na Figura 14 temos alguns ajustes levando em consideração os \textit{Splines} de Regressão Cúbico. Sendo assim, quando avaliamos as curvas geradas, observamos que o Ajuste1 (linha vermelha), não representa adequadamente os dados. O Ajuste4 (curva roxa), observamos nos extremos da curva uma leve ondulação. Logo se observamos a curva do Ajuste3 (curva verde), percebemos que esta curva se ajusta de forma suave, logo podemos concluir que para estes dados, ao utilizarmos os \textit{Splines} de Regressão Cúbico, o melhor ajuste está quando utilizamos valores próximos de 7 nós.

```{r,fig.cap="Ajustes com Splines de regressão para diferentes valores de span", fig.height=2.5}
library(igraph)
library(zoo)
library(splines)


df        <- cbind(dados$x,dados$y)
nos       = c(3,5,7,9)


for(k in nos){

  
  p         <-  seq(1,k-1,1)/k
  knots     <-  quantile(dados$x, p = p)
  fit       <-  additive.spline.cubic(x = dados$x, y = dados$y, k = k,knots = knots)$fitted.values
  df        <-  cbind(df,fit)
}
fit5  <- additive.spline.cubic(x = dados$x, y = dados$y, k = k,knots = c(0.25,0.5,0.75,1.25))$fitted.values
df        <-  cbind(df,fit5)
colnames(df) <- c("x","y",
                    paste("Ajuste1 - ", "Nós : ",nos[1]),
                    paste("Ajuste2 - ", "Nós : ",nos[2]),
                    paste("Ajuste3 - ", "Nós : ",nos[3]),
                    paste("Ajuste4 - ", "Nós : ",nos[4]),
                    paste("Ajuste5 - ", "Nós : ","c(0.25,0.5,0.75,1.25)")
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)




plot.mult.curves(df = df,title = NULL)


```

A Figura 15 nos mostra, um comparativo entre a técnicas vista até agora.Em vermelho temos o ajuste considerando a a técnica suavizadora \textit{Loess}, com \textit{span} igual a 0.25. Em azul temos a curva ajustada com o suavizador Kernel (kernel gaussiano), considerando uma largura de banda de 0.22. Os dois ajustes são parecidos, e captam de forma adequada a tendência dos dados. Porém a curva em azul (\textit{kernel smoother}), aparenta ter uma pequena oscilação no começo da curva. 

```{r,fig.cap="Ajustes comparativos para as técnicas Lowess, Kernel Smoother e Splines de Regressão Cubico", fig.height=2}

library(additive.models)
source(file = "funcoes.R",encoding = "UTF-8")
library(tidyverse)
library(binsmooth)
library(knitr)
library(kableExtra)
library(additive.models)

knitr::opts_chunk$set(echo = FALSE,warning= FALSE, message= FALSE,
                      out.width = "100%",fig.align = "center",size ="large",fig.height = 3)

library(additive.models)
library(caret)
n <- 1e3
set.seed(103159)
n           <- 50
x           <- seq(0,2,0.01)
norms       <- rnorm(length(x),0,0.25)
dens_gamma  <- dgamma(x = x,shape = 6,rate = 10)
y           <- dens_gamma + norms

dados       <- data.frame(x=x,y=y,variable = "Gamma(6,10)",value = dens_gamma)


df        <- cbind(dados$x,dados$y) 



  
  fit.loess              <-  lowess(x = dados$x,y = dados$y,f = 0.25)$y
  fit.kssmooth           <-  ksmooth(x = dados$x,y = dados$y,kernel ="normal", bandwidth = 0.22)$y
  k=3
   p         <-  seq(1,k-1,1)/k
  knots     <-  quantile(dados$x, p = p)
  #fit.splinecubico       <-  lm(dados$y ~ ns(dados$x,knots = knots,df = 3) )$fitted.values
  fit.splinecubico       <-  additive.spline.cubic(x = dados$x, y = dados$y, k = k,knots = c(0,0.5,0.75,1.25))$fitted.values

  df1      <-  cbind(df,fit.loess,fit.kssmooth,fit.splinecubico)


colnames(df1) <- c("x","y",
                    paste("Ajuste1\nLowess\nSpan : 0.25 \n"),
                    paste("Ajuste2\nKernel Smoother\nBandwidth : 0.22 \n"),
                    paste("Ajuste3\nSplines de Regressão\nCubico\nNós : c(0.5,0.75,1.25)"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

plot.mult.curves(df = df1,title = NULL)


```
 

\newpage

 Ao observarmos o Ajuste3 (\textit{Splines} de Regressão Cúbica), utilizando os seguintes nós localizados em $c(0,0.5,0.75,1.25)$, consegue captar de forma mais adequada a tendência em torno do ponto $x = 0.5$. Logo podemos concluir que dentre as técnicas abordadas até o momento, para estes dados simulados, os \textit{splines} de regressão cúbico aparentam ajustar a tendência dos dados de forma mais suave levando em consideração um equilibrio entre viés e variância.

**Consegue recuperar os ajustes? Se sim, aplicar as métricas de qualidade da predição e concluir qual método tem melhor poder preditivo.**
```{r,fig.cap="Ajustes comparativos para as técnicas Lowess, Kernel Smoother e Splines de Regressão Cubico", fig.height=2}

library(additive.models)
source(file = "funcoes.R",encoding = "UTF-8")
library(tidyverse)
library(binsmooth)
library(knitr)
library(kableExtra)
library(additive.models)

knitr::opts_chunk$set(echo = FALSE,warning= FALSE, message= FALSE,
                      out.width = "100%",fig.align = "center",size ="large",fig.height = 3)

library(additive.models)
library(caret)
n <- 1e3
set.seed(103159)
n           <- 50
x           <- seq(0,2,0.01)
norms       <- rnorm(length(x),0,0.25)
dens_gamma  <- dgamma(x = x,shape = 6,rate = 10)
y           <- dens_gamma + norms

dados       <- data.frame(x=x,y=y,variable = "Gamma(6,10)",value = dens_gamma)


df        <- cbind("X" = dados$x,"Y" = dados$y) 



  
  fit.loess              <-  lowess(x = dados$x,y = dados$y,f = 0.25)$y
  fit.kssmooth           <-  ksmooth(x = dados$x,y = dados$y,kernel ="normal", bandwidth = 0.22)$y
  k=3
   p         <-  seq(1,k-1,1)/k
  knots     <-  quantile(dados$x, p = p)
  #fit.splinecubico       <-  lm(dados$y ~ ns(dados$x,knots = knots,df = 3) )$fitted.values
  fit_splinecubico       <-  additive.spline.cubic(x = dados$x, y = dados$y, k = k,knots = c(0,0.5,0.75,1.25))$fitted.values[,1]

  df1      <-  cbind( df,"Loess" = fit.loess,"Kernel" = fit.kssmooth,"Spline" = fit_splinecubico) %>% as.data.frame


# colnames(df1) <- c("x","y",
#                     paste("Ajuste1\nLowess\nSpan : 0.25 \n"),
#                     paste("Ajuste2\nKernel Smoother\nBandwidth : 0.22 \n"),
#                     paste("Ajuste3\nSplines de Regressão\nCubico\nNós : c(0.5,0.75,1.25)"))
# 
# df1 <- as.tibble(df1) %>%
#   gather(key = "variable", value = "value",-x,-y)

#plot.mult.curves(df = df1,title = NULL)

library(Metrics)
df_metrics <- data.frame(Smoother = c("Loess","Kernel","Cubic Spline"),
                         EQM      =  c(rmse(actual = df1$Y,df1$Loess),rmse(actual = df1$Y,df1$Kernel),rmse(actual = df1$Y,df1$Spline)))
```

**Comparação entre os Erros Quadráticos Médios dos Ajustes**

```{r,include=T,echo=FALSE}

kable_data(data = df_metrics,cap = "Erro Quadrático Médio para os suavizadores Loess, Kernel e Spline Cúbico",foot = NULL)
```
\subsubsection{Avaliando melhores parâmetros para os suavizadores}

\subsubsection{Leave One Out Cross Validation}



```{r,fig.cap="Gráfico contendo valor de span ótimo para o ajuste Loess, considerando o LOOCV", fig.height=2}

nd <- length(df1$x)
xx <- x
yy <- y

ntrial <- 50
span1 <- seq(from = 0.1, by = 0.01, length = ntrial)

output.lo <- locv1(xx, yy, nd, span1, ntrial)
#cv <- output.lo
gcv <- output.lo

# plot(span1, gcv, type = "n", xlab = "span", ylab = "GCV")
# points(span1, gcv, pch = 3)
# lines(span1, gcv, lwd = 2)

eqm1 = gcv[which.min(gcv)]
gpcvmin <- seq(along = gcv)[gcv == min(gcv,na.rm = T)]
spangcv <- span1[gpcvmin]
gcvmin <- gcv[gpcvmin]
# points(spangcv, gcvmin, cex = 1, pch = 15)


df <- data.frame(x = span1, y = gcv)
 df1      <-  cbind(df,EQM = gcv)


colnames(df1) <- c("x","y",
                    paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p1 <- ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Span",y = "EQM")+
  geom_vline(xintercept = spangcv,color ="red")+
  annotate("text",x = spangcv+0.025,y = max(df1$y),label=spangcv,) +
  axis.theme()


```


```{r,fig.cap="Gráfico contendo ajuste consideran valor ótimo para o ajuste LOESS", fig.height=2}

# best_span <- function(f,df){
#   
#   fit.loess              <-  lowess(x = df$X,y = df$Y,f = f)$y
#   eqm                    <-  rmse(actual = df$Y,fit.loess)
#   return(c(Span = f,EQM = eqm))
# }
# 
# teste = sapply(X = seq(0.001,1,0.001),FUN = best_span,df1) %>%
#   t %>% 
#   as.data.frame 
# 
# bf <- teste[which.min(teste$EQM),]
df <- data.frame(x = x, y = y)
 fit.best.loess              <-  loess(data = df,formula =  y ~x,span = spangcv)
 
 
  
  df1      <-  cbind( df,"Loess" = fit.best.loess$fitted) %>% as.data.frame


colnames(df1) <- c("x","y",
                    paste0("Ajuste1\nLowess\nSpan :", spangcv, "\n")
                    #paste("Ajuste2\nKernel Smoother\nBandwidth : 0.22 \n"),
                    #paste("Ajuste3\nSplines de Regressão\nCubico\nNós : c(0.5,0.75,1.25)")
                   )

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

#plot.mult.curves(df = df1,title = NULL)



```



```{r,fig.cap="Gráfico contendo valor de span ótimo para o ajuste Loess, considerando o K-Fold", fig.height=2,echo=FALSE,include=FALSE}

# df <- data.frame(x = x, y = y)
# span.seq <- seq(from = 0.15, to = 0.95, by = 0.05) #explores range of spans
# k <- 10 #number of folds
# set.seed(1) # replicate results
# folds <- sample(x = 1:k, size = length(x), replace = TRUE)
# cv.error.mtrx <- matrix(rep(x = NA, times = k * length(span.seq)), 
#                         nrow = length(span.seq), ncol = k)
# 
# for(i in 1:length(span.seq)) {
#   for(j in 1:k) {
#     loess.fit <- loess(formula = y ~ x, data = df[folds != j, ], span = span.seq[i])
#     preds <- predict(object = loess.fit, newdata = df[folds == j, ])
#     cv.error.mtrx[i, j] <- mean((df$y[folds == j] - preds)^2, na.rm = TRUE)
#     # some predictions result in `NA` because of the `x` ranges in each fold
#  }
# }
# 
# 
# cv.errors <- rowMeans(cv.error.mtrx)
# 
# 
# best.span.i <- which.min(cv.errors)
# # best.span.i
# # span.seq[best.span.i]
# 
# 
# # plot(x = span.seq, y = cv.errors, type = "l", main = "CV Plot")
# # points(x = span.seq, y = cv.errors, 
# #        pch = 20, cex = 0.75, col = "blue")
# # points(x = span.seq[best.span.i], y = cv.errors[best.span.i], 
# #        pch = 20, cex = 1, col = "red")
# 
# best.loess.fit <- loess(formula = y ~ x, data = df, 
#                         span = span.seq[best.span.i])
# 
# x.seq <- seq(from = min(x), to = max(x), length = 100)
# 
# df1 <- data.frame(x = span.seq, y = cv.errors,EQM = cv.errors)
# 
# 
# 
# colnames(df1) <- c("x","y",
#                     paste("EQM"))
# 
# df1 <- as.tibble(df1) %>%
#   gather(key = "variable", value = "value",-x,-y)
# 
# ggplot(df1,aes(x = x,y=y))+
#   geom_line()+
#   geom_point()+
#   labs(x = "Span",y = "EQM")+
#   geom_vline(xintercept = span.seq[best.span.i],color ="red")+
#   annotate("text",x = span.seq[best.span.i]+0.025,y = max(df1$y),label=span.seq[best.span.i]) +
#   axis.theme()

```


```{r,fig.cap="Gráfico contendo ajuste consideran valor ótimo para o ajuste LOESS, retornado pelo processo K-Fold", fig.height=2,include=FALSE,echo=FALSE}




# plot(x = df$x, y = df$y, main = "Best Span Plot")
# lines(x = x.seq, y = predict(object = best.loess.fit, 
#                              newdata = data.frame(x = x.seq)), 
#       col = "red", lwd = 2)


# df1      <-  cbind( df,"Loess" = best.loess.fit$fitted) %>% as.data.frame
# 
# 
# colnames(df1) <- c("x","y",
#                     paste0("Ajuste1\nLowess\nSpan :", span.seq[best.span.i], "\n")
#                     #paste("Ajuste2\nKernel Smoother\nBandwidth : 0.22 \n"),
#                     #paste("Ajuste3\nSplines de Regressão\nCubico\nNós : c(0.5,0.75,1.25)")
#                    )
# 
# df1 <- as.tibble(df1) %>%
#   gather(key = "variable", value = "value",-x,-y)
# 
# plot.mult.curves(df = df1,title = NULL)


```




```{r,fig.cap="Gráfico contendo valor para a LArgura da Janela ótimo para o ajuste Kernel", fig.height=2,echo=FALSE,include=TRUE}

fit.kernel <- cv_bws_npreg(x = x,y = y)
kbest <- fit.kernel$CV_MSEs %>% as.data.frame
bw <- rownames(kbest) %>% as.numeric
cverrors <- kbest[,1]
df <- data.frame(x = x, y = y)
df1 <- data.frame(x = bw, y = cverrors,EQM = cverrors)



colnames(df1) <- c("x","y",
                    paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p2 <- ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM")+
  geom_vline(xintercept = fit.kernel$best.bw,color ="red")+
  annotate("text",x = fit.kernel$best.bw+0.025,y = max(cverrors),label=fit.kernel$best.bw) +
  axis.theme()



```



```{r,echo=FALSE,include=FALSE}
library(ks)
ksmooth.gcv <- function(x, y){
  nobs <- length(y)
  xs <- sort(x, index.return = TRUE)
  x <- xs$x
  y <- y[xs$ix]
  xdif <- outer(x, x, FUN = "-")
  tune.ksmooth <- function(h){
    xden <- dnorm(xdif / h)
    xden <- xden / rowSums(xden)
    df <- sum(diag(xden))
    fit <- xden %*% y
    mean((fit - y)^2) / (1 - df/nobs)^2
  }
  xrng <- diff(range(x))
  oh <- optimize(tune.ksmooth, interval = c(xrng/nobs, xrng))$minimum
  if(any(oh == c(xrng/nobs, xrng)))
    warning("Minimum found on boundary of search range.\nYou should retune model with expanded range.")
  xden <- dnorm(xdif / oh)
  xden <- xden / rowSums(xden)
  df <- sum(diag(xden))
  fit <- xden %*% y
  list(x = x, y = fit, df = df, h = oh)
}


loess.gcv <- function(x, y){
  nobs <- length(y)
  xs <- sort(x, index.return = TRUE)
  x <- xs$x
  y <- y[xs$ix]
  tune.loess <- function(s){
    lo <- loess(y ~ x, span = s)
    mean((lo$fitted - y)^2) / (1 - lo$trace.hat/nobs)^2
  }
  os <- optimize(tune.loess, interval = c(.01, 99))$minimum
  lo <- loess(y ~ x, span = os)
  list(x = x, y = lo$fitted, df = lo$trace.hat, span = os)
}

```


```{r,fig.cap="Gráfico contendo ajuste consideran valor ótimo para o ajuste LOESS, retornado pelo processo K-Fold", fig.height=2}
library(car)

fit.best.kernel <- ksmooth(x = x, y = y,kernel = "normal",bandwidth = fit.kernel$best.bw)
df1      <-  cbind( df,"Kernel" =fit.best.kernel$y ) %>% as.data.frame


colnames(df1) <- c("x","y",
                    paste0("Ajuste1\nKernel\nSpan :", fit.kernel$best.bw, "\n")
                    #paste("Ajuste2\nKernel Smoother\nBandwidth : 0.22 \n"),
                    #paste("Ajuste3\nSplines de Regressão\nCubico\nNós : c(0.5,0.75,1.25)")
                   )

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

#plot.mult.curves(df = df1,title = NULL)


```


```{r}

library(splines)
library(ISLR)
attach(Wage)
set.seed(103159)
df <- data.frame(x = x,y = y)

tr= sample(1:nrow(df), nrow(df)/2)
te= (-tr)
dftr= df[tr,]
# Tuning parameter= the bins, suggest some possible values
number_of_bins = seq(1,20)
k = nrow(dftr)
folds = sample( x = 1:k, size = nrow(dftr), replace=FALSE ) 
cv.errors = matrix( NA, k, length(number_of_bins) )

for( i in 1:length(number_of_bins)){ # for each number of knots to test
  
  for( j in 1:k ){ # for each fold
    cubicfit = glm( y ~ bs( x, df=i), data=dftr[folds!=j,] )
    cubicpred = predict( cubicfit, newdata=dftr[folds==j,] )
    cv.errors[j,i] = mean( ( dftr[folds==j,]$y - cubicpred )^2 ) }
  
}

cv.errors.mean = apply(cv.errors,2,mean)
cv.errors.stderr = apply(cv.errors,2,sd)/sqrt(k)

min.cv.index = which.min( cv.errors.mean )
one_se_up_value = ( cv.errors.mean+cv.errors.stderr )[min.cv.index] 

# Set up the x-y limits for plotting:
min_lim=min( one_se_up_value, cv.errors.mean, cv.errors.mean-cv.errors.stderr, cv.errors.mean+cv.errors.stderr ) * 0.9
max_lim=max( one_se_up_value, cv.errors.mean, cv.errors.mean-cv.errors.stderr, cv.errors.mean+cv.errors.stderr ) * 1.1

df <- data.frame(x = x, y = y)
df1 <- data.frame(x = number_of_bins, y = cv.errors.mean,EQM = cv.errors.mean)



colnames(df1) <- c("x","y",
                    paste("EQM"))

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

p3<-ggplot(df1,aes(x = x,y=y))+
  geom_line()+
  geom_point()+
  labs(x = "Binwidth",y = "EQM")+
  geom_vline(xintercept = min.cv.index,color ="red")+
  annotate("text",x = min.cv.index+0.05,y = max(cv.errors.mean),label=min.cv.index) +
  axis.theme()




# plot( number_of_bins, cv.errors.mean, ylim=c(min_lim,max_lim), pch=19, type='b', xlab='number of cut bins', ylab='CV estimate of the prediction error' )
# lines( number_of_bins, cv.errors.mean-cv.errors.stderr, lty='dashed' )
# lines( number_of_bins, cv.errors.mean+cv.errors.stderr, lty='dashed' )
# abline( h=one_se_up_value, col='red' )




```

```{r,fig.cap="Gráfico contendo ajuste consideran valor ótimo para a quantidade Nós.", fig.height=2}
library(car)

nob = min.cv.index

fit.best.spline = glm( y ~ bs( x, df=nob), data= df)

# dfte= df[te,]
# plot( dfte$x, dfte$y )
# aRng = range(dfte$x)
# 
# Xage= seq( from=aRng[1], to=aRng[2], length.out=100 )
# Ywage= predict( fit, newdata=list( x=Xage ) )
# lines( Xage, Ywage, col='red', lw=4 )


df1      <-  cbind( df,"Spline Cúbico" = fit.best.spline$fitted.values) %>% as.data.frame


colnames(df1) <- c("x","y",
                    paste0("Ajuste1\nSpline Cúbico\nNós : ", min.cv.index, "\n")
                    #paste("Ajuste2\nKernel Smoother\nBandwidth : 0.22 \n"),
                    #paste("Ajuste3\nSplines de Regressão\nCubico\nNós : c(0.5,0.75,1.25)")
                   )

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

#plot.mult.curves(df = df1,title = NULL)


```

```{r,fig.cap="Comparação entre os ajustes considerando os parâmetros ótimos.", fig.height=2}





partial_plots <- cowplot::plot_grid(p1,p2,p3,ncol=3,labels=LETTERS[1:3]);partial_plots

```

```{r,fig.cap="Comparação entre os ajustes considerando os parâmetros ótimos.", fig.height=2}
library(car)

# nob = min.cv.index
# fit.best.spline = glm( y ~ bs( x, df=nob), data= df)

# dfte= df[te,]
# plot( dfte$x, dfte$y )
# aRng = range(dfte$x)
# 
# Xage= seq( from=aRng[1], to=aRng[2], length.out=100 )
# Ywage= predict( fit, newdata=list( x=Xage ) )
# lines( Xage, Ywage, col='red', lw=4 )


df1      <-  cbind( df,
                    "Loess" = fit.best.loess$fitted,
                    "Kernel Smoother" = fit.best.kernel$y,
                    "Spline Cúbico" = fit.best.spline$fitted.values) %>% as.data.frame


colnames(df1) <- c("x","y",
                    paste0("Ajuste1\nLoess\nSpan : ", spangcv, "\n"),
                    paste("Ajuste2\nKernel Smoother\nBandwidth :", fit.kernel$best.bw, "\n"),
                    paste("Ajuste3\nSplines de Regressão\nCubico\nNós :", min.cv.index, "\n")
                   )

df1 <- as.tibble(df1) %>%
  gather(key = "variable", value = "value",-x,-y)

plot.mult.curves(df = df1,title = NULL)


```


```{r,include=T,echo=FALSE}
library(Metrics)
df1      <-  cbind( df,
                    "Loess" = fit.best.loess$fitted,
                    "Kernel Smoother" = fit.best.kernel$y,
                    "Spline Cúbico" = fit.best.spline$fitted.values) %>% as.data.frame
df_metrics <- data.frame(Smoother = c("Loess","Kernel","Cubic Spline"),
                         EQM      =  c(rmse(actual = df1$y,df1$Loess),rmse(actual = df1$y,df1$Kernel),rmse(actual = df1$y,df1$Spline)))
kable_data(data = df_metrics,cap = "Erro Quadrático Médio para os suavizadores Loess, Kernel e Spline Cúbico",foot = NULL)
```





  \subsubsection{Cenário 3}

\hspace{1.25cm} Iremos agora incluir os modelos aditivos para dados simulados considerando duas covariáveis. Foram geradas 250 observações, $x_1 \sim N(5,20)$ e $x_2 \sim N(75,15)$ e os valores das funções reais para $f_1(x_1)$ e $f_2(x_2)$ foram gerados de funções quadráticas, sendo elas:


$f_1(x_1) = \frac{25(x_1-20) ^2 - (x_1 - 20)}{20}$ e $f_2(x_2) = \frac{-25(x_2-100)^2 - (x_2 - 100)}{8} + 2000$


Esta sendo considerado um valor aleatório para o termo $\alpha$ do modelo, os erros $\varepsilon$ são normais com média 0 e variância constante ($\varepsilon \sim N(0,200)$). Os valores para y foram gerados da seguinte forma:

$$y = \alpha + f_1(x_1) + f_2(x_2) + \varepsilon$$

A Figura 16 mostra o comportamento dos resíduos parciais de $f_1(x_1)$ e $f_2(x_2)$ e em vermelho suas respectivas curvas reais.


```{r, fig.cap="Gráfico de dispersão contendo as curvas reais simuladas, A - $f_1(x_1)$ e B - $f_2(x_2)$.", fig.height=2.5}
rm(list=ls())
source(file = "funcoes.R",encoding = "UTF-8")
library(tidyverse)
library(binsmooth)
library(knitr)
library(kableExtra)
library(additive.models)
library(splines)
library(cowplot)
library(dplyr)
library(ggplot2)
my.norm.2 <- function(x) sqrt(sum(x^2))

set.seed(103159)
alpha = 500
x1           <- rnorm(n = 250,5,20)
x1_ord       <- order(x1)
#x1           <- sort(sample(x = x1,size = 600,replace = FALSE))
fx1          <- (25*(x1-20)**2 - (x1-20)) / 20


x2           <- rnorm(n = 250,75,15) 
x2_ord       <- order(x2)
fx2          <- ((-25*(x2-100)**2 - (x2-100))  / 8) + 2000

# par(mfrow=c(1,2))
# plot(x1,fx1)
# plot(x2,fx2)



norms        <- rnorm(length(x1), 0, 200)

y_real           <- alpha + fx1 + fx2 
y_sim            <- alpha + fx1 + fx2 + norms
res.x1           <- y_sim - alpha - fx2
res.x2           <- y_sim - alpha - fx1 

# plot(x = x1,y = res.x1)
# plot(x = x2,y = res.x2)


p3           <- plot.curves(data = NULL, x =  x1, y = res.x1,fit = fx1[x1_ord],
                            type = "g",title = NULL)  + axis.theme(title_size = 10)
p4           <- plot.curves(data = NULL, x =  x2, y = res.x2,fit = fx2[x2_ord],
                            type = "g",title = NULL) + axis.theme(title_size = 10)
partial_plots <- cowplot::plot_grid(p3,p4,ncol=2,labels=LETTERS[1:2]);partial_plots


```


Iremos aplicar o algoritmo de retroajuste (\textit{backfitting}), para conjunto de dados simulados $(y,x_1,x_2)$ com o intuito de captar  a mesma tendência quando observamos a Figura 16, comparando dois ajustes utilizando as técnicas \textit{Loess} e \textit{Splines} de Regressão Cúbico. Aplicando o algoritmo nos dados e obtendo as estimativas e resíduos parciais para cada função $f_1(x_1)$ e $f_2(x_2)$. Na Figura 17 (gráfico A) temos os ajuste e resíduos parciais para $f_1(x_1)$, onde temos dois ajustes, o Ajuste1 (técnica suavizadora \textit{Loess}) foi obtido utilizando um valor para o parâmetro $span = 0.15$. Para o Ajuste2  (técnica suavizadora \textit{splines} de regressão cúbico) de primeiro momento foi tentado utilizar os nós distribuídos de forma equidistante, porém conseguimos um melhor ajuste utilizando um nó posicionado no quantil $x= -25$. Portanto percebemos que utilizando os parâmetros em questão para estes dados simulados, utilizando o algoritmos de reatroasjute, conseguimos de forma satisfatória recuperar a forma real para a função $f_1(x_1)$. Quando observamos os ajustes vemos as curvas dos dois ajustes são bem próximas, os pontos dos resíduos parciais se encontram dispostos ao longo da curva indicando que os ajustes sejam adequados para representar estes dados.


```{r, fig.cap="Ajuste e resíduos parciais para $f(x_1)$", fig.height=2.5}

y     <- y_sim   
x     <- cbind(x1,x2)
  
fit1   <- backfitting2(x=x,y=y,span = c(.15,.18),bdw = c(20,75),k = c(8,8),method = "loess")
fit2   <- backfitting2(x=x,y=y,span = c(.15,.35),bdw = c(20,75),k = c(8,8),method = "sr.linear")
fit3   <- backfitting2(x=x,y=y,span = c(.15,.35),bdw = c(20,75),k = c(1,1),method = "sr.cubic",knots = list(v1=c(-25),v2=c(25)))
lim_y_x1  <- range(fit1$y.x1, (fit1$fx_11-mean(fit1$fx_11)),fx1)
lim_y_x2  <- range(fit1$y.x2, (fit1$fx_21-mean(fit1$fx_21)),fx2)
df        <- as.data.frame(cbind(x1,fit1$y.x1,fit1$fx_11,fit3$fx_11))

colnames(df) <- c("x","y",
                    paste("\nAjuste1\nLoess\n", "Span : ",.15),
                    paste("\nAjuste2 \nSplines Reg. Cúbico\n", "Nós : c(-25)")
                   
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

 plot1 <- ggplot(data = df,aes(x=x,y=y))+
    geom_point(alpha=point.alpha,size=point.size, col = point.color)+
    labs(x = "Eixo X", 
         y = "Eixo Y",
         color = "Legenda")+
    xlim(range(x1)+c(-0,+0))+
    ylim(lim_y_x1)+
    #ggtitle(paste0("Gráfico de dispersão \n Resíduos parciais de f(x1)"))+
    geom_line(aes(x=x,y = value,color=variable),size=line.size,alpha=line.alpha) +
    scale_color_manual(values = cores) + axis.theme(title_size = 10,pos_leg = "right")

```



```{r, fig.cap="Ajuste e resíduos parciais para $f(x_1)$ e $f(x_2)$",fig.height=5}
df        <- as.data.frame(cbind(x2,fit1$y.x2,fit1$fx_21,fit3$fx_21))
colnames(df) <- c("x","y",
                    paste("\nAjuste1\nLoess\n", "Span : ",.18),
                    
                    paste("\nAjuste2\nSplines Reg. Cúbico\n", "Nós : c(25)")
                   
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

 plot2 <- ggplot(data = df,aes(x=x,y=y))+
    geom_point(alpha=point.alpha,size=point.size, col = point.color)+
    labs(x = "Eixo X", 
         y = "Eixo Y",
         color = "Legenda")+
    xlim(range(x2)+c(-0,+0))+
    ylim(lim_y_x2)+
    #ggtitle(paste0("Gráfico de dispersão \n Resíduos parciais de f(x2)"))+
    geom_line(aes(x=x,y = value,color=variable),size=line.size,alpha=line.alpha) +
    scale_color_manual(values = cores) + axis.theme(pos_leg = "right")

 
 cowplot::plot_grid(plot1,plot2,nrow = 2,ncol=1,labels=LETTERS[1:2])
save_plot(filename = "ajuste2.png",plot = plot2)
```
 

Quando observamos a Figura 17 (gráfico B), para o Ajuste1 (\textit{Loess}), utilizamos um $span = 0.18$,  e para Ajuste2, utilizamos um nó posicionado no quantil $x= 25$. Quando avaliamos as curvas ajustadas para os resíduos parciais de $f_2(x_2)$ percebemos que os pontos residuais se encontram dispostos aleatoriamente em torno das curvas, logo conseguimos captar utilizando os parâmetros citados anteriormente,para estes dados, de forma adequada a tendência de $f_2(x_2)$.



Embora todas as curvas em ambos ajustes ($f_1(x_1)$ e $f_2(x_2)$), sejam semelhantes, destacamos que, quando observamos atentamente o Ajuste2 (\textit{Splines} de Regressão Cúbico), visualmente para a simulação em questão, se adequa e capta melhor a tendência dos dados para ambas as funções. Até o momento estamos utilizando métodos visuais, utilizando gráficos de dispersão para concluirmos quais ajustes aparentam se adequar e captar melhor a tendência dos dados. Relembramos que para as técnicas vistas até o momento a escolha do melhor parâmetro \textit{span} ou nós, pode ser uma tarefa dificil.
 




\subsection{Aplicações}

  \subsubsection{Aplicação 1}

\hspace{1.25cm} O banco de dados é constituído por 19 observações relacionadas ao consumo de energia em hotéis de luxo na província de Hainan, China. Além da variável consumo de energia (\textit{enrgcons}), medida em kilowatt-horas, há outras cinco variáveis: área em quilômetros quadrados (\textit{area}), idade em anos (\textit{age}), número de quartos de hóspedes (\textit{numrooms}), taxa de ocupação em porcentagem (\textit{occrate}) e número efetivo de quartos de hóspedes (\textit{effrooms}), esse último dado como uma função de outras duas covariáveis (nº de quartos * taxa de ocupação/100). 

O objetivo é encontrar o modelo de regressão com as variáveis que melhor explicam o consumo de energia. Na Figura 18 tem-se os gráficos de dispersão de cada covariável versus a variável resposta equanto que na Tabela 2 é apresentada a matriz de correlação amostral dos dados.


```{r h1, message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Gráficos de dispersão da variável resposta Consumo de energia versus cada uma das preditoras"}

library(ggplot2)
library(gridExtra)

dados <- read.table("hotel_energy.csv", header=T, sep=",", dec=".") 

# retirei a coluna "hotel" pois era somente a identificacao do hotel
dados <- dados[,-1]

p1 = ggplot(aes(dados$area, dados$enrgcons), data= dados) + geom_point(size = 3, alpha = .5, color = "darkgrey") +
        ylab("Cons. de Energia") + xlab("Area")

p2 = ggplot(aes(dados$age, dados$enrgcons), data= dados) + geom_point(size = 3, alpha = .5, color = "darkgrey") +
        ylab("Cons. de Energia") + xlab("Idade")

p3 = ggplot(aes(dados$numrooms, dados$enrgcons), data= dados) + geom_point(size = 3, alpha = .5, color = "darkgrey") +
        ylab("Cons. de Energia") + xlab("Num. de quartos")

p4 = ggplot(aes(dados$occrate, dados$enrgcons), data= dados) + geom_point(size = 3, alpha = .5, color = "darkgrey") +
        ylab("Cons. de Energia") + xlab("Taxa de ocupacao")

p5 = ggplot(aes(dados$effrooms, dados$enrgcons), data= dados) + geom_point(size = 3, alpha = .5, color = "darkgrey") +
        ylab("Cons. de Energia") + xlab("Num. efetivo de quartos")

grid.arrange(p1, p2, p3, p4, p5, ncol=3)

```

\begin{table}[H] 
    \centering
    \caption{Matriz de correlação}
    \begin{tabular}{c c c c c c c}
    \hline 
    & enrgcons   &  area  &  age &  numrooms  &   occrate  &  effrooms \\
    \hline
enrgcons & 1.00  & 0.87 & -0.09 & 0.68  & 0.031 & 0.65 \\
area     & 0.87  & 1.00 & -0.23 & 0.85  & -0.21 & 0.65 \\
age      & -0.09 & -0.23 & 1.00 & -0.27 & 0.40 & -0.045 \\
numrooms & 0.68  & 0.85 & -0.27 & 1.00  & -0.15 & 0.82 \\
occrate  & 0.03  & -0.21 & 0.40 & -0.15 & 1.00 & 0.40 \\
effrooms & 0.65  & 0.65 & -0.04 & 0.82  & 0.40 & 1.00 \\\hline
    \end{tabular}
    \label{tab:tab1}
\end{table}


A partir da Tabela \ref{tab:tab1} percebe-se que o consumo de energia (\textit{enrgcons}) tem correlação considerável com \textit{area} (0.879) ,  \textit{numrooms} (0.685) e   \textit{effrooms} (0.657). As covariáveis \textit{area} e \textit{numrooms} têm uma correlação relativamente alta (0.853), bem como as covariáveis \textit{numrooms} e \textit{effrooms} (0.826). Essa correlação entre as covariáveis pode ser vista como uma indicação de multicolinearidade. Analisando o Fator de Inflação da Variância (VIF) das variáveis explicativas é possível perceber o valor elevado para a variável \textit{effroms} (47.45), o que faz muito sentido já que ela é função de outras covariáveis, logo essa pode ter sido a razão desse problema de multicolinearidade. Dessa forma, essa covariável foi retirada e, calculando o VIF novamente, todos os valores ficaram satisfatórios. 


Logo após foi realizada a seleção de variáveis através do método \textit{backward} e vale ressaltar que  esse método leva em considereção os pvalores, logo foi preciso validar a suposição de normalidade dos dados, para isso foi analisada a normalidade dos resíduos para o modelo através dos testes de Shapiro-Wilk e Kolmogorov-Smirnov. A um nível de significância $\alpha = 5\%$, em ambos os testes não rejeitou-se a hipótese de que os resíduos provêm de uma distribuição Normal, logo há evidências da normalidade dos resíduos desse modelo.

Voltando ao método \textit{backward}, o submodelo final ficou com as covariáveis área e occrate (taxa de ocupação), sendo que esse modelo realmente é significativo, já que através do teste F parcial entre esse e o modelo completo ficou claro que as covariáveis age e numrooms não são significativas.




```{r h2, echo=FALSE,include=FALSE, message=FALSE, warning=FALSE, fig.cap="Gráficos dos resíduos"}

fit1 <- lm(enrgcons ~ area + occrate, data = dados)

# ANALISE DE RESIDUOS

## Residuos padronizados
d = rstandard(fit1)

## Residuos Studentizados
r = rstudent(fit1)

## Residuo Deletado studentizado (externamente)

n <- nrow(dados)    # tamanho da amostra
p <- 2              # numero de covariaveis
t = r*((n-p-2)/(n-p-1-r^2))^0.5


par(mfrow=c(2,3))

#  residuos vs valores ajustados  
plot(fitted(fit1), d, main = 'Res. padronizados', xlab = "Valores ajustados", pch=19)
abline(h=0, col = 2)

plot(fitted(fit1), r, main = 'Res. studentizados', xlab = "Valores ajustados", pch=19)
abline(h=0, col=2)

plot(fitted(fit1), t, main = 'Res. deletado studentizado', xlab = "Valores ajustados", pch=19)
abline(h=0, col=2)

# Suposicao de independencia 
plot(dados$enrgcons, t, xlab = "Consumo de energia", ylab = "Res. Stud. deletado", pch = 19,
     main = "Residuos vs ordem de coleta")  # res deletado studentizado
abline(h=0, col=2)

# normalidade
plot(fit1, which = 2)


```





 
 
```{r h3, echo=FALSE,include=FALSE,message=FALSE, warning=FALSE, fig.cap="Gráficos das medidas específicas para verificar a presença de outliers"}
# VERIFICANDO A PRESENCA DE OUTLIERS / PONTOS INFLUENTES
par(mfrow = c(2,3))

# DISTANCIA DE COOK 
plot(cooks.distance(fit1), xlab = '', ylab = 'Dist. de Cook', pch=19, main = "Distancia de Cook")
abline(h = 1, col="red")

# DFFITS 
plot(abs(dffits(fit1)), xlab = '', ylab = 'DFFIT', pch=19, main = "DFFIT")
abline(h=(2*sqrt(3/19)), col=2)

# DFBETAS
plot(abs(dfbetas(fit1)[,'area']), xlab = '', ylab = 'DFBETA', pch=19, main ="DFBETAS - area")
abline(h=(2/sqrt(19)), col=2)

plot(abs(dfbetas(fit1)[,'occrate']), xlab = '', ylab = 'DFBETA', pch=19, main ="DFBETAS - occrate")
abline(h=(2/sqrt(19)), col=2)


plot(hatvalues(fit1), xlab = '', ylab = 'hat', pch=19, main = "Valores de alavancagem")


``` 
 

 
```{r h4, message=FALSE, include=FALSE,warning=FALSE, echo=FALSE, fig.cap="Gráficos dos resíduos"}

dados2 <- dados[-6, ]

# modelo sem o ponto influente
fit2 <- lm(enrgcons ~ area + occrate, data = dados2)
# ANALISE DE RESIDUOS fit2

# plotando os mesmos graficos para o modelo fit2 (sem o outlier)
par(mfrow=c(2,3))

plot(fitted(fit2), rstandard(fit2),main = 'Res. padronizados', xlab = "Valores ajustados", 
     ylab = 'd', pch=19)
abline(h=0, col = 2)

plot(fitted(fit2), rstudent(fit2), main = 'Res. studentizados', xlab = "Valores ajustados", 
     ylab = 'r',pch=19)
abline(h=0, col=2)

n2 = nrow(dados2)
r2 = rstudent(fit2)
t2 = r2*((n-p-2)/(n-p-1-r2^2))^0.5

plot(fitted(fit2), t2, main = 'Res. studentizado deletado', xlab = "Valores ajustados", 
     ylab = 't', pch=19)
abline(h=0, col=2)

# Suposicao de independencia 
plot(dados2$enrgcons, t2, main = 'Residuos vs ordem de coleta', xlab = 'Consumo de energia', 
     ylab = 'Res. Stud. Deletado', pch = 19)
abline(h=0, col=2)
# aparentemente nao ha padrao de alternancia ou sequencias de residuos positivos ou negativos,
# logo os residuos parecem ser independentes

# suposicao de normalidade qqplot
plot(fit2, which = 2)

 

```
 
 

Pode ser observado na Figura 19, forte  relação linear entre os valores ajustados e a covariável area, Ja entre os valores ajustados e a covariável Taxa de ocupação (occrate) a relação linear não é tão acentuada. Ressalta-se que o intercepto também é significativo para o modelo. 










```{r h5, message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Gráficos de dispersão dos valores ajustados versus area e taxa de ocupação, respectivamente"}
# valor ajustado vs variaveis explicativas
par(mfrow = c(1,2))

plot(dados2$area, fitted(fit2), xlab = 'Area', ylab = 'Valores ajustados', pch=19)
plot(dados2$occrate, fitted(fit2), xlab = 'Taxa de ocupação', ylab = 'Valores ajustados', pch=19)

```

Assim, o modelo linear final ajustado é dado por

\begin{equation*}
    \hat{y} = -6.335e^{06} + 2.070e^{02} \mbox{area} + 6.348e^{06} \mbox{occrate}
\end{equation*}

Os parâmetros estimados do modelo podem ser interpretados da seguinte forma: 

\begin{itemize}
    \item $\hat{\beta_0}$: nesse caso o intercepto não tem interpretação prática, já que nos dados não há uma observação na qual $x_1 = x_2 = 0$;
    \item $\hat{\beta_1}$: quando a taxa de ocupação (\textit{occrate}) é constante, a cada mudança unitária no metro 
    quadrado do hotel, é esperado que o consumo de energia aumente em $2.070e^{02}$ kilowatt-hora;
    \item $\hat{\beta_2}$: quando a área é constante, a cada mudança unitária na taxa de ocupação do hotel, é esperado que o consumo de energia aumente em $6.348e^{06}$ kilowatt-hora.
\end{itemize}

Nesse momento, vamos ajustar o modelo aditivo ao mesmo conjunto de dados. Foram consideradas as 5 variáveis independentes do problema inicial, sendo que as mesmas foram definidas como $X_1: area$, $X_2: age$, $X_3: numrooms$, $X_4: effrooms$ e $X_5: occrate$. A partir disso, o modelo aditivo será da forma

\begin{equation*}
  y = \alpha + f_1(X_1) + f_2(X_2) + f_3(X_3) + f_4(X_4) + f_5(X_5) +\epsilon
\end{equation*}
 
Neste caso, foi utilizado o algoritmo de retroajuste para estimar as funções do componente sistemático através de duas técnicas de suavização vistas anteriormente: Kernel e \textit{Loess}, obtendo-se gráfico a seguir:

```{r echo=FALSE, fig.cap="Curvas ajustadas", message=FALSE, warning=FALSE}

library(ggplot2)
library(gridExtra)

dados <- read.table("hotel_energy.csv", header=T, sep=",", dec=".")

# retirei a coluna "hotel" pois era somente a identificacao do hotel
dados <- dados[,-1]

# ajustando o modelo completo
fit2 <- lm(enrgcons ~ ., data = dados)

x = cbind(dados$area, dados$age, dados$numrooms, dados$effrooms, dados$occrate)
# x1 = area, x2 = age,...


# y = b0 + f(x1) + f(x2)+ f(x3) + f(x4) + f(x5) erro
y = fit2$coefficients[1] + (fit2$coefficients[2]*dados$area) + (fit2$coefficients[3]*dados$age)+ 
  (fit2$coefficients[4]*dados$numrooms)+ (fit2$coefficients[5]*dados$occrate) + (fit2$coefficients[6]*dados$effrooms)

# suavizador com loess
centered.smoothing <- function(x,y) {
  require(splines)
  fit <- lm(y ~ bs(x, knots = c(0)))$fitted
  return(fit - mean(fit))
}

backfit <- function(x,residuals,f) {
  stopifnot(identical(dim(x),dim(f)),nrow(x)==length(residuals))
  p <- ncol(x)
  g <- f
  n = nrow(x)
  partial.residuals = matrix(0, nrow = n, ncol = p)
  for (j in 1:p) {
    partial.residuals[,j] <- residuals + f[,j]
    g[,j] = centered.smoothing(x[,j],partial.residuals[,j])
    residuals <- residuals +f[,j] - g[,j]
  }
  result = list(g = g, res.parciais = partial.residuals)
  return(result)
}

am <- function(x,y,maxiter=100,tol=0.00001) {
  x = as.matrix(x)
  n <- nrow(x)
  stopifnot(length(y)==n)
  p <- ncol(x)
  alpha <- mean(y)
  residuals <- y-alpha
  f <- matrix(0,nrow=n,ncol=p)
  converged <- FALSE
  i <- 0
  while (!converged & (i < maxiter)) {
    g = backfit(x,residuals,f)$g
    res.parciais = backfit(x,residuals,f)$res.parciais
    change.in.fitted.values <- rowSums(f) - rowSums(g)
    residuals <- residuals + change.in.fitted.values
    converged <- all(abs(change.in.fitted.values) < tol)
    f <- g
    i <- i+1
  }
  fitted <- rowSums(f) + alpha
  fit <- list(fitted=fitted,residuals=residuals,mean=alpha,
              partials=f,x=x,converged=converged,iterations=i,
              tol=tol, res.parciais = res.parciais)
  class(fit) = "am"
  return(fit)
}

# estimando as curvas

# com suavizador loess
mag.loess = am(x,y)

x1 = data.frame(x[,1], mag.loess$res.parciais[,1])
colnames(x1) = c("x1", "res_x1")

x2 = data.frame(x[,2], mag.loess$res.parciais[,2])
colnames(x2) = c("x2", "res_x2")

x3 = data.frame(x[,3], mag.loess$res.parciais[,3])
colnames(x3) = c("x3", "res_x3")

x4 = data.frame(x[,4], mag.loess$res.parciais[,4])
colnames(x4) = c("x4", "res_x4")

x5 = data.frame(x[,5], mag.loess$res.parciais[,5])
colnames(x5) = c("x5", "res_x5")

# com a vermelha = loess
# com a verde     = kernel

ks1 = ksmooth(x1$x1, x1$res_x1, kernel = "normal", bandwidth = 20000, n.points = length(x1$x1))

plot1 = ggplot(aes(x1, res_x1), data= x1) + geom_point(size = 3, alpha = .5, color = "darkgrey") + 
  geom_line(aes(x1, mag.loess$partials[,1]), color="red") + geom_line(aes(ks1$x, ks1$y), color="green") + 
  xlab(expression(x[1])) +   ylab(paste0("Res."," x1")) + axis.theme() 


ks2 = ksmooth(x2$x2, x2$res_x2, kernel = "normal", bandwidth = 4, n.points = length(x2$x2))

plot2 =  ggplot(aes(x2, res_x2), data= x2)+ geom_point(size = 3, alpha = .5, color = "darkgrey") + 
  geom_line(aes(x2, mag.loess$partials[,2]), color="red") + geom_line(aes(ks2$x, ks2$y), color="green") +
  xlab(expression(x[2])) + ylab(paste0("Res."," x2")) + axis.theme()


ks3 = ksmooth(x3$x3, x3$res_x3, kernel = "normal", bandwidth = 180, n.points = length(x3$x3))

plot3 =  ggplot(aes(x3, res_x3), data= x3)+ geom_point(size = 3, alpha = .5, color = "darkgrey") + 
  geom_line(aes(x3, mag.loess$partials[,3]), color="red") + geom_line(aes(ks3$x, ks3$y), color="green") +
  xlab(expression(x[3])) +  ylab(paste0("Res."," x3")) + axis.theme()

ks4 = ksmooth(x4$x4, x4$res_x4, kernel = "normal", bandwidth = 90, n.points = length(x4$x4))

plot4 =  ggplot(aes(x4, res_x4), data= x4)+ geom_point(size = 3, alpha = .5, color = "darkgrey") + 
  geom_line(aes(x4, mag.loess$partials[,4]), color="red") + geom_line(aes(ks4$x, ks4$y), color="green") +
  xlab(expression(x[4])) +  ylab(paste0("Res."," x4")) + axis.theme()

ks5 = ksmooth(x5$x5, x5$res_x5, kernel = "normal", bandwidth = 0.2, n.points = length(x5$x5))

plot5 =  ggplot(aes(x5, res_x5), data= x5)+ geom_point(size = 3, alpha = .5, color = "darkgrey") + 
  geom_line(aes(x5, mag.loess$partials[,5]), color="red") + geom_line(aes(ks5$x, ks5$y), color="green") +
  xlab(expression(x[5])) + ylab(paste0("Res."," x5")) + axis.theme()

 grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol=2)

```

O valor de $\hat{\alpha}$ para o modelo foi de `r format(mag.loess[["mean"]],scientifc = F) ` e a Figura 20 é apresentado a curva ajustada pelo método \textit{Loess} em vermelho e pelo método Kernel em verde. Pode-se perceber visualmente que o comportamento dos pontos, que são os resíduos parciais de cada função, não é explicado da mesma forma pelos dois métodos, porém ambos captam bem a tendência dos dados. Nota-se que as curvas em vermelho são mais suaves, enquanto que as curvas em verde têm uma oscilação maior. Portanto, para as técnicas apresentadas para os dados em questão, o ajuste através do método \textit{Loess} foi mais interessante, já que as curvas possuem uma variabilidade menor.

Diferente do que ocorreu no ajuste do modelo linear, podemos ver graficamente que todas as covariáveis contribuem de forma significativa para explicar a variável resposta consumo de energia (enrgcons)", já que nenhuma das funções suavizadas tem o comportamento de uma reta constante. Desta forma, tem-se um ajuste mais adequado, se comparado ao ajuste linear.


**Inserir as métricas para verificar as predições**
 
  \subsubsection{Aplicação 2}

\hspace{1.25cm} Os dados em questão são resultados de um experimento relacionado  ao rendimento de bombas de poço contendo 54  observações composto pelas variáveis:

```{r Os Dados}
source(file = "funcoes.R",encoding = "UTF-8")
dados        <-    read.table(file = "aquiferborehole.csv",
                         header = T,sep = ",",encoding = "UTF-8")
dados        <-    dados[,2:length(dados)]
var.resposta <- 6
dados2       <- dados[,c(3:5,var.resposta)]
y            <- dados[,var.resposta]
modelo       <- lm(data = dados2,formula = bh.yield ~ I(aq.resist) + I(aq.thick) + I(anisotrophy) )
```

* **easting**  : coordenadas relacionadas a posição leste do poço;

* **northing** : coordenadas relacionadas a posição norte do poço;

* **aq.resist** : resistividade do aquecedor;

* **aq.thick	** : espessura;

* **anisotrophy	** : coeficiente de anisotrofia;

* **bh.yield	** : rendimento dos testes da bomba de poço ($litros/s$).

Iremos ajustar um modelo de regressão que explique rendimento dos testes da bomba de poço relacionando as três variáveis geologicas  que são: resistividade do aquecedor; espessura; coeficiente de anisotrofia. A Figura 21 mostra os box-plots para cada variável que será utilizada no ajuste.


```{r Resumo Descritivo,echo=FALSE}
medidas   <- summary(dados2)
var       <- paste0("Var : ",round(apply(dados2,2,var),3))
resumo    <- rbind(medidas,var)
```


```{r,include=F}
kable_data(data = resumo,cap = "Tabela resumo contendo as medidas descritivas para cada variável",foot = NULL)
```

```{r  echo=FALSE,fig.cap="Box plots para as variáveis do conjunto de dados",fig.height=2.5}
source(file = "funcoes.R",encoding = "UTF-8")
b1 <- boxggplot(data = dados$aq.resist,title = "Resistividade",xtitle = "Resistividade",ytitle = "Valores")
b2 <- boxggplot(data = dados$aq.thick,title = "Espessura",xtitle = "Espessura",ytitle = "Valores")
b3 <- boxggplot(data = dados$anisotrophy,title = "coeficiente de anisotrofia",xtitle = "coeficiente de anisotrofia",ytitle = "Valores")
b4 <- boxggplot(data = dados$bh.yield,title = "   Rendimento do Poço",xtitle = "Rendimento do Poço",ytitle = "Valores")

cowplot::plot_grid(b1$plot,b2$plot,b3$plot,b4$plot)


```

Ao observar a variável resistividade do aquecedor podemos observar uma variância alta (2844,57), o que pode ser explicado pelos pontos discrepantes que se encontram dispersos acima de 200, enquanto 75% dos valores se encontram abaixo de 135,8. Ainda ao observarmos o rendimento dos poços encontramos pontos discrepantes que se encontram próximos e acima de 3 e que 75% dos dados se encontram abaixo  1,8. Podemos observar na variável espessura uma leve assimetria e pontos outliers na variável coeficiente de anisotrofia.

A Tabela \ref{tab:corrap2} contém a matriz de correlações para as variáveis onde verifica-se que  não apresentam forte correlações entre si, sendo a mais alta observada entre a variável resistividade do aquecedor e coeficiente de anisotrofia com um valor de 0,68. O valor mais alto para os fatores de inflação foi de 2,67, portanto não temos indícios de que as variáveis possuam variâncias inflacionadas devido a colinearidade, fazendo com que as estimativas para o modelo sejam ruins. 

```{r Correlação ,echo=FALSE}
correlacao <- cor(dados2)
```



```{r,echo=FALSE}
kable_data(data = round(correlacao,2),cap = "Tabela contendo as correlações entre cada variável",foot = NULL,label = "corrap2")
```



```{r Multicolinearidade,echo=FALSE}
VIF        <- car::vif(modelo)
```

```{r, include=F}
kable_data(data = t(VIF),cap = "Valor dos Fatores de Inflação da Variância para cada variável",foot = NULL)
```



Ajustando o modelo linear, considerando todas as covariáveis, a Tabela \ref{tab:fit1} contém estimativas para o modelo ajustado completo  e os valores das estatística t e F, para o teste de ANOVA, para avaliarmos se ao menos alguma das covariáveis influenciam de forma significativa para explicar o rendimento das bombas de poço. Verificamos que o p-valor da estatistica F é zero, portanto como um nível de significância de 5%, temos indícios de que ao menos uma das covariáveis influenciam de forma significativa para explicar o redimento das bombas dos poços. 


```{r,echo=FALSE}
modelo                                                  %>% kable_estimates(cap = "Estimativas para o modelo ajustado completo",label= "fit1")
```



```{r Seleção de variáveis,echo=FALSE}
library(olsrr)
best   <- ols_step_best_subset(modelo)
``` 


Em seguida fora utilizado uma função obtida da biblioteca "olsrr", chamada "ols_step_best_subset()", que retorna os melhores ajustes de acordo com cada quantidade de covariáveis que o modelo pode obter, ou seja, a função retorna com uma única covariável o modelo obteve o melhor ajuste, em seguida seleciona qual ajuste com 2 covariáveis obtem o melhor ajuste e assim por diante. Podemos ver na tabela \ref{tab:best1}, quais covariáveis foram consideradas como melhores ajustes:

```{r,echo=FALSE}
best[,c("rsquare","adjr")] <- format(round(best[,c("rsquare","adjr")],2),decimal.mark = ",")
best[,c("predictors","rsquare","adjr")]                 %>% kable_data(cap = "3 Melhores Ajustes",label = "best1")
```
Os três melhores modelos que podemos ajustar e levando em consideração o coeficiente de determinação ajustado $R^2_{adj}$, o melhor modelo ajustado com uma covariável obtendo assim um $R^2_{adj}=0,83$, seria com a covariável resistividade do aquecedor (aq.resist). Ainda o melhor ajuste com duas covariáveis ajustado seria com a covariável resistividade e coeficiente de anisotrofia obtendo assim um  $R^2_{adj}=0,96$. E por fim a função sempre retornara o modelo ajustado completo. Porém, ao avaliarmos seus respectivos coeficientes de determinação podemos notar que, o segundo modelo sugerido com apenas duas covariáveis consegue explicar aproximadamente a mesma variabilidade para nossa variável resposta, quando comparamos com o modelo ajustado completo. 
Levando em consideração o que vimos anteriormente um modelo ajustado com duas covariáveis possuira quase o mesmo efeito explicativo quando comparado com o modelo ajustado completo. Porém se analisarmos o p-valor para o teste de significância de $\beta_3$, nos indicia que este seja significativo para o modelo,ou seja, com um nível de singificância de 5% temos evidências de que $\beta_3$ seja diferente de zero, portanto iremos continuar considerando o modelo completo sendo o modelo ajustado dado por:

```{r,echo=FALSE}
modelo       <- lm(data = dados2,formula = bh.yield ~ I(aq.resist) + I(aq.thick) + I(anisotrophy)    )
#modelo       %>% kable_estimates(cap = "Estimativas para o modelo completo")
modelo_glm      <- glm(data = dados2,formula = bh.yield ~ I(aq.resist) + I(aq.thick) + I(anisotrophy),family = gaussian(link = "identity"))
#summary(modelo_glm)
```

$$\hat{Y}= -2,493 + 0,021\text{aq.resist} + 0,072\text{aq.thick}+ 2,8969\text{anisotrophy}$$

```{r,echo=FALSE}
source(file = "funcoes.R",encoding = "UTF-8")
coeficientes <- as.vector(round(modelo$coefficients,3))
#coeficientes %>% print_ajuste()
```



Onde :

* **$\hat{Y}$** : representa o valor esperado para o redimento da bomba dos poços em $litros/s$;

* **$X_1$ ** : representa a variável resistividade do aquecedor, e dado que $X_2=0$ e $X_3=0$, nos indica que a cada aumento em uma unidade em $X_1$ representa o aumento em 0.021 no valor esperado do rendimento da bomba dos poços;

* **$X_2$** : representa a variável espessura, e dado que $X_1=0$ e $X_3=0$, nos indica que a cada aumento em uma unidade em $X_2$, representa o aumento em 0.072 no valor esperado no rendimento da bomba dos poços;

* **$X_3$** : representa a variável coeficiente de anisotrofia, e dado que $X_1=0$ e $X_2=0$, nos indica que a cada aumento em uma unidade em $X_3$, representa o aumento em  2.896 no valor esperado do rendimento da bomba dos poços;



```{r,echo=FALSE,include=FALSE, fig.cap="Gráfico normal de probabilidade com envelope",warning=F}
source(file = "funcoes.R",encoding = "UTF-8")
library(hnp)
set.seed(103159)
env1 <- envelope(modelo_glm) + axis.theme()

```


```{r,include=F}
hnp_data <- hnp(modelo_glm,how.many.out= T,halfnormal = F,plot.sim=F,warn = F,print.on = F)
d <- data.frame(qtd_residuos=hnp_data$out,porcentagem=(hnp_data$out/hnp_data$total)*100) 
d %>% kable_data(cap = "Quantidade de Resíduos Fora dos Limites do Gráfico de Envelope",foot = NULL)


```



```{r,echo=FALSE, include=FALSE}
set.seed(103159)
y  <- dados2$bh.yield
x1 <- dados2$aq.resist**2
x2 <- dados2$aq.thick**(-4)
x3 <- (dados2$anisotrophy)**(-0.8)
modelo_transformado       <- lm(formula = y ~ I(x1) + I(x2) + I(x3)    )
# modelo_transformado       %>% kable_estimates(cap = "Estimativas para o modelo completo utilizando as transformações para as covariáveis")

```



```{r,echo=FALSE,include=FALSE,fig.cap="Gráfico normal de probabilidade para o modelo sem transformação e com a transformação 1",fig.height=2}
modelo_glm_transf      <- glm( formula = y ~ I(x1) + I(x2) + I(x3),family = gaussian(link = "identity"))
env2 <- envelope(modelo_glm_transf) + axis_theme
#summary(modelo_transformado)
# hnp_data <- modelo_transformado       %>% hnp(how.many.out= TRUE,halfnormal = F,plot.sim=F)
# d <- data.frame(Quantidade=hnp_data$out,`Percentual`=(hnp_data$out/hnp_data$total)*100) 
# d %>% kable_data(cap = "Quantidade de Resíduos Fora dos Limites do Gráfico de Envelope",foot = NULL)

cowplot::plot_grid(env1,env2,ncol = 2,nrow = 1,labels = LETTERS[1:2])

```




```{r ,echo=FALSE}
source(file = "funcoes.R",encoding = "UTF-8")
residuos    <- resid(modelo_transformado)
r_padrao    <- rstandard(modelo_transformado)
r_stu       <- rstudent(modelo_transformado)
r_external  <- r_student_external(modelo_transformado)
fit_values  <- modelo_transformado$fitted.values

```



```{r  echo=FALSE,include=FALSE, fig.cap= "Gráfico de probabilidade normais para os resíduos"}

source(file = "funcoes.R",encoding = "UTF-8")
b1 <- residual_qqnorm(data = residuos,title = " dos Resíduos")
b2 <- residual_qqnorm(data = r_padrao,title = " dos Resíduos \n Padronizados")
b3 <- residual_qqnorm(data = r_stu,title = " dos Resíduos \n Studentizados")
b4 <- residual_qqnorm(data = r_external,title = " dos Resíduos \n Studentizados Externalizados")

#gridExtra::grid.arrange(b1,b2,b3,b4)

```



```{r,echo=FALSE}

shapiro_test <- shapiro.test(x = r_external) 
shapiro_test <- data.frame(Estatistica=shapiro_test$statistic,P_Valor=shapiro_test$p.value,row.names = "Shapiro-Wilk") 
ks_test      <- ks.test(x = r_external,y = "pnorm")
ks_test      <- data.frame(Estatistica=ks_test$statistic,P_Valor=ks_test$p.value,row.names = "Kolmogorov Smirnov") 
#rbind(shapiro_test,ks_test) %>% kable_data(cap = "Estatísticas para os testes de Shapiro-Wilk e Kolmogorov Smirnov para normalidade")


```



```{r  echo=FALSE}
source(file = "funcoes.R",encoding = "UTF-8")
b1 <- plot_resvfit(y = residuos,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Resíduos vs \n Valores Ajustados")
b2 <- plot_resvfit(y =  r_padrao,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Resíduos Padronizados vs \n Valores Ajustados")
b3 <- plot_resvfit(y =  r_stu,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Resíduos Studentizados vs \n Valores Ajustados")
b4 <- plot_resvfit(y =  r_external,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Res. Stu. Externalizados vs \n Valores Ajustados")
#b4
#grid.arrange(b1,b2,b3,b4)


```


```{r}

library(lmtest)
library(tseries) 

bp_test <- bptest(modelo_transformado)
gq_test <- gqtest(modelo_transformado)
 
bp_test      <- data.frame(Estatistica=bp_test$statistic,P_Valor=bp_test$p.value,row.names = "Breusch-Pagan") 
gq_test      <- data.frame(Estatistica=gq_test$statistic,P_Valor=gq_test$p.value,row.names = "Goldfeld-Quandt") 
#rbind(bp_test,gq_test) %>% kable_data(cap = "Estatísticas para os testes de Breusch-Pagan e Goldfeld-Quandt para heterocedasticidade")
```



```{r}

boxcox     <- boxcox(object = modelo, lambda = seq(-0.5, 1, by = 0.1),plotit = FALSE)
pos_lambda <- match(max(boxcox$y),boxcox$y)
lamb       <- round(boxcox$x[pos_lambda],2)

```



Tentando realizar um modelo alternativo, considerando  uma transformação box-cox para ajustar um modelo mais representativo dos dados. Primeiramente obtemos o coeficiente $\lambda$. Utilizando a função 'boxcox' da biblioteca "MASS", obtemos um valor de $\lambda$ = `r lamb`, portanto teremos uma transformação em $y$ da forma :
 

```{r}
exp <- paste0("$$ \\frac{y^{",lamb,"} - 1}{",lamb,"} $$")
knitr::raw_latex(exp)

```


Além da transformação em $Y$ podemos testar algumas transformações nas covariáveis para afim de obter uma melhor representatividade, principalmente no intuito de melhorar e atingir o pressuposto de normalidade para o modelo. Fora realizado as seguintes transformações para as covariáveis: $X_1^*=X_1^3$, $X_2^*=X_2^2$ e $X_3^*=X_3^5$.


```{r,echo=FALSE}
y  <- ((dados2$bh.yield^lamb) - 1)/lamb
x1 <- dados2$aq.resist**3
x2 <- (dados2$aq.thick)**(2)
x3 <- dados2$anisotrophy**(5)
modelo_boxcox_glm_transf   <- glm( formula = y ~ I(x1) + I(x2) + I(x3),family = gaussian(link = "identity"))
modelo_boxcox              <- lm(formula = y ~ I(x1) + I(x2) + I(x3))
#modelo_boxcox %>% kable_estimates(cap = "Estimativas para o modelo completo utilizando transformação de Box-Cox")

```

```{r , include=FALSE,fig.cap="Gráfico de probabilidade normal para o modelo transformado Box-Cox"}
env3 <- envelope(modelo_boxcox_glm_transf) + axis_theme

```


```{r,include=F}
hnp_data <- modelo_boxcox       %>% hnp(how.many.out= TRUE,halfnormal = F,plot.sim=F)
d <- data.frame(Quantidade=hnp_data$out,`Percentual`=(hnp_data$out/hnp_data$total)*100) 
#d %>% kable_data(cap = "Quantidade de Resíduos Fora dos Limites do Gráfico de Envelope",foot = NULL)

```



```{r Análise de Resíduos2,echo=FALSE}
source(file = "funcoes.R",encoding = "UTF-8")
residuos    <- resid(modelo_boxcox)
r_padrao    <- rstandard(modelo_boxcox)
r_stu       <- rstudent(modelo_boxcox)
r_external  <- r_student_external(modelo_boxcox)
fit_values  <- modelo_boxcox$fitted.values

```



```{r  echo=FALSE}

source(file = "funcoes.R",encoding = "UTF-8")
b1 <- residual_qqnorm(data = residuos,title = " dos Resíduos")
b2 <- residual_qqnorm(data = r_padrao,title = " dos Resíduos \n Padronizados")
b3 <- residual_qqnorm(data = r_stu,title = " dos Resíduos \n Studentizados")
b4 <- residual_qqnorm(data = r_external,title = NULL)

#gridExtra::grid.arrange(b1,b2,b3,b4)

```



```{r,echo=FALSE}

shapiro_test <- shapiro.test(x = r_external) 
shapiro_test <- data.frame(Estatistica=shapiro_test$statistic,P_Valor=shapiro_test$p.value,row.names = "Shapiro-Wilk") 
ks_test      <- ks.test(x = r_external,y = "pnorm")
ks_test      <- data.frame(Estatistica=ks_test$statistic,P_Valor=ks_test$p.value,row.names = "Kolmogorov Smirnov") 
#rbind(shapiro_test,ks_test) %>% kable_data(cap = "Estatísticas para os testes de Shapiro-Wilk e Kolmogorov Smirnov para normalidade")


```




```{r  include=FALSE,echo=FALSE,fig.height=2,fig.cap="Gráfico de envelope e valores ajustados vs. resíduos"}
source(file = "funcoes.R",encoding = "UTF-8")
b1 <- plot_resvfit(y = residuos,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Resíduos vs \n Valores Ajustados")
b2 <- plot_resvfit(y =  r_padrao,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Resíduos Padronizados vs \n Valores Ajustados")
b3 <- plot_resvfit(y =  r_stu,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = " Resíduos Studentizados vs \n Valores Ajustados")
b4 <- plot_resvfit(y =  r_external,x = fit_values,labelx = "Valores Ajustados",labely = "Resíduos",
                   title = NULL)

#grid.arrange(b1,b2,b3,b4)

cowplot::plot_grid(env3,b4,ncol = 2,nrow = 1,labels = LETTERS[1:2])

```




```{r,echo=FALSE,include=FALSE}

library(lmtest)
library(tseries) 

bp_test <- bptest(modelo_transformado)
gq_test <- gqtest(modelo_transformado)
 
bp_test      <- data.frame(Estatistica=bp_test$statistic,P_Valor=bp_test$p.value,row.names = "Breusch-Pagan") 
gq_test      <- data.frame(Estatistica=gq_test$statistic,P_Valor=gq_test$p.value,row.names = "Goldfeld-Quandt") 
rbind(bp_test,gq_test) %>% kable_data(cap = "Estatísticas para os testes de Breusch-Pagan e Goldfeld-Quandt para heterocedasticidade",foot=NULL,label = "varh")
```

  
Portanto considerando as transformação de box-cox realizada na variável resposta, e as transformações nas covariáveis temos o ajuste sendo o seguinte modelo ajustado:

```{r,echo=FALSE}
source(file = "funcoes.R",encoding = "UTF-8")
coeficientes <- as.vector(round(modelo_boxcox$coefficients,3))
#coeficientes %>% print_ajuste()

```


$$\hat{Y} = - 1.053 + 0.01 \text{aq.thick} - 1.069 \text{anisotrophy}$$

Vamos agora considerar o ajuste do modelo aditivo para tentar captar a tendência dos dados referente ao rendimento de bombas de poço. Considerando as variáveis  $X_1 = \text{aq.resist}$, $X_2 = \text{aq.thick}$ e $X_3 = \text{anisotrophy}$ temos o modelo aditivo,

\begin{equation*}
  y = \alpha + f_1(X_1) + f_2(X_2) + f_3(X_3)  +\epsilon
\end{equation*}

Aplicando o método de retroajuste para ajustar a funções por meio das técnicas \textit{Loess}, \textit{Splines} de Regressão Linear e Cúbico, a Figura 22 apresenta as curvas residuais parciais utilizando cada método. Verificamos o gráfico A e B (figura 29), que o ajuste \textit{Loess} capta muito bem a tendência sofrendo baixa influência de pontos mais distantes. Já para as curvas azul e verde (\textit{Splines} de Regressão), percebemos que este sugerem um comportamento quadrático para o gráfico A e uma tendência exponecial para o gráfico B, ambos tentando captar uma maior quantidade de pontos mais distantes e dispersos quando comparamos com o ajuste \textit{Loess}. Para o gráfico C (\textit{anisotrophy}), percebemos que as três técnicas se ajustam de forma bem equivalente, porém a técnica \textit{Loess} consegue captar alguns pontos mais extremos quando comparamos com as demais curvas. Percebe-se que todas as covariáveis são significativas para explicar a resposta, com as covariáveis aq.thick e anisotropphy tendo uma tendência linear.

```{r,echo=F}

backfitting3 <- function(x,y,tol   = 0.001*sd(y)/sqrt(length(y)),span  = c(3/4,3/4),bdw  = c(10,10,10), k = c(5,5), knots = list(v1=NULL,v2=NULL,v3=NULL),method = "loess"){
  n     <- length(y)
  fx_11 <- numeric(n)
  fx_21 <- numeric(n)
  fx_31 <- numeric(n)
  
  
  alpha     <- mean(y)
  converged <- FALSE
  
  while( !converged ){
    
    fx_10           <- fx_11
    fx_20           <- fx_21
    fx_30           <- fx_31
    
    if(method=="loess"){
      y_x3                  <- y - alpha - fx_11 - fx_21
      fx_31[order(x[,3])]   <- lowess(x = x[,3],y = y_x3,f = span[3])$y
      
      y_x2            <- y - alpha - fx_11 - fx_31
      fx_21[order(x[,2])]   <- lowess(x = x[,2],y = y_x2,f = span[2])$y
      
      y_x1            <-  y - alpha - fx_21 - fx_31
      fx_11[order(x[,1])]   <- lowess(x = x[,1],y_x1,f = span[1])$y
      
    }else if( method == "kernel"){
      # y_x2            <- y - alpha - fx_11
      # fx_21[x2_ord]   <- ksmooth(x = x[,2],y = y_x2,kernel = "normal",bandwidth =  bdw[2])$y
      # y_x1            <-  y - alpha - fx_21
      # fx_11[x1_ord]   <- ksmooth(x = x[,1],y = y_x1,kernel = "normal",bandwidth = bdw[1])$y
      
      
       y_x3                  <- y - alpha - fx_11 - fx_21
      fx_31[order(x[,3])]   <- ksmooth(x = x[,3],y = y_x3,kernel = "normal",bandwidth =  bdw[3])$y
      
      y_x2            <- y - alpha - fx_11 - fx_31
      fx_21[order(x[,2])]   <- ksmooth(x = x[,2],y = y_x2,kernel = "normal",bandwidth =  bdw[2])$y
      
      y_x1            <-  y - alpha - fx_21 - fx_31
      fx_11[order(x[,1])]   <- ksmooth(x = x[,1],y = y_x1,kernel = "normal",bandwidth =  bdw[1])$y
      
      
    }else if( method == "sr.linear"){
      # y_x2            <- y - alpha - fx_11
      # fx_21[x2_ord]   <- additive.spline.linear(x = x[,2],y = y_x2,k = k[2])$fitted.values
      # y_x1            <-  y - alpha - fx_21
      # fx_11[x1_ord]   <- additive.spline.linear(x = x[,1],y = y_x1,k = k[1])$fitted.values
      # 
      
       y_x3                  <- y - alpha - fx_11 - fx_21
      fx_31[order(x[,3])]   <- additive.spline.linear(x = x[,3],y = y_x3,k = k[3])$fitted.values
      
      y_x2            <- y - alpha - fx_11 - fx_31
      fx_21[order(x[,2])]   <- additive.spline.linear(x = x[,2],y = y_x2,k = k[2])$fitted.values
      
      y_x1            <-  y - alpha - fx_21 - fx_31
      fx_11[order(x[,1])]   <- additive.spline.linear(x = x[,1],y = y_x1,k = k[1])$fitted.values
      
      
      
    }else if( method == "sr.cubic"){
      # y_x2            <- y - alpha - fx_11
      # fx_21[x2_ord]   <- additive.spline.cubic(x = x[,2],y = y_x2,k = k[2],knots = knots$v2)$fitted.values
      # y_x1            <-  y - alpha - fx_21
      # fx_11[x1_ord]   <- additive.spline.cubic(x = x[,1],y = y_x1,k = k[1],knots = knots$v1)$fitted.values
      
       y_x3                  <- y - alpha - fx_11 - fx_21
      fx_31[order(x[,3])]   <- additive.spline.cubic(x = x[,3],y = y_x3,k = k[3],knots = knots$v3)$fitted.values
      
      y_x2            <- y - alpha - fx_11 - fx_31
      fx_21[order(x[,2])]   <- additive.spline.cubic(x = x[,2],y = y_x2,k = k[2],knots = knots$v2)$fitted.values
      
      y_x1            <-  y - alpha - fx_21 - fx_31
      fx_11[order(x[,1])]   <- additive.spline.cubic(x = x[,1],y = y_x1,k = k[1],knots = knots$v1)$fitted.values
      
      
    }
    
    Norma_f1_0 <- sqrt(t(fx_10)%*%(fx_10))
    Norma_f2_0 <- sqrt(t(fx_20)%*%(fx_20))
    Norma_f3_0 <- sqrt(t(fx_30)%*%(fx_30))
    
    Norma_dif_f1 <- sqrt(t(fx_11 - fx_10)%*%(fx_11 - fx_10))
    Norma_dif_f2 <- sqrt(t(fx_21 - fx_20)%*%(fx_21 - fx_20))
    Norma_dif_f3 <- sqrt(t(fx_31 - fx_30)%*%(fx_31 - fx_30))
    
    Dif_rel <- (Norma_dif_f1 + Norma_dif_f2 + Norma_dif_f3)/(Norma_f1_0 + Norma_f2_0 + Norma_f3_0) # Diferen?a Relativa
    if(Dif_rel > tol){
      
      converged = TRUE
      
    }
    
    return(list(y.x1=(y - alpha - (fx_21 - mean(fx_21)) - (fx_31 - mean(fx_31))),
                y.x2=(y - alpha - (fx_11 - mean(fx_11)) - (fx_31 - mean(fx_31))),
                y.x3=(y - alpha - (fx_11 - mean(fx_11)) - (fx_21 - mean(fx_21))),
                fx_11 = fx_11 - mean(fx_11),
                fx_21 = fx_21 - mean(fx_21),
                fx_31 = fx_31 - mean(fx_31)))
  }
}



```
  
  
```{r}

source(file = "funcoes.R",encoding = "UTF-8")
dados        <-    read.table(file = "aquiferborehole.csv",
                         header = T,sep = ",",encoding = "UTF-8")
dados        <-    dados[,2:length(dados)]
var.resposta <- 6
dados2       <- dados[,c(3:5,var.resposta)]
#y            <- dados[,var.resposta]
modelo       <- lm(data = dados2,formula = bh.yield ~ I(aq.resist) + I(aq.thick) + I(anisotrophy) )
y            <- modelo$coefficients[1] + (modelo$coefficients[2] * dados$aq.resist) + (modelo$coefficients[3] * dados$aq.thick ) + (modelo$coefficients[4] * dados$anisotrophy )    
x            <- dados[,3:5]
```





```{r, fig.cap="Resíduos parciais das variáveis aq.resist, aq.thick e anisotrophy", fig.height=5.5,echo=F}

span_v <- c(.4,.65,.75)
nos    <- list(v1=c(10),v2=c(10),v3=c(10))  
nos1   <- c(4,2,4)
bdw    <- c(5,5,5)
fit1   <- backfitting3(x=x,y=y,span = span_v,bdw = NULL,k = NULL,method = "loess")
fit2   <- backfitting3(x=x,y=y,span = NULL,bdw = bdw,k = nos1,method = "sr.linear")
fit3   <- backfitting3(x=x,y=y,span = NULL,bdw = NULL,k = c(3,3,3),method = "sr.cubic",knots = nos)
lim_y_x1  <- range(fit1$y.x1, (fit1$fx_11-mean(fit1$fx_11)))
 lim_y_x2  <- range(fit1$y.x2, (fit1$fx_21-mean(fit1$fx_21)))
 lim_y_x3  <- range(fit1$y.x3, (fit1$fx_31-mean(fit1$fx_31)))

#df        <- as.data.frame(cbind(x1,fit1$y.x1,fit1$fx_11,fit3$fx_11))
df        <- as.data.frame(cbind(x[,1],fit1$y.x1,fit1$fx_11,fit2$fx_11,fit3$fx_11))

colnames(df) <- c("x","y",
                    paste("\nAjuste1\nLoess\n", "Span : ",span_v[1]),
                    paste("\nAjuste2\nSplines Reg. Linear\n", "Nós : ",nos1[1]),
                    paste("\nAjuste3\nSplines Reg. Cúbico\n", "Nós : ",nos[[1]])
                   
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

 plot1 <- ggplot(data = df,aes(x=x,y=y))+
    geom_point(alpha=point.alpha,size=point.size, col = point.color)+
    labs(x = "aq.resist", 
         y = "Resíd. de aq.resist",
         color = "Legenda")+
    xlim(range(x[,1])+c(-0,+0))+
    ylim(lim_y_x1)+
    #ggtitle(paste0("Gráfico de dispersão \n Resíduos parciais de f(x1)"))+
    geom_line(aes(x=x,y = value,color=variable),size=line.size,alpha=line.alpha) +
    scale_color_manual(values = cores) + axis.theme(title_size = 10,pos_leg = "right")

 
 ### plot2
 df        <- as.data.frame(cbind(x[,2],fit1$y.x2,fit1$fx_21,fit2$fx_21,fit3$fx_21))
colnames(df) <- c("x","y",
                    paste("\nAjuste1\nLoess\n","Span : ",span_v[2]),
                    paste("\nAjuste2\nSplines Reg. Linear\n", "Nós : ",nos1[2]),
                    paste("\nAjuste3\nSplines Reg. Cúbico\n", "Nós : ",nos[[2]])
                   
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

 plot2 <- ggplot(data = df,aes(x=x,y=y))+
    geom_point(alpha=point.alpha,size=point.size, col = point.color)+
     labs(x = "aq.thick", 
         y = "Resíd. de aq.thick",
         color = "Legenda")+
    xlim(range(x[,2])+c(-0,+0))+
    ylim(lim_y_x2)+
    #ggtitle(paste0("Gráfico de dispersão \n Resíduos parciais de f(x2)"))+
    geom_line(aes(x=x,y = value,color=variable),size=line.size,alpha=line.alpha) +
    scale_color_manual(values = cores) + axis.theme(pos_leg = "right")
 
 
 df        <- as.data.frame(cbind(x[,3],fit1$y.x3,fit1$fx_31,fit2$fx_31,fit3$fx_31))
colnames(df) <- c("x","y",
                    paste("\nAjuste1\nLoess\n","Span : ",span_v[3]),
                    paste("\nAjuste2\nSplines Reg. Linear\n", "Nós : ",nos1[3]),
                    paste("\nAjuste3\nSplines Reg. Cúbico\n", "Nós : ",nos[[3]])
                   
                    )

df <- as.tibble(df) %>%
  gather(key = "variable", value = "value",-x,-y)

 plot3 <- ggplot(data = df,aes(x=x,y=y))+
    geom_point(alpha=point.alpha,size=point.size, col = point.color)+
    labs(x = "anisotropphy", 
         y = "Resíd. de anisotropphy",
         color = "Legenda")+
    xlim(range(x[,3])+c(-0,+0))+
    ylim(lim_y_x3)+
    #ggtitle(paste0("Gráfico de dispersão \n Resíduos parciais de f(x2)"))+
    geom_line(aes(x=x,y = value,color=variable),size=line.size,alpha=line.alpha) +
    scale_color_manual(values = cores) + axis.theme(pos_leg = "right")

 
 full_plot <- cowplot::plot_grid(plot1,plot2,plot3,nrow = 3,ncol=1,labels=LETTERS[1:3]);full_plot
 ggsave(filename = "plot_aplicacao.png",plot = full_plot,width = 10,height = 8)
 
 
```


  **Inserir comparações de prediçoes**

  
   
\section{Conclusão}

\hspace{1.25cm} Para investigar e modelar relações entre variáveis o modelo de regressão linear pode ser utilizado, porém quando essa relação não possui forma linear, uma alternativa é o uso de ferramentas que não impõem suposições paramétricas. Nesse contexto, existem técnicas de suavização que podem ser utilizadas, inclusive na estimação das funções do componente sistemático dos modelos aditivos. Caso haja mais de uma covariável para predizer Y, o algoritmo de retroajuste pode ser uma solução.

Para validar a metodologia estudada, foram realizadas análises em dados simulados e dados reais, onde foram ajustados o modelo de regressão linear múltipla e o modelo aditivo, de acordo com o que foi estudado e com as ferramentas disponíveis. Em dados simulados em diferentes cenários, foram observados os resultados em relação ao comportamento de técnicas de suavização, bem como a aplicação do algoritmo de retroajuste em cenários simulados com modelos aditivos.

Vale ressaltar que a escolhas dos parâmetros de suavização foram feitas arbitrariamente, porém existem técnicas específicas para fazer essa escolha de modo otimizado, sendo a mais popular a validação cruzada e a validação cruzada generalizada. Além disso, destaca-se que as análises realizadas em relação à qualidade dos ajustes foram estritamente gráficas. Existem medidas numéricas adequadas para fazer essa análise, que serão abordadas em trabalhos futuros.



\clearpage

\section{Referências}

\noindent BUJA, A., HASTIE, T. & TIBSHIRANI, R. (1989). **Linear smoothers and additive models**. The Annals of Statistics, 17, 453-510.

\noindent CLEVELAND, W. S. (1979). **Robust locally weighted regression and smoothing scatterplots**. Journal of the American Statistical Association, 74,
829-836.  
HASTIE, T. J. & TIBSHIRANI, R. J. (1990). **Generalized additive models**, volume 43. Chapman and Hall, Ltd., London. ISBN 0-412-34390-8.

\noindent MONTGOMERY, D. C.; PECK, E. A.; VINING, G. G. **Introduction to Linear Regression Analysis**. 5th Edition. John Wiley & Sons, 2012. 

\noindent Izbicki, Rafael; Santos, Tiago Mendonça. **Aprendizado de máquina: uma abordagem estatística**. ISBN 978-65-00-02410-4.

\noindent TEAM, R. CORE. R: **A language and environment for statistical computing**. (2013). 




